\section{Experimental Protocol}
\label{sec:experiments-results}

This section presents the datasets,
metrics, 
and protocols used for evaluating the proposed method.

\subsection{Datasets}
We evaluated the proposed methods in two datasets widely used for evaluating text localization methods, the ICDAR'11 and ICDAR'13. We also used the SynthText dataset to help
training our network due to the small size of the ICDAR's datasets.

\begin{itemize}
    \item {\bf SynthText:} This dataset comprises $858,750$ synthesized text images, which were generated by blending rendered words with natural images~\cite{Gupta2016CVPR}. The synthetic engine proposed by the authors automatically choose the location, in a target image, and transforms a word by using an algorithm that selects contiguous regions based on local color and texture cues. Next, the words were rendered using a randomly selected font,
transformed according to the local surface orientation, and 
finally blended into the scene using the Poisson image editing approach~\cite{Perez2003ToG}.

\item{\bf ICDAR'11:} The ICDAR'11 dataset~\cite{Karatzas2011ICDAR} was introduced in \textit{ICDAR 2011 Robust Reading Competition -- ``Reading Text in Born-Digital Images (Web and Email)''}. 
It is an extension of the dataset used for the text locating competitions of ICDAR 2003~\cite{Lucas2003ICDAR} and ICDAR 2005~\cite{Lucas2005ICDAR}, and contains $551$ images, which were divided into two subsets, 
$410$ images for training and $141$ for test.
The images of this dataset have texts digitally created on them, such as headers, logos, captions, among others. The annotations were built in terms of rectangle word bounding boxes and contains $5,003$ words.

%\paragraph*{ICDAR'13:} 
\item{\bf ICDAR'13:}
This dataset was introduced in \textit{ICDAR 2013 -- ``Focused Scene Text challenge''} and has $462$ images divided into two subsets, training and testing sets, which contains $229$ and $233$ images, respectively~\cite{Karatzas2013ICDAR}. The images in this dataset are born-digital or scene text (captured under a wide variety, such as blur, varying distance to camera, font style and sizes, color, texture, etc). All the text lines are horizontal or near horizontal. The annotations were built in terms of rectangles word bounding boxes and comprise $1,943$ words.

\end{itemize}

\subsection{Evaluation Metrics}

We evaluated the methods in terms of effectiveness and efficiency, according to the metrics described as follow:

\begin{itemize}
\item{\bf  Effectiveness:} We evaluated the effectiveness of the methods in terms of recall, precision, and f-measure. %Here, we consider a correct detection (true positive) if the overlap between the ground-truth annotation and detected bounding box, which is measured by computing the intersection over union, is greater than $50\%$ (similar to standard practice in object recognition~\cite{Everingham2015}). Otherwise, the detected bounding box is considered an incorrect detection (false positive).

\begin{itemize}

      \item \textbf{Intersection over Union (IoU)} is used as protocol to measure the accuracy level between the text detected bounding boxes and the text ground-truth bounding boxes. 
    A detected bounding boxes is considered a correct detection (true positive), if the overlap between the ground-truth annotation and detected bounding box, which is measured by computing the Intersection of Union (Equation~\ref{eq:iou}), is greater than $50\%$. Otherwise, the detected bounding box is considered an incorrect detection (false positive). This protocol was proposed by \etal{Everingham} in the context of the PASCAL VOC challenge~\cite{Everingham2010IJCV} in 2009. Nowadays, it is adopted in ICDAR competitions.
    \begin{equation}
    IoU = \frac{area(B_p \cap B_{gt})}{area(B_p \cup B_{gt})},
    \label{eq:iou}
    \end{equation}
    where $B_p \cap B_{gt}$ and  $B_p \cup B_{gt}$ stand, respectively, for the intersection and the union of the predicted ($B_p$) and ground truth ($B_{gt}$) bounding boxes.
    %
    \item  \textbf{Recall ($R$)} refers to the fraction of text regions correctly detected, given the set of all text regions labeled in the dataset:
    \begin{equation}
        R = \frac{\sum \textrm{true positive}}{\sum (\textrm{true positive + false negative})}
    \end{equation}
    %
    \item  \textbf{Precision ($P$)} refers to the fraction of text regions correctly detected, given all text regions detected by the text detector:
    \begin{equation}
        P = \frac{\sum \textrm{true positive}}{\sum (\textrm{true positive + false positive})}
    \end{equation}
    %
    \item \textbf{F-measure} combines $P$ and $R$, allowing the possibility of having one single effectiveness score to assess the overall quality of a detector. It is defined as:
    \begin{equation}
        \textrm{F-measure} = 2 \times \left( \frac{P \times R}{P + R} \right)
    \end{equation}
    %
  
\end{itemize}

%\todo[inline]{DEFINE THE METRICS!!!!!}

%\paragraph*{Efficiency:} 
\item {\bf Efficiency:} The efficiency aspects considered both the processing time and the disk usage (in MB). We used the Linux {\em time} command to measure the processing time, %since this tool can be applied to all evaluated methods, regardless the programming language.
while the disk usage considered the size of the learned models. %and excluding the disk usage regarding the source code and the possible libraries used by the methods (e.g., Tesseract (tessdata) tool).
All experiments were performed considering a Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz with 12 cores, a Nvidia GTX 1080 TI GPU, and 64GB of RAM.
% \begin{itemize}
%     \item \textit{CPU}: Intel(R) Core(TM) i7-8700 CPU @ 3.20GH;
%     \item \textit{GPU}: Nvidia GTX 1080 ti 11GB and Nvidia GTX Titan X 12GB;
%     \item \textit{Memory RAM}: 62GB;
%     \item \textit{Operating system}: Ubuntu 16.04 LTS (Xenial Xerus).
% \end{itemize}
\end{itemize}

\subsection{Evaluation Protocols}
\label{sec:protocols}

Here, we describe the experimental protocols used for evaluating the proposed method. 

\subsubsection{Comparisons with Baselines}

The experiments were divided into three steps: training, fine-tuning, and test. For the training step, we used three subsets of the SynthText dataset. This dataset comprises of images with synthetic texts added in different backgrounds and we selected samples of the dataset considering $10$ ($9.25\%$), $20$ ($18.48\%$), and $30$ ($27.71\%$) images per background. The resulting subsets were again divided into train and validation, using $70\%$ for training and $30\%$ for validation. Using these collections, we trained a model with random initialization parameters until we found no significant variance in the loss function. For the fine-tuning step, we took the model trained in SynthText and continued this training using ICDAR'11 or ICDAR'13 training subsets, stopping when we found no significant variance in the loss function. Finally, for the test step, we evaluated each fine-tuned model in the test subset of ICDAR'11 or ICDAR'13.

\begin{itemize}
\item {\bf Experimental setup}

We conducted the training of the proposed method considering a single-scale input, and therefore, all input images were resized to $300 \times 300$ pixels. The training phase was performed using a batch size of $24$ and we used the RMSprop optimizer~\cite{Tieleman2012} with a learning rate of $4 \times 10^3$. We also use the regularization L2-norm, with a $\lambda=4 \times 10^5$, to prevent possible over-fitting. We conducted the training until the network converge.



\item{\bf Baselines}


This section provides an overview of the chosen methods for comparison purpose. For a fair comparison, we selected recent approaches specifically designed for a fast detection, including SqueezeDet and YOLOv3. We also use state-of-the-art methods for text localization as baselines.

\begin{itemize}
%\paragraph*{TextBoxes:} 
\item{\bf TextBoxes:} This method consists of a Fully Convolutional Network (FCN) adapted for text detection and recognition~\cite{Liao2017AAAI}. This network uses the VGG-16 network as feature extractor followed by multiple output layers (text-boxes layers), similar to SSD network. At the end, the Non-maximum suppression (NMS) process is applied to the aggregated outputs of all text-box layers. The authors also adopt an extra NMS for multi-scale inputs on the task of text localization.

\item{\bf TextBoxes++:} Liao et al.~\cite{Liao2018TIP} proposed an end-to-end solution able to predict arbitrary orientation word bounding boxes. This architecture is a Fully Convolutional Neural Network (FCN) that detects arbitrary-oriented text. This architecture is inherited from the  popular VGG-16  architecture used for the ImageNet competition. First, the last two FCN layers of VGG16 are converted into convolutional layers (conv6 and conv7). Next, other eight convolution layers divided into four stages (conv8 and conv11) with  different  resolutions  by  max-pooling  are  appended  after conv7. In the following, multiple output layers (text boxes layers) are inserted after the last and intermediate convolutional layers to predict text presence and bounding boxes. Finally, a non-maximum suppression (NMS) process is applied to the aggregated outputs of all text-box layers.

\item {\bf Single-Shot Text Detector (SSTD):} \etal{He}~\cite{He2017ICCV} designed a natural scene text detector that directly outputs word-level bounding boxes without post-processing, except for a simple NMS. The detector can be decomposed into three parts: a convolutional component, a text-specific component, and a box prediction component. The convolutional and box prediction components are inherited from the SSD detector~\cite{Liu2016ECCV} and the authors proposed a text-specific component which consists of a text attention module and a hierarchical inception module.

\item {\bf SqueezeDet:} This network was proposed to detect objects for the autonomous driving problem, which requires a real-time detection~\cite{Wu2017CVPRW}. The SqueezeDet contains a single-stage detection 
% pipeline, which comprises essentially three components: a FCN, a Convolutional layer, and a filtering stage to remove redundant bounding boxes. The FCN is responsible for generating the feature map, while % for the input images. Next, these feature maps are used to feed 
% the convolutional layer %, which 
% is responsible for detecting, localizing, and classifying objects at the same time. The non-maximum suppression (NMS) is also applied to remove the overlapped bounding boxes.
pipeline, which comprises three components: (i) a FCN responsible for generating the feature map for the input images; (ii)  a convolutional layer responsible for detecting, localizing, and classifying objects at the same time; and (iii) the non-maximum suppression (NMS) method, which is applied to remove the overlapped bounding boxes.

\item {\bf YOLOv3:} This is a convolutional network originally proposed for the object detection problem~\cite{Redmon2018CoRR}. Similarly to SSD network, the YOLOv3 predicts bounding boxes and class probabilities, at the same time. The bounding boxes are predicted using dimension clusters as anchor boxes and predicts an objectness score for each bounding box using logistic regression.

\end{itemize}

\end{itemize}

\subsubsection{Experiments on Mobile-Oriented Environment}

This section provides a description of experiments performed on a device with restricted computing capacity. %After a description of the  mobile device technical specifications, the experimental protocol is defined alongside with the data used to evaluate the solution.

To emulate the use of our proposed solution on real-world usage scenarios and to evaluate the performance on a real constrained computing, an Android application was developed and executed. The developed Android application (APP) utilizes TensorFlow~\cite{tensorflow}'s Android API so it can load and execute the same model used for inference in our previous evaluations. 
The chosen portable device for the implementation and execution of our test was a \textit{Xiaomi Redmi Note 5} smartphone, running Android OS version 9 on a \textit{Qualcomm Snapdragon 636} chipset, comprehending a quad-core 1.8Ghz processor alongside a quad-core 1.6Ghz processor and 4 Gb of RAM.

Two sets of experiments where conducted with the goal of assessing the embedded system: 
\begin{enumerate}
    \item Evaluation of the processing time of the proposed approach when running on a mobile device. Experiments considered the detection of images belonging to the ICDAR'11 and ICDAR'13 datasets; and
    \item Evaluation on a real-world mobile-based usage scenarios.
\end{enumerate}

To ensure that the model executed on the mobile device has the same quantitative effectiveness that the one executed on a non-restrictive computing scenario, the same experiments used to evaluate the proposal on the non-restrictive device were executed on the mobile device, conserving datasets and evaluation metric configurations.

The proposed solution was also evaluated in real-world usage scenarios. Given the portability of the embedded APP, the proposed approach was evaluated in detection scenarios involving several images depicting texts in scenes captured using the portable device. To evaluate the efficiency of the proposed method considering a restricted computing environment, we collected $250$ images containing text and non-text elements, captured directly from the built-in camera of the mobile device.

