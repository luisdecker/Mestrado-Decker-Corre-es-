% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@String { ELSEVIER_J_PR = {Pattern Recognition} }
@String { IEEE_C_CVPR   = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)} }
@String { IEEE_C_ICCV   = {The IEEE International Conference on Computer Vision (ICCV)} }
@String { IEEE_J_CASVT  = {{IEEE} Transactions on Circuits and Systems for Video Technology} }
@String { IEEE_J_IP     = {{IEEE} Transactions on Image Processing} }
@String { IEEE_J_ITS    = {{IEEE} Transactions on Intelligent Transportation Systems} }
@String { IEEE_J_PAMI   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence} }

@InProceedings{l1loss,
author = {Girshick, Ross},
title = {Fast R-CNN},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@Article{Ait-Mohand2014TPAMI,
  Title                    = {Combining Structure and Parameter Adaptation of HMMs for Printed Text Recognition},
  Author                   = {Kamel Ait{-}Mohand and Thierry Paquet and Nicolas Ragot},
  Journal                  = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  Year                     = {2014},
  Number                   = {9},
  Pages                    = {1716--1732},
  Volume                   = {36},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/pami/Ait-MohandPR14},
  Doi                      = {10.1109/TPAMI.2014.2306423},
  Timestamp                = {Wed, 14 Jun 2017 20:35:31 +0200},
  Url                      = {https://doi.org/10.1109/TPAMI.2014.2306423}
}

@Article{Bahdanau2014CoRR,
  Title                    = {Neural Machine Translation by Jointly Learning to Align and Translate},
  Author                   = {Dzmitry Bahdanau and
 Kyunghyun Cho and
 Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1409.0473},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  Eprint                   = {1409.0473},
  Timestamp                = {Mon, 13 Aug 2018 01:00:00 +0200},
  Url                      = {http://arxiv.org/abs/1409.0473}
}

@Article{Bai2016TIP,
  Title                    = {Strokelets: {A} Learned Multi-Scale Mid-Level Representation for Scene Text Recognition},
  Author                   = {Xiang Bai and Cong Yao and Wenyu Liu},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2016},
  Number                   = {6},
  Pages                    = {2789--2802},
  Volume                   = {25},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/BaiYL16},
  Doi                      = {10.1109/TIP.2016.2555080},
  Timestamp                = {Thu, 27 Jul 2017 10:19:10 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2016.2555080}
}

@Article{Bhowmik2018IJDAR,
  Title                    = {Text and non-text separation in offline document images: a survey},
  Author                   = {Showmik Bhowmik and Ram Sarkar and Mita Nasipuri and David S. Doermann},
  Journal                  = {{IJDAR}},
  Year                     = {2018},
  Number                   = {1-2},
  Pages                    = {1--20},
  Volume                   = {21},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/ijdar/BhowmikSND18},
  Doi                      = {10.1007/s10032-018-0296-z},
  Timestamp                = {Tue, 29 May 2018 13:36:43 +0200},
  Url                      = {https://doi.org/10.1007/s10032-018-0296-z}
}

@Article{Blei2003JMLR,
  Title                    = {Latent Dirichlet Allocation},
  Author                   = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  Journal                  = {J. Mach. Learn. Res.},
  Year                     = {2003},

  Month                    = mar,
  Pages                    = {993--1022},
  Volume                   = {3},

  Acmid                    = {944937},
  ISSN                     = {1532-4435},
  Issue_date               = {3/1/2003},
  Numpages                 = {30},
  Publisher                = {JMLR.org},
  Url                      = {http://dl.acm.org/citation.cfm?id=944919.944937}
}

@InProceedings{Busta2015ICCV,
  Title                    = {FASText: Efficient Unconstrained Scene Text Detector},
  Author                   = {M. Busta and L. Neumann and J. Matas},
  Booktitle                = {2015 IEEE International Conference on Computer Vision (ICCV)},
  Year                     = {2015},
  Month                    = {Dec},
  Pages                    = {1206-1214},

  Doi                      = {10.1109/ICCV.2015.143},
  ISSN                     = {2380-7504},
  Keywords                 = {character recognition;feature extraction;image classification;image segmentation;FASText;unconstrained scene text detector;stroke detector;pixel intensity comparison;stroke-specific keypoint detection;text fragment extraction;local thresholding;classification;scene text localization;recognition pipeline;text localization accuracy;Detectors;Lead;Text recognition;Standards;Pipelines;Robustness;Image edge detection}
}

@InProceedings{deCampos2009VISAPP,
  Title                    = {Character recognition in natural images},
  Author                   = {de Campos, T.~E. and Babu, B.~R. and Varma, M.},
  Booktitle                = {Proceedings of the International Conference on Computer
 Vision Theory and Applications, Lisbon, Portugal},
  Year                     = {2009},
  Month                    = {February}
}

@Article{Canny1986TPAMI,
  Title                    = {A Computational Approach to Edge Detection},
  Author                   = {Canny, J},
  Journal                  = {IEEE Trans. Pattern Anal. Mach. Intell.},
  Year                     = {1986},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {679--698},
  Volume                   = {8},

  Acmid                    = {11275},
  Address                  = {Washington, DC, USA},
  Doi                      = {10.1109/TPAMI.1986.4767851},
  ISSN                     = {0162-8828},
  Issue_date               = {June 1986},
  Keywords                 = {Edge detection, feature extraction, image processing, machine vision, multiscale image analysis},
  Numpages                 = {20},
  Publisher                = {IEEE Computer Society},
  Url                      = {https://doi.org/10.1109/TPAMI.1986.4767851}
}

@InProceedings{Kheng2017ICDAR,
  Title                    = {Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition},
  Author                   = {Chee Kheng Ch'ng and Chee Seng Chan},
  Booktitle                = {14th IAPR International Conference on Document Analysis and Recognition {ICDAR}},
  Year                     = {2017},
  Pages                    = {935--942},

  Doi                      = {10.1109/ICDAR.2017.157}
}

@Article{CHEN2004595,
  Title                    = {Text detection and recognition in images and video frames},
  Author                   = {Datong Chen and Jean-Marc Odobez and Hervï¿½ Bourlard},
  Journal                  = ELSEVIER_J_PR,
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {595 - 608},
  Volume                   = {37},

  Abstract                 = {This paper presents a new method for detecting and recognizing text in complex images and video frames. Text detection is performed in a two-step approach that combines the speed of a text localization step, enabling text size normalization, with the strength of a machine learning text verification step applied on background independent features. Text recognition, applied on the detected text lines, is addressed by a text segmentation step followed by an traditional OCR algorithm within a multi-hypotheses framework relying on multiple segments, language modeling and OCR statistics. Experiments conducted on large databases of real broadcast documents demonstrate the validity of our approach.},
  Doi                      = {https://doi.org/10.1016/j.patcog.2003.06.001},
  ISSN                     = {0031-3203},
  Keywords                 = {Text localization, Text segmentation, Text recognition, SVM, MRF, Video OCR},
  Timestamp                = {2018.08.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0031320303002462}
}

@Article{Cheng2017arXiv-survey,
  Title                    = {A Survey of Model Compression and Acceleration for Deep Neural Networks},
  Author                   = {Yu Cheng and Duo Wang and Pan Zhou and Tao Zhang},
  Journal                  = {CoRR},
  Year                     = {2017},
  Volume                   = {abs/1710.09282},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/corr/abs-1710-09282},
  Eprint                   = {1710.09282},
  Timestamp                = {Mon, 08 Jan 2018 16:19:01 +0100},
  Url                      = {http://arxiv.org/abs/1710.09282}
}

@InProceedings{Cho2016CVPR,
  Title                    = {Canny Text Detector: Fast and Robust Scene Text Localization Algorithm},
  Author                   = {H. Cho and M. Sung and B. Jun},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2016},
  Month                    = {June},
  Pages                    = {3566-3573},

  Doi                      = {10.1109/CVPR.2016.388},
  ISSN                     = {1063-6919},
  Keywords                 = {object detection;Canny text detector;scene text localization algorithm;scene text detection algorithm;recall rate;structural information;word-sentence sharing;double threshold;hysteresis tracking;Erbium;Image edge detection;Detectors;Robustness;Hysteresis;Stability criteria;Vegetation}
}

@Article{Chorowski2015CORR,
  Title                    = {Attention-Based Models for Speech Recognition},
  Author                   = {Jan Chorowski and Dzmitry Bahdanau and Dmitriy Serdyuk and KyungHyun Cho and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1506.07503},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/corr/ChorowskiBSCB15},
  Eprint                   = {1506.07503},
  Timestamp                = {Mon, 13 Aug 2018 16:47:01 +0200},
  Url                      = {http://arxiv.org/abs/1506.07503}
}

@InProceedings{Clavelli2010IAPR,
  Title                    = {A Framework for the Assessment of Text Extraction Algorithms on Complex Colour Images},
  Author                   = {Clavelli, A. and Karatzas, D. and Llad\'{o}s, J.},
  Booktitle                = {Proceedings of the 9th IAPR International Workshop on Document Analysis Systems},
  Year                     = {2010},

  Address                  = {New York, NY, USA},
  Pages                    = {19--26},
  Publisher                = {ACM},
  Series                   = {DAS '10},

  Acmid                    = {1815333},
  Doi                      = {10.1145/1815330.1815333},
  ISBN                     = {978-1-60558-773-8},
  Keywords                 = {colour, performance evaluation, text extraction},
  Location                 = {Boston, Massachusetts, USA},
  Numpages                 = {8},
  Url                      = {http://doi.acm.org/10.1145/1815330.1815333}
}

@Article{Crandall2003,
  Title                    = {Extraction of special effects caption text events from digital video},
  Author                   = {Crandall, David and Antani, Sameer and Kasturi, Rangachar},
  Journal                  = {International Journal on Document Analysis and Recognition},
  Year                     = {2003},

  Month                    = {Apr},
  Number                   = {2},
  Pages                    = {138--157},
  Volume                   = {5},

  Day                      = {01},
  Doi                      = {10.1007/s10032-002-0091-7},
  ISSN                     = {1433-2833},
  Url                      = {https://doi.org/10.1007/s10032-002-0091-7}
}

@InProceedings{Dalal2005CVPR,
  Title                    = {Histograms of Oriented Gradients for Human Detection},
  Author                   = {Dalal, Navneet and Triggs, Bill},
  Booktitle                = {Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1 - Volume 01},
  Year                     = {2005},

  Address                  = {Washington, DC, USA},
  Pages                    = {886--893},
  Publisher                = {IEEE Computer Society},
  Series                   = {CVPR '05},

  Acmid                    = {1069007},
  Doi                      = {10.1109/CVPR.2005.177},
  ISBN                     = {0-7695-2372-2},
  Numpages                 = {8},
  Url                      = {http://dx.doi.org/10.1109/CVPR.2005.177}
}

@Article{Dollar2014TPAMI,
  Title                    = {Fast Feature Pyramids for Object Detection},
  Author                   = {P. DollÃ¡r and R. Appel and S. Belongie and P. Perona},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2014},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {1532-1545},
  Volume                   = {36},

  Doi                      = {10.1109/TPAMI.2014.2300479},
  ISSN                     = {0162-8828}
}

@InProceedings{Epshtein2010CVPR,
  Title                    = {Detecting text in natural scenes with stroke width transform},
  Author                   = {B. Epshtein and E. Ofek and Y. Wexler},
  Booktitle                = IEEE_C_CVPR,
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {2963-2970},

  Doi                      = {10.1109/CVPR.2010.5540041},
  ISSN                     = {1063-6919},
  Keywords                 = {image processing;text analysis;natural scenes;stroke width transform;image operator;text detection;image pixel;natural images;Layout;Optical character recognition software;Pixel;Computer vision;Filter bank;Engines;Image segmentation;Colored noise;Geometry;Robustness},
  Timestamp                = {2018.08.12}
}

@InProceedings{Escalera2009,
  Title                    = {Text Detection in Urban Scenes.},
  Author                   = {Escalera, Sergio and Bar{\'o}, Xavier and Vitri{\`a}, Jordi and Radeva, Petia},
  Year                     = {2009}
}

@Article{Everingham2010IJCV,
  Title                    = {The Pascal Visual Object Classes (VOC) Challenge},
  Author                   = {Everingham, Mark
and Van Gool, Luc
and Williams, Christopher K. I.
and Winn, John
and Zisserman, Andrew},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2010},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {303--338},
  Volume                   = {88},

  Abstract                 = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
  Day                      = {01},
  Doi                      = {10.1007/s11263-009-0275-4},
  ISSN                     = {1573-1405},
  Url                      = {https://doi.org/10.1007/s11263-009-0275-4}
}

@InProceedings{Galteri2017ICCVW,
  Title                    = {Reading Text in the Wild from Compressed Images},
  Author                   = {L. Galteri and D. Bazazian and L. Seidenari and M. Bertini and A. D. Bagdanov and A. Nicolaou and D. Karatzas and A. D. Bimbo},
  Booktitle                = {2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
  Year                     = {2017},
  Month                    = {Oct},
  Pages                    = {2399-2407},

  Doi                      = {10.1109/ICCVW.2017.283},
  ISSN                     = {2473-9944},
  Keywords                 = {computer vision;data compression;feedforward neural nets;image coding;image colour analysis;image recognition;image texture;text localization;compressed images;computer vision community;deep convolutional neural network;CNN;ICDAR-Challenge4 dataset;text recognition;text-specific compression artifacts;captured images;distort image content;Image coding;Text recognition;Image color analysis;Proposals;Transform coding;Discrete cosine transforms}
}

@Article{Gomez2017PR,
  Title                    = {TextProposals: a Text-specific Selective Search Algorithm for Word Spotting in the Wild},
  Author                   = {Gomez, Lluis and Karatzas, Dimosthenis},
  Journal                  = {Pattern Recognition},
  Year                     = {2017},

  Month                    = {10},
  Pages                    = {60-74},
  Volume                   = {70},

  Doi                      = {10.1016/j.patcog.2017.04.027}
}

@Article{Gonzalez2013IVC,
  Title                    = {A text reading algorithm for natural images},
  Author                   = {{\'{A}}lvaro Gonzalez and Luis Miguel Bergasa},
  Journal                  = {Image Vision Comput.},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {255--274},
  Volume                   = {31},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/ivc/GonzalezB13},
  Doi                      = {10.1016/j.imavis.2013.01.003},
  Timestamp                = {Tue, 06 Jun 2017 22:17:27 +0200},
  Url                      = {https://doi.org/10.1016/j.imavis.2013.01.003}
}

@Article{Gonzalez2014TITS,
  Title                    = {Text Detection and Recognition on Traffic Panels From Street-Level Imagery Using Visual Appearance},
  Author                   = {{\'{A}}lvaro Gonzalez and Luis Miguel Bergasa and Jos{\'{e}} Javier Yebes Torres},
  Journal                  = {{IEEE} Trans. Intelligent Transportation Systems},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {228--238},
  Volume                   = {15},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tits/GonzalezBT14},
  Doi                      = {10.1109/TITS.2013.2277662},
  Timestamp                = {Thu, 15 Jun 2017 21:30:53 +0200},
  Url                      = {https://doi.org/10.1109/TITS.2013.2277662}
}

@InProceedings{Gonzalez2012ICPR,
  Title                    = {Text location in complex images},
  Author                   = {Gonzalez, Alvaro and Bergasa, Luis M and Yebes, J Javier and Bronte, Sebasti{\'a}n},
  Booktitle                = {Pattern Recognition (ICPR), 2012 21st International Conference on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {617--620}
}

@Article{Greenhalgh2015TITS,
  Title                    = {Recognizing Text-Based Traffic Signs},
  Author                   = {Jack Greenhalgh and Majid Mirmehdi},
  Journal                  = {{IEEE} Trans. Intelligent Transportation Systems},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {1360--1369},
  Volume                   = {16},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tits/GreenhalghM15},
  Doi                      = {10.1109/TITS.2014.2363167},
  Timestamp                = {Wed, 04 Jul 2018 13:22:12 +0200},
  Url                      = {https://doi.org/10.1109/TITS.2014.2363167}
}

@InProceedings{Gupta2016CVPR,
  Title                    = {Synthetic Data for Text Localisation in Natural Images},
  Author                   = {A. Gupta and A. Vedaldi and A. Zisserman},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2016},
  Month                    = {June},
  Pages                    = {2315-2324},

  Doi                      = {10.1109/CVPR.2016.254},
  ISSN                     = {1063-6919},
  Keywords                 = {computational geometry;learning (artificial intelligence);neural nets;object detection;regression analysis;text detection;text localisation;synthetic image generation;synthetic text;background images;local 3D scene geometry;fully-convolutional regression network training;FCRN training;text detection;bounding-box regression;FCRN;YOLO detector;end-to-end object detection systems;deep learning;natural images;F-measure;standard ICDAR 2013 benchmark;Image color analysis;Pipelines;Proposals;Image segmentation;Detectors;Text recognition;Feature extraction}
}

@InProceedings{Hanif2009ICDAR,
  Title                    = {Text Detection and Localization in Complex Scene Images using Constrained AdaBoost Algorithm},
  Author                   = {S. M. Hanif and L. Prevost},
  Booktitle                = {2009 10th International Conference on Document Analysis and Recognition},
  Year                     = {2009},
  Month                    = {July},
  Pages                    = {1-5},

  Doi                      = {10.1109/ICDAR.2009.172},
  ISSN                     = {1520-5363},
  Keywords                 = {image sequences;learning (artificial intelligence);natural scenes;neural nets;object detection;text analysis;video signal processing;text detection;text localization;constrained AdaBoost algorithm;gray scale scene image;heterogeneous feature set;neural network based-localization;text locating database;video sequence;natural scene;Layout;Detectors;Image segmentation;Colored noise;Histograms;Intelligent robots;Neural networks;Robustness;Spatial databases;Video sequences;Text detection and localization;AdaBoost;Cascade of Boosted Ensembles;Feature selection;Feature combination;Feature complexity}
}

@InProceedings{He2017CVPR,
  Title                    = {Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting in the Wild},
  Author                   = {D. He and X. Yang and C. Liang and Z. Zhou and A. G. Ororbia and D. Kifer and C. L. Giles},
  Booktitle                = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2017},
  Month                    = {July},
  Pages                    = {474-483},

  Doi                      = {10.1109/CVPR.2017.58},
  ISSN                     = {1063-6919},
  Keywords                 = {document image processing;image segmentation;neural nets;text analysis;text detection;arbitrary orientations;curved text lines;multiscale FCN;cascaded instance aware segmentation;arbitrary oriented word spotting;multiscale fully convolutional neural network;text block region extraction;street view text benchmark datasets;scene text detection algorithm;ICDAR 2013;ICDAR 2015;CUTE80;Feature extraction;Proposals;Image segmentation;Training;Neural networks;Algorithm design and analysis}
}

@InProceedings{He2016AAAI,
  Title                    = {Reading Scene Text in Deep Convolutional Sequences},
  Author                   = {He, Pan and Huang, Weilin and Qiao, Yu and Loy, Chen Change and Tang, Xiaoou},
  Booktitle                = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  Year                     = {2016},
  Pages                    = {3501--3508},
  Publisher                = {AAAI Press},
  Series                   = {AAAI'16},

  Acmid                    = {3016396},
  Location                 = {Phoenix, Arizona},
  Numpages                 = {8},
  Url                      = {http://dl.acm.org/citation.cfm?id=3016387.3016396}
}

@Article{He2016TIP,
  Title                    = {Text-Attentional Convolutional Neural Network for Scene Text Detection},
  Author                   = {T. He and W. Huang and Y. Qiao and J. Yao},
  Journal                  = IEEE_J_IP,
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {2529-2541},
  Volume                   = {25},

  Doi                      = {10.1109/TIP.2016.2547588},
  ISSN                     = {1057-7149},
  Keywords                 = {image enhancement;neural nets;text detection;text-attentional convolutional neural network;scene text detection;deep learning models;text region mask;character label;binary text/non-text information;ambiguous texts;contrast-enhancement maximally stable extremal regions;ICDAR 2013 dataset;Detectors;Feature extraction;Robustness;Computational modeling;Neural networks;Text recognition;Training;Maximally Stable Extremal Regions;text detector;convolutional neural networks;multi-level supervised information;multi-task learning;Maximally stable extremal regions;text detector;convolutional neural networks;multi-level supervised information;multi-task learning},
  Timestamp                = {2018.08.12}
}

@Article{He2018TIP,
  Title                    = {Multi-Oriented and Multi-Lingual Scene Text Detection With Direct Regression},
  Author                   = {W. He and X. Zhang and F. Yin and C. Liu},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2018},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {5406-5419},
  Volume                   = {27},

  Doi                      = {10.1109/TIP.2018.2855399},
  ISSN                     = {1057-7149},
  Keywords                 = {computer vision;edge detection;feature extraction;feedforward neural nets;image classification;image sampling;image segmentation;object detection;regression analysis;text detection;long text lines;multilingual scene text datasets;convolutional neural network-based object detection frameworks;downsampled segmentation-based module;scene text detection framework;quadrilateral text boundaries;word-level detection;line-level detection;nontext classification;multioriented-multilingual scene text detection;direct regression;computer vision;high performance CNN-based scene text detection systems;bitask prediction module;pixel-wise classification;Feature extraction;Task analysis;Proposals;Manuals;Object detection;Detectors;Stress;Fully convolutional network;scene text detection;multi-oriented;multi-task}
}

@Book{Horn1986McGrawHill,
  Title                    = {Robot Vision},
  Author                   = {Berthold Klaus Paul Horn},
  Publisher                = {McGraw-Hill Book Company, Cambridge, Massachusetts},
  Year                     = {1986},
  Note                     = {MIT Electrical Engineering and Computer Science Series}
}

@Article{Hua2004TCSVT,
  Title                    = {An automatic performance evaluation protocol for video text detection algorithms},
  Author                   = {Xian-Sheng Hua and Liu Wenyin and Hong-Jiang Zhang},
  Journal                  = {IEEE Transactions on Circuits and Systems for Video Technology},
  Year                     = {2004},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {498-507},
  Volume                   = {14},

  Doi                      = {10.1109/TCSVT.2004.825538},
  ISSN                     = {1051-8215},
  Keywords                 = {video signal processing;protocols;text analysis;image retrieval;content-based retrieval;indexing;automatic performance evaluation protocol;video text detection algorithm;video indexing;video retrieval;detection difficulty level;detectability index;Protocols;Detection algorithms;Indexing;Information retrieval;Algorithm design and analysis;Content based retrieval;Image analysis;Information analysis;Data mining}
}

@InProceedings{Huang2016MM,
  Title                    = {Detecting Arbitrary Oriented Text in the Wild with a Visual Attention Model},
  Author                   = {Huang, Wenyi and He, Dafang and Yang, Xiao and Zhou, Zihan and Kifer, Daniel and Giles, C. Lee},
  Booktitle                = {Proceedings of the 2016 ACM on Multimedia Conference},
  Year                     = {2016},

  Address                  = {New York, NY, USA},
  Pages                    = {551--555},
  Publisher                = {ACM},
  Series                   = {MM '16},

  Acmid                    = {2967282},
  Doi                      = {10.1145/2964284.2967282},
  ISBN                     = {978-1-4503-3603-1},
  Keywords                 = {deep learning, scene text detection, visual attention},
  Location                 = {Amsterdam, The Netherlands},
  Numpages                 = {5},
  Url                      = {http://doi.acm.org/10.1145/2964284.2967282}
}

@InProceedings{Ioffe2015ICML,
  Title                    = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  Author                   = {Sergey Ioffe and Christian Szegedy},
  Booktitle                = {Proceedings of the 32nd International Conference on Machine Learning},
  Year                     = {2015},

  Address                  = {Lille, France},
  Editor                   = {Francis Bach and David Blei},
  Month                    = {07--09 Jul},
  Pages                    = {448--456},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {37},

  Pdf                      = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  Url                      = {http://proceedings.mlr.press/v37/ioffe15.html}
}

@InProceedings{Iwamura2016ICDAR,
  Title                    = {Downtown Osaka Scene Text Dataset},
  Author                   = {Iwamura, Masakazu
and Matsuda, Takahiro
and Morimoto, Naoyuki
and Sato, Hitomi
and Ikeda, Yuki
and Kise, Koichi},
  Booktitle                = {Computer Vision -- ECCV 2016 Workshops},
  Year                     = {2016},

  Address                  = {Cham},
  Editor                   = {Hua, Gang
and J{\'e}gou, Herv{\'e}},
  Pages                    = {440--455},
  Publisher                = {Springer International Publishing},

  Abstract                 = {This paper presents a new scene text dataset named Downtown Osaka Scene Text Dataset (in short, DOST dataset). The dataset consists of sequential images captured in shopping streets in downtown Osaka with an omnidirectional camera. Unlike most of existing datasets consisting of scene images intentionally captured, DOST dataset consists of uncontrolled scene images; use of an omnidirectional camera enabled us to capture videos (sequential images) of whole scenes surrounding the camera. Since the dataset preserved the real scenes containing texts as they were, in other words, they are scene texts in the wild. DOST dataset contained 32,147 manually ground truthed sequential images. They contained 935,601 text regions consisting of 797,919 legible and 137,682 illegible. The legible regions contained 2,808,340 characters. The dataset is evaluated using two existing scene text detection methods and one powerful commercial end-to-end scene text recognition method to know the difficulty and quality in comparison with existing datasets.},
  ISBN                     = {978-3-319-46604-0}
}

@Article{Jaderberg2016IJCV,
  Title                    = {Reading Text in the Wild with Convolutional Neural Networks},
  Author                   = {Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  Journal                  = {Int. J. Comput. Vision},
  Year                     = {2016},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {1--20},
  Volume                   = {116},

  Acmid                    = {2877148},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1007/s11263-015-0823-z},
  ISSN                     = {0920-5691},
  Issue_date               = {January 2016},
  Keywords                 = {Convolutional neural networks, Deep learning, Synthetic data, Text detection, Text recognition, Text retrieval, Text spotting},
  Numpages                 = {20},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dx.doi.org/10.1007/s11263-015-0823-z}
}

@Article{Jaderberg2014CoRR,
  Title                    = {Deep Structured Output Learning for Unconstrained Text Recognition},
  Author                   = {Jaderberg, M. and Simonyan, K. and Vedaldi, A. and Zisserman, A.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2014},

  Month                    = dec,

  Archiveprefix            = {arXiv},
  Eprint                   = {1412.5903},
  Keywords                 = {Computer Science - Computer Vision and Pattern Recognition},
  Primaryclass             = {cs.CV}
}

@Article{Jaderberg2014CoRRb,
  Title                    = {Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition},
  Author                   = {Max Jaderberg and Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1406.2227},

  Url                      = {http://arxiv.org/abs/1406.2227}
}

@Article{Jaderberg2015CORR,
  Title                    = {Spatial Transformer Networks},
  Author                   = {Max Jaderberg and
 Karen Simonyan and
 Andrew Zisserman and
 Koray Kavukcuoglu},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1506.02025},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/corr/JaderbergSZK15},
  Eprint                   = {1506.02025},
  Timestamp                = {Mon, 13 Aug 2018 16:49:02 +0200},
  Url                      = {http://arxiv.org/abs/1506.02025}
}

@InProceedings{Jaderberg2014ECCV,
  Title                    = {Deep Features for Text Spotting},
  Author                   = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  Booktitle                = {Computer Vision -- ECCV 2014},
  Year                     = {2014},

  Address                  = {Cham},
  Editor                   = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  Pages                    = {512--528},
  Publisher                = {Springer International Publishing},

  ISBN                     = {978-3-319-10593-2}
}

@InProceedings{Jetley2016CVPR,
  Title                    = {End-to-End Saliency Mapping via Probability Distribution Prediction},
  Author                   = {S. Jetley and N. Murray and E. Vig},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2016},
  Month                    = {June},
  Pages                    = {5753-5761},

  Doi                      = {10.1109/CVPR.2016.620},
  ISSN                     = {1063-6919},
  Keywords                 = {face recognition;regression analysis;statistical distributions;end-to-end saliency mapping;probability distribution prediction;saliency estimation methods;low-level conspicuity cues;blobs;face detection;text detection;saliency models;eye-fixation data;deep architectures;regression tasks;topographical maps;saliency map model;generalized Bernoulli distribution;softmax activation function;public benchmark datasets;Computational modeling;Training;Computer architecture;Loss measurement;Probability distribution;Estimation;Standards}
}

@Article{Jiang2017CoRR,
  Title                    = {R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection},
  Author                   = {{Jiang}, Y. and {Zhu}, X. and {Wang}, X. and {Yang}, S. and
 {Li}, W. and {Wang}, H. and {Fu}, P. and {Luo}, Z.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2017},

  Month                    = jun,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2017arXiv170609579J},
  Archiveprefix            = {arXiv},
  Eprint                   = {1706.09579},
  Keywords                 = {Computer Science - Computer Vision and Pattern Recognition},
  Primaryclass             = {cs.CV}
}

@Article{Jung2004PR,
  Title                    = {Text information extraction in images and video: a survey},
  Author                   = {Keechul Jung and Kwang In Kim and Anil K. Jain},
  Journal                  = {Pattern Recognition},
  Year                     = {2004},
  Number                   = {5},
  Pages                    = {977--997},
  Volume                   = {37},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/pr/JungKJ04},
  Doi                      = {10.1016/j.patcog.2003.10.012},
  Timestamp                = {Fri, 27 Oct 2017 15:34:05 +0200},
  Url                      = {https://doi.org/10.1016/j.patcog.2003.10.012}
}

@InProceedings{Kang2017AAAI,
  Title                    = {Detection and Recognition of Text Embedded in Online Images via Neural Context Models},
  Author                   = {Chulmoo Kang and Gunhee Kim and Suk I. Yoo},
  Booktitle                = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, {USA.}},
  Year                     = {2017},
  Pages                    = {4103--4110},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/conf/aaai/KangKY17},
  Timestamp                = {Mon, 06 Mar 2017 11:36:24 +0100},
  Url                      = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14154}
}

@InProceedings{Kang2014CVPR,
  Title                    = {Orientation Robust Text Line Detection in Natural Images},
  Author                   = {Le Kang and Yi Li and David S. Doermann},
  Booktitle                = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2014, Columbus, OH, USA, June 23-28, 2014},
  Year                     = {2014},
  Pages                    = {4034--4041},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/conf/cvpr/KangLD14},
  Doi                      = {10.1109/CVPR.2014.514},
  Timestamp                = {Thu, 25 May 2017 00:41:21 +0200},
  Url                      = {https://doi.org/10.1109/CVPR.2014.514}
}

@Article{Karaoglu2017TIP,
  Title                    = {Con-Text: Text Detection for Fine-Grained Object Classification},
  Author                   = {Sezer Karaoglu and Ran Tao and Jan C. van Gemert and Theo Gevers},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2017},
  Number                   = {8},
  Pages                    = {3965--3980},
  Volume                   = {26},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/KaraogluTGG17},
  Doi                      = {10.1109/TIP.2017.2707805},
  Timestamp                = {Wed, 26 Jul 2017 18:02:03 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2017.2707805}
}

@InProceedings{Karatzas2015ICDAR,
  Title                    = {ICDAR 2015 competition on Robust Reading},
  Author                   = {D. Karatzas and L. Gomez-Bigorda and A. Nicolaou and S. Ghosh and A. Bagdanov and M. Iwamura and J. Matas and L. Neumann and V. R. Chandrasekhar and S. Lu and F. Shafait and S. Uchida and E. Valveny},
  Booktitle                = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
  Year                     = {2015},
  Month                    = {Aug},
  Pages                    = {1156-1160},

  Doi                      = {10.1109/ICDAR.2015.7333942},
  Keywords                 = {image sequences;text detection;video signal processing;ICDAR 2015 robust reading competition;incidental scene text;born-digital images;focused scene images;video text;text localisation;word recognition;end-to-end pipelines;video sequences;Yttrium;IP networks}
}

@InProceedings{Karatzas2011ICDAR,
  Title                    = {ICDAR 2011 Robust Reading Competition - Challenge 1: Reading Text in Born-Digital Images (Web and Email)},
  Author                   = {D. Karatzas and S. R. Mestre and J. Mas and F. Nourbakhsh and P. P. Roy},
  Booktitle                = {2011 International Conference on Document Analysis and Recognition},
  Year                     = {2011},
  Month                    = {Sept},
  Pages                    = {1485-1490},

  Doi                      = {10.1109/ICDAR.2011.295},
  ISSN                     = {2379-2140},
  Keywords                 = {electronic mail;feature extraction;image recognition;image segmentation;Internet;text analysis;ICDAR 2011 Robust Reading Competition;born digital images;Web pages;emails;text localization;text segmentation;word recognition;Image segmentation;Text recognition;Feature extraction;Measurement;Image edge detection;Electronic mail;Web images;email images;born-digital;text extraction;localization;segmentation;recognition}
}

@InProceedings{Karatzas2013ICDAR,
  Title                    = {ICDAR 2013 Robust Reading Competition},
  Author                   = {Karatzas, Dimosthenis and Shafait, Faisal and Uchida, Seiichi and Iwamura, Masakazu and Bigorda, Lluis Gomez i. and Mestre, Sergi Robles and Mas, Joan and Mota, David Fernandez and Almaz\`{a}n, Jon Almaz\`{a}n and de las Heras, Llu\'{\i}s Pere},
  Booktitle                = {Proceedings of the 2013 12th International Conference on Document Analysis and Recognition},
  Year                     = {2013},

  Address                  = {Washington, DC, USA},
  Pages                    = {1484--1493},
  Series                   = {ICDAR '13},

  Acmid                    = {2549448},
  Doi                      = {10.1109/ICDAR.2013.221},
  ISBN                     = {978-0-7695-4999-6},
  Keywords                 = {robust reading, scene text, text extraction, text localization, text segmentation, text recognition, video},
  Numpages                 = {10},
}

@Article{Kasturi2008PAMI,
  Title                    = {Framework for Performance Evaluation of Face, Text, and Vehicle Detection and Tracking in Video: Data, Metrics, and Protocol},
  Author                   = {R. Kasturi and D. Goldgof and P. Soundararajan and V. Manohar and J. Garofolo and R. Bowers and M. Boonstra and V. Korzhova and J. Zhang},
  Journal                  = IEEE_J_PAMI,
  Year                     = {2009},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {319-336},
  Volume                   = {31},

  Abstract                 = {Common benchmark datasets, standardized performance metrics, and baseline algorithms have demonstrated considerable impact on research and development in a variety of application domains. These resources provide both consumers and developers of technology with a common framework to objectively compare the performance of different algorithms and algorithmic improvements. In this paper, we present such a framework for evaluating object detection and tracking in video: specifically for face, text, and vehicle objects. This framework includes the source video data, ground-truth annotations (along with guidelines for annotation), performance metrics, evaluation protocols, and tools including scoring software and baseline algorithms. For each detection and tracking task and supported domain, we developed a 50-clip training set and a 50-clip test set. Each data clip is approximately 2.5 minutes long and has been completely spatially/temporally annotated at the I-frame level. Each task/domain, therefore, has an associated annotated corpus of approximately 450,000 frames. The scope of such annotation is unprecedented and was designed to begin to support the necessary quantities of data for robust machine learning approaches, as well as a statistically significant comparison of the performance of algorithms. The goal of this work was to systematically address the challenges of object detection and tracking through a common evaluation framework that permits a meaningful objective comparison of techniques, provides the research community with sufficient data for the exploration of automatic modeling techniques, encourages the incorporation of objective evaluation into the development process, and contributes useful lasting resources of a scale and magnitude that will prove to be extremely useful to the computer vision research community for years to come.},
  Doi                      = {10.1109/TPAMI.2008.57},
  ISSN                     = {0162-8828},
  Keywords                 = {face recognition;learning (artificial intelligence);object detection;optical character recognition;protocols;vehicles;video signal processing;object detection;performance evaluation protocol;vehicle detection;face detection;text detection;object tracking;video data;performance metrics;baseline algorithm;scoring software;machine learning;Face detection;Vehicle detection;Protocols;Measurement;Object detection;Research and development;Vehicles;Guidelines;Software performance;Software tools;Performance evaluation;object detection and tracking;baseline algorithms;face;text;vehicle.;Tracking;Miscellaneous;Video analysis;performance evaluation;object detection and tracking;Algorithms;Artificial Intelligence;Automatic Data Processing;Face;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Motor Vehicles;Pattern Recognition, Automated;Sensitivity and Specificity;Subtraction Technique;Video Recording},
  Timestamp                = {2018.08.12}
}

@Article{Koo2016TIP,
  Title                    = {Text-Line Detection in Camera-Captured Document Images Using the State Estimation of Connected Components},
  Author                   = {Hyung Il Koo},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2016},
  Number                   = {11},
  Pages                    = {5358--5368},
  Volume                   = {25},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/Koo16},
  Doi                      = {10.1109/TIP.2016.2607418},
  Timestamp                = {Thu, 27 Jul 2017 10:19:11 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2016.2607418}
}

@Article{Koo2013TIP,
  Title                    = {Scene Text Detection via Connected Component Clustering and Nontext Filtering},
  Author                   = {H. I. Koo and D. H. Kim},
  Journal                  = IEEE_J_IP,
  Year                     = {2013},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {2296-2305},
  Volume                   = {22},

  Doi                      = {10.1109/TIP.2013.2249082},
  ISSN                     = {1057-7149},
  Keywords                 = {heuristic programming;image classification;multilayer perceptrons;text detection;scene text detection algorithm;component clustering;nontext filtering;CC;machine learning classifiers;heuristic rules;pairwise relations;normalized images;text-nontext classifier;multilayer perceptrons;multichannel information;ICDAR 2005;ICDAR 2011;Clustering algorithms;Image color analysis;Training;Feature extraction;Partitioning algorithms;Robustness;Algorithm design and analysis;CC clustering;connected component (CC)-based approach;machine learning classifier;nontext filtering;scene text detection;Algorithms;Artificial Intelligence;Cluster Analysis;Databases, Factual;Image Processing, Computer-Assisted;Pattern Recognition, Automated},
  Timestamp                = {2018.09.24}
}

@InProceedings{Kumar2014ICACCT,
  Title                    = {A Detailed Review of Feature Extraction in Image Processing Systems},
  Author                   = {G. Kumar and P. K. Bhatia},
  Booktitle                = {2014 Fourth International Conference on Advanced Computing Communication Technologies},
  Year                     = {2014},
  Month                    = {Feb},
  Pages                    = {5-12},

  Abstract                 = {Feature plays a very important role in the area of image processing. Before getting features, various image preprocessing techniques like binarization, thresholding, resizing, normalization etc. are applied on the sampled image. After that, feature extraction techniques are applied to get features that will be useful in classifying and recognition of images. Feature extraction techniques are helpful in various image processing applications e.g. character recognition. As features define the behavior of an image, they show its place in terms of storage taken, efficiency in classification and obviously in time consumption also. Here in this paper, we are going to discuss various types of features, feature extraction techniques and explaining in what scenario, which features extraction technique, will be better. Hereby in this paper, we are going to refer features and feature extraction methods in case of character recognition application.},
  Doi                      = {10.1109/ACCT.2014.74},
  ISSN                     = {2327-0632},
  Keywords                 = {feature extraction;image classification;optical character recognition;feature extraction;image processing systems;image classification;image recognition;character recognition application;Feature extraction;Shape;Transforms;Vectors;Character recognition;Data mining;Feature Extraction;Image Processing;Optical Character Recognition (OCR);Pattern Recognition}
}

@InProceedings{Lee2014CVPR,
  Title                    = {Region-Based Discriminative Feature Pooling for Scene Text Recognition},
  Author                   = {Chen{-}Yu Lee and Anurag Bhardwaj and Wei Di and Vignesh Jagadeesh and Robinson Piramuthu},
  Booktitle                = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2014, Columbus, OH, USA, June 23-28, 2014},
  Year                     = {2014},
  Pages                    = {4050--4057},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/conf/cvpr/LeeBDJP14},
  Doi                      = {10.1109/CVPR.2014.516},
  Timestamp                = {Thu, 25 May 2017 00:41:15 +0200},
  Url                      = {https://doi.org/10.1109/CVPR.2014.516}
}

@InProceedings{Lee2016CVPR,
  Title                    = {Recursive Recurrent Nets with Attention Modeling for OCR in the Wild},
  Author                   = {C. Lee and S. Osindero},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2016},
  Month                    = {June},
  Pages                    = {2231-2239},

  Doi                      = {10.1109/CVPR.2016.245},
  ISSN                     = {1063-6919},
  Keywords                 = {backpropagation;feature extraction;optical character recognition;recurrent neural nets;OCR;recursive recurrent neural networks with attention modeling;R2AM;optical character recognition;natural scene images;convolutional neural networks;CNN;image feature extraction;learned character-level language model;end-to-end training;standard backpropagation framework;street view text;benchmark datasets;IIIT5k;ICDAR;Synth90k;Feature extraction;Optical character recognition software;Predictive models;Recurrent neural networks;Character recognition;Text recognition;Computational modeling}
}

@InProceedings{Lee2010ICPR,
  Title                    = {Scene Text Extraction with Edge Constraint and Text Collinearity},
  Author                   = {S. Lee and M. S. Cho and K. Jung and J. H. Kim},
  Booktitle                = {2010 20th International Conference on Pattern Recognition},
  Year                     = {2010},
  Month                    = {Aug},
  Pages                    = {3983-3986},

  Doi                      = {10.1109/ICPR.2010.969},
  ISSN                     = {1051-4651},
  Keywords                 = {edge detection;feature extraction;image colour analysis;image segmentation;Markov processes;natural scenes;pattern clustering;random processes;text analysis;scene text extraction;edge constraint;text collinearity;natural scene images;text region candidates;k-means clustering algorithm;texture features;edge information;color information;Markov random field model;collinearity weight;ICDAR 2003 database;Image color analysis;Pixel;Image edge detection;Clustering algorithms;Image segmentation;Shape;Lighting;scene text extraction;color clustering;markov random field}
}

@Article{Li2014TIP,
  Title                    = {Characterness: An Indicator of Text in the Wild},
  Author                   = {Yao Li and Wenjing Jia and Chunhua Shen and Anton van den Hengel},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2014},
  Number                   = {4},
  Pages                    = {1666--1677},
  Volume                   = {23},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/LiJSH14},
  Doi                      = {10.1109/TIP.2014.2302896},
  Timestamp                = {Tue, 20 Mar 2018 14:21:13 +0100},
  Url                      = {https://doi.org/10.1109/TIP.2014.2302896}
}

@Article{Liang2015TIP,
  Title                    = {Multi-Spectral Fusion Based Approach for Arbitrarily Oriented Scene Text Detection in Video Images},
  Author                   = {Guozhu Liang and Palaiahnakote Shivakumara and Tong Lu and Chew Lim Tan},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2015},
  Number                   = {11},
  Pages                    = {4488--4501},
  Volume                   = {24},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/LiangSLT15},
  Doi                      = {10.1109/TIP.2015.2465169},
  Timestamp                = {Fri, 26 May 2017 22:51:53 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2015.2465169}
}

@Article{Liang2005IJDAR,
  Title                    = {Camera-based analysis of text and documents: a survey},
  Author                   = {Jian Liang and David S. Doermann and Huiping Li},
  Journal                  = {{IJDAR}},
  Year                     = {2005},
  Number                   = {2-3},
  Pages                    = {84--104},
  Volume                   = {7},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/ijdar/LiangDL05},
  Doi                      = {10.1007/s10032-004-0138-z},
  Timestamp                = {Wed, 17 May 2017 14:25:22 +0200},
  Url                      = {https://doi.org/10.1007/s10032-004-0138-z}
}

@Article{Liao2018TIP,
  Title                    = {TextBoxes++: A Single-Shot Oriented Scene Text Detector},
  Author                   = {M. Liao and B. Shi and X. Bai},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2018},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {3676-3690},
  Volume                   = {27},

  Doi                      = {10.1109/TIP.2018.2825107},
  ISSN                     = {1057-7149},
  Keywords                 = {image recognition;text detection;scene text detection;scene text recognition system;fast scene text detector;arbitrary-oriented scene text;1024 ICDAR 2015 incidental text images;768 Ã 768 COCO-Text images;text recognizer;end-to-end text recognition tasks;text localization;Text detector;TextBoxes;Text recognition;Object detection;Detectors;Feature extraction;Proposals;Image recognition;Runtime;Scene text detection;multi-oriented text;word spotting;scene text recognition;convolutional neural networks}
}

@InProceedings{Liao2017AAAI,
  Title                    = {TextBoxes: {A} Fast Text Detector with a Single Deep Neural Network},
  Author                   = {Minghui Liao and Baoguang Shi and Xiang Bai and Xinggang Wang and Wenyu Liu},
  Booktitle                = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
 February 4-9, 2017, San Francisco, California, {USA.}},
  Year                     = {2017},
  Pages                    = {4161--4167}
}

@Misc{Liberman1999,
  Title                    = {The Creation, Distribution And Use Of Linguistic Data: The Case Of The Linguistic Data Consortium},

  Author                   = {Liberman, Mark and Cieri, Christopher},
  Month                    = {May},
  Year                     = {1999},

  Pages                    = {5}
}

@InProceedings{Liu2018CVPR,
  Title                    = {FOTS: Fast Oriented Text Spotting With a Unified Network},
  Author                   = {Liu, Xuebo and Liang, Ding and Yan, Shi and Chen, Dagui and Qiao, Yu and Yan, Junjie},
  Booktitle                = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2018},
  Month                    = {June}
}

@InProceedings{Liu2017CVPR,
  Title                    = {Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection},
  Author                   = {Y. Liu and L. Jin},
  Booktitle                = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2017},
  Month                    = {July},
  Pages                    = {3454-3461},

  Doi                      = {10.1109/CVPR.2017.368},
  ISSN                     = {1063-6919},
  Keywords                 = {image classification;image matching;learning (artificial intelligence);Monte Carlo methods;neural nets;text detection;video signal processing;shared Monte-Carlo method;polygonal areas;relative regression;auxiliary smooth Ln loss;multioriented scene text database;ICDAR 2015 Robust Reading Competition Challenge 4 Incidental scene text localization;perspective distortion;text size;even information loss;quadrilateral sliding windows;overlapping area;multiorientation text detection;incidental scene text detection;text scale;text color;convolutional neural network based method;CNN based method;deep matching prior network;DMPNet;sequential protocol;public word-level multioriented scene text database;F-measure;Shape;Noise measurement;Monte Carlo methods;Robustness;Distortion;Protocols}
}

@Article{Liu2017CoRR,
  Title                    = {Detecting Curve Text in the Wild: New Dataset and New Solution},
  Author                   = {Yuliang Liu and Lianwen Jin and Shuaitao Zhang and Sheng Zhang},
  Journal                  = {CoRR},
  Year                     = {2017},
  Volume                   = {abs/1712.02170},

  Archiveprefix            = {arXiv},
  Eprint                   = {1712.02170},
  Url                      = {http://arxiv.org/abs/1712.02170}
}

@InProceedings{Long2015CVPR,
  Title                    = {Fully convolutional networks for semantic segmentation},
  Author                   = {J. Long and E. Shelhamer and T. Darrell},
  Booktitle                = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2015},
  Month                    = {June},
  Pages                    = {3431-3440},

  Doi                      = {10.1109/CVPR.2015.7298965},
  ISSN                     = {1063-6919},
  Keywords                 = {image classification;image segmentation;inference mechanisms;learning (artificial intelligence);fully convolutional networks;semantic segmentation;visual models;pixels-to-pixels;inference;learning;contemporary classification networks;PASCAL VOC;NYUDv2;SIFT flow;Semantics;Training;Convolution;Image segmentation;Computer architecture;Deconvolution;Adaptation models}
}

@InProceedings{Lucas2005ICDAR,
  Title                    = {ICDAR 2005 text locating competition results},
  Author                   = {S. M. Lucas},
  Booktitle                = {Eighth International Conference on Document Analysis and Recognition (ICDAR'05)},
  Year                     = {2005},
  Month                    = {Aug},
  Pages                    = {80-84 Vol. 1},

  Doi                      = {10.1109/ICDAR.2005.231},
  ISSN                     = {1520-5363},
  Keywords                 = {image recognition;image retrieval;document image processing;ICDAR 2005 text locating system competition result;camera captured scenes;information retrieval;Web-based system;Testing;Layout;Robustness;Character recognition;Computer science;Information retrieval;Pattern recognition;Digital cameras;Roads;Books}
}

@InProceedings{Lucas2003ICDAR,
  Title                    = {ICDAR 2003 robust reading competitions},
  Author                   = {S. M. Lucas and A. Panaretos and L. Sosa and A. Tang and S. Wong and R. Young},
  Booktitle                = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},
  Year                     = {2003},
  Month                    = {Aug},
  Pages                    = {682-687},

  Doi                      = {10.1109/ICDAR.2003.1227749},
  Keywords                 = {Robustness;Layout;Text recognition;Optical character recognition software;Character recognition;Computer science;Packaging machines;Image converters;Image databases;Visual databases}
}

@Article{Lucas2005IJDAR,
  Title                    = {ICDAR 2003 robust reading competitions: entries, results, and future directions},
  Author                   = {Lucas, Simon M. and Panaretos, Alex and Sosa, Luis and Tang, Anthony and Wong, Shirley and Young, Robert and Ashida, Kazuki and Nagai, Hiroki and Okamoto, Masayuki and Yamamoto, Hiroaki and Miyao, Hidetoshi and Zhu, JunMin and Ou, WuWen and Wolf, Christian and Jolion, Jean-Michel and Todoran, Leon and Worring, Marcel and Lin, Xiaofan},
  Journal                  = {International Journal of Document Analysis and Recognition (IJDAR)},
  Year                     = {2005},

  Month                    = {Jul},
  Number                   = {2},
  Pages                    = {105--122},
  Volume                   = {7},

  Doi                      = {10.1007/s10032-004-0134-3},
  ISSN                     = {1433-2825},
  Url                      = {https://doi.org/10.1007/s10032-004-0134-3}
}

@Article{Manjani2017TIFS,
  Title                    = {Detecting Silicone Mask-Based Presentation Attack via Deep Dictionary Learning},
  Author                   = {I. Manjani and S. Tariyal and M. Vatsa and R. Singh and A. Majumdar},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2017},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1713-1723},
  Volume                   = {12},

  Doi                      = {10.1109/TIFS.2017.2676720},
  ISSN                     = {1556-6013},
  Keywords                 = {biometrics (access control);entertainment;face recognition;silicone mask-based presentation attack detection;deep dictionary learning;movies;film stars;entertainment purpose;automatic face recognition systems;biometrics systems;SMAD database;replay-attack;CASIA-FASD;3DMAD;UVAD;Dictionaries;Databases;Videos;Face;Detection algorithms;Training;Protocols;Face recognition;silicone mask;presentation attack detection;deep dictionary}
}

@Article{Matas2004IVC,
  Title                    = {Robust wide-baseline stereo from maximally stable extremal regions},
  Author                   = {J Matas and O Chum and M Urban and T Pajdla},
  Journal                  = {Image and Vision Computing},
  Year                     = {2004},
  Note                     = {British Machine Vision Computing 2002},
  Number                   = {10},
  Pages                    = {761 - 767},
  Volume                   = {22},

  Abstract                 = {The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied. A new set of image elements that are put into correspondence, the so called extremal regions, is introduced. Extremal regions possess highly desirable properties: the set is closed under (1) continuous (and thus projective) transformation of image coordinates and (2) monotonic transformation of image intensities. An efficient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an affinely invariant stable subset of extremal regions, the maximally stable extremal regions (MSER). A new robust similarity measure for establishing tentative correspondences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from extremal regions), some that are significantly larger (and hence discriminative) than the MSERs, may be used to establish tentative correspondences. The high utility of MSERs, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5ï¿½), illumination conditions, out-of-plane rotation, occlusion, locally anisotropic scale change and 3D translation of the viewpoint are all present in the test problems. Good estimates of epipolar geometry (average distance from corresponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained.},
  Doi                      = {https://doi.org/10.1016/j.imavis.2004.02.006},
  ISSN                     = {0262-8856},
  Keywords                 = {Wide-baseline stereo, Distinguished regions, Maximally stable extremal regions, MSER, Robust metric}
}

@Article{Minetto2014CVIU,
  Title                    = {SnooperText: {A} text detection system for automatic indexing of urban scenes},
  Author                   = {Rodrigo Minetto and Nicolas Thome and Matthieu Cord and Neucimar J. Leite and Jorge Stolfi},
  Journal                  = {Computer Vision and Image Understanding},
  Year                     = {2014},
  Pages                    = {92--104},
  Volume                   = {122},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/cviu/MinettoTCLS14},
  Doi                      = {10.1016/j.cviu.2013.10.004},
  Timestamp                = {Sun, 28 May 2017 13:23:21 +0200},
  Url                      = {https://doi.org/10.1016/j.cviu.2013.10.004}
}

@Article{Minetto2013PR,
  Title                    = {{T-HOG}: An effective gradient-based descriptor for single line text regions},
  Author                   = {Rodrigo Minetto and Nicolas Thome and Matthieu Cord and Neucimar J. Leite and Jorge Stolfi},
  Journal                  = {Pattern Recognition},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {1078--1090},
  Volume                   = {46},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/pr/MinettoTCLS13},
  Doi                      = {10.1016/j.patcog.2012.10.009},
  Timestamp                = {Thu, 18 May 2017 09:54:36 +0200},
  Url                      = {https://doi.org/10.1016/j.patcog.2012.10.009}
}

@InProceedings{Minetto2011ICIP,
  Title                    = {Snoopertrack: Text detection and tracking for outdoor videos},
  Author                   = {R. Minetto and N. Thome and M. Cord and N. J. Leite and J. Stolfi},
  Booktitle                = {2011 18th IEEE International Conference on Image Processing},
  Year                     = {2011},
  Month                    = {Sept},
  Pages                    = {505-508},

  Doi                      = {10.1109/ICIP.2011.6116563},
  ISSN                     = {2381-8549},
  Keywords                 = {object detection;optical character recognition;text analysis;video signal processing;SnooperTrack;text object detection;text object tracking;outdoor video;temporal coherence;particle filtering framework;Videos;Histograms;Algorithm design and analysis;Trajectory;Detection algorithms;Partitioning algorithms;Robustness;text detection;text tracking;particle filtering}
}

@InProceedings{Mishra2012CVPR,
  Title                    = {Top-Down and Bottom-Up Cues for Scene Text Recognition},
  Author                   = {Mishra, A. and Alahari, K. and Jawahar, C.~V.},
  Booktitle                = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2012}
}

@InProceedings{Mosleh_2012_BMVC,
  Title                    = {Image Text Detection Using a Bandlet-Based Edge Detector and Stroke Width Transform},
  Author                   = {Ali Mosleh and Nizar Bouguila and A. Ben Hamza},
  Booktitle                = {BMVC},
  Year                     = {2012},

  Timestamp                = {2018.09.24}
}

@InProceedings{Nagy2011ICDAR,
  Title                    = {NEOCR: A Configurable Dataset for Natural Image Text Recognition},
  Author                   = {Nagy, Robert
and Dicker, Anders
and Meyer-Wegener, Klaus},
  Booktitle                = {Camera-Based Document Analysis and Recognition},
  Year                     = {2012},

  Address                  = {Berlin, Heidelberg},
  Editor                   = {Iwamura, Masakazu
and Shafait, Faisal},
  Pages                    = {150--163},
  Publisher                = {Springer Berlin Heidelberg},

  Abstract                 = {Recently growing attention has been paid to recognizing text in natural images. Natural image text OCR is far more complex than OCR in scanned documents. Text in real world environments appears in arbitrary colors, font sizes and font types, often affected by perspective distortion, lighting effects, textures or occlusion. Currently there are no datasets publicly available which cover all aspects of natural image OCR. We propose a comprehensive well-annotated configurable dataset for optical character recognition in natural images for the evaluation and comparison of approaches tackling with natural image text OCR. Based on the rich annotations of the proposed NEOCR dataset new and more precise evaluations are now possible, which give more detailed information on where improvements are most required in natural image text OCR.},
  ISBN                     = {978-3-642-29364-1}
}

@Article{Neumann2016TPAMI,
  Title                    = {Real-Time Lexicon-Free Scene Text Localization and Recognition},
  Author                   = {Lukas Neumann and Jiri Matas},
  Journal                  = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  Year                     = {2016},
  Number                   = {9},
  Pages                    = {1872--1885},
  Volume                   = {38},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/pami/NeumannM16},
  Doi                      = {10.1109/TPAMI.2015.2496234},
  Timestamp                = {Wed, 17 May 2017 10:56:26 +0200},
  Url                      = {https://doi.org/10.1109/TPAMI.2015.2496234}
}


@InProceedings{Neumann2013ICDAR,
  Title                    = {On Combining Multiple Segmentations in Scene Text Recognition},
  Author                   = {Lukas Neumann and Jiri Matas},
  Booktitle                = {12th International Conference on Document Analysis and Recognition, {ICDAR} 2013, Washington, DC, USA, August 25-28, 2013},
  Year                     = {2013},
  Pages                    = {523--527},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/conf/icdar/NeumannM13},
  Doi                      = {10.1109/ICDAR.2013.110},
  Timestamp                = {Tue, 30 Jan 2018 12:57:23 +0100},
  Url                      = {https://doi.org/10.1109/ICDAR.2013.110}
}

@InProceedings{Neumann2012CVPR,
  Title                    = {Real-time scene text localization and recognition},
  Author                   = {L. Neumann and J. Matas},
  Booktitle                = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {3538-3545},

  Abstract                 = {An end-to-end real-time scene text localization and recognition method is presented. The real-time performance is achieved by posing the character detection problem as an efficient sequential selection from the set of Extremal Regions (ERs). The ER detector is robust to blur, illumination, color and texture variation and handles low-contrast text. In the first classification stage, the probability of each ER being a character is estimated using novel features calculated with O(1) complexity per region tested. Only ERs with locally maximal probability are selected for the second stage, where the classification is improved using more computationally expensive features. A highly efficient exhaustive search with feedback loops is then applied to group ERs into words and to select the most probable character segmentation. Finally, text is recognized in an OCR stage trained using synthetic fonts. The method was evaluated on two public datasets. On the ICDAR 2011 dataset, the method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end-to-end text recognition. On the more challenging Street View Text dataset, the method achieves state-of-the-art recall. The robustness of the proposed method against noise and low contrast of characters is demonstrated by ''false positives'' caused by detected watermark text in the dataset.},
  Doi                      = {10.1109/CVPR.2012.6248097},
  ISSN                     = {1063-6919},
  Keywords                 = {computational complexity;image classification;image segmentation;image watermarking;optical character recognition;probability;real-time systems;text detection;real-time scene text localization;real-time scene text recognition;real-time performance;character detection problem;sequential selection;extremal regions;ER detector;classification stage;computational complexity;locally maximal probability;computationally expensive features;exhaustive search;feedback loops;character segmentation;OCR stage;synthetic fonts;public datasets;ICDAR 2011 dataset;state-of-the-art text localization;end-to-end text recognition;street view text dataset;state-of-the-art recall;robustness;character contrast;watermark text detection;Erbium;Text recognition;Complexity theory;Real time systems;Detectors;Robustness;Optical character recognition software},
  Timestamp                = {2018.08.12}
}

@InProceedings{Neumann2011ICDAR,
  Title                    = {Text Localization in Real-World Images Using Efficiently Pruned Exhaustive Search},
  Author                   = {L. Neumann and J. Matas},
  Booktitle                = {2011 International Conference on Document Analysis and Recognition},
  Year                     = {2011},
  Month                    = {Sept},
  Pages                    = {687-691},

  Doi                      = {10.1109/ICDAR.2011.144},
  ISSN                     = {2379-2140},
  Keywords                 = {object recognition;text analysis;text localization;real-world images;pruned exhaustive search;text recognition;word text lines;grouping stage;character detector;error compensation;maximally stable extremal regions;region topology;ICDAR dataset;Text recognition;Image recognition;Vegetation;Training;Character recognition;Robustness;Detectors;text localization;real-world images;text-in-the-wild}
}

@InProceedings{Neumann2010ACCV,
  Title                    = {A Method for Text Localization and Recognition in Real-World Images},
  Author                   = {Lukas Neumann and Jiri Matas},
  Booktitle                = {Computer Vision - {ACCV} 2010 - 10th Asian Conference on Computer Vision, Queenstown, New Zealand, November 8-12, 2010, Revised Selected Papers, Part {III}},
  Year                     = {2010},
  Pages                    = {770--783},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/conf/accv/NeumannM10},
  Doi                      = {10.1007/978-3-642-19318-7\_60},
  Timestamp                = {Sun, 21 May 2017 00:17:37 +0200},
  Url                      = {https://doi.org/10.1007/978-3-642-19318-7\_60}
}

@Article{Nguyen2014WACV,
  Title                    = {Video text detection and recognition: Dataset and benchmark},
  Author                   = {Phuc Xuan Nguyen and Kai Wang and Serge J. Belongie},
  Journal                  = {IEEE Winter Conference on Applications of Computer Vision},
  Year                     = {2014},
  Pages                    = {776-783}
}

@InProceedings{Nister2008ECCV,
  Title                    = {Linear Time Maximally Stable Extremal Regions},
  Author                   = {Nist{\'{e}}r, David and Stew{\'{e}}nius, Henrik},
  Booktitle                = {Computer Vision -- ECCV 2008},
  Year                     = {2008},

  Address                  = {Berlin, Heidelberg},
  Editor                   = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  Pages                    = {183--196},
  Publisher                = {Springer Berlin Heidelberg},

  Abstract                 = {In this paper we present a new algorithm for computing Maximally Stable Extremal Regions (MSER), as invented by Matas et al. The standard algorithm makes use of a union-find data structure and takes quasi-linear time in the number of pixels. The new algorithm provides exactly identical results in true worst-case linear time. Moreover, the new algorithm uses significantly less memory and has better cache-locality, resulting in faster execution. Our CPU implementation performs twice as fast as a state-of-the-art FPGA implementation based on the standard algorithm.},
  ISBN                     = {978-3-540-88688-4}
}

@InProceedings{Ntirogiannis2008IAPR,
  Title                    = {An Objective Evaluation Methodology for Document Image Binarization Techniques},
  Author                   = {K. Ntirogiannis and B. Gatos and I. Pratikakis},
  Booktitle                = {2008 The Eighth IAPR International Workshop on Document Analysis Systems},
  Year                     = {2008},
  Month                    = {Sept},
  Pages                    = {217-224},

  Doi                      = {10.1109/DAS.2008.41},
  Keywords                 = {Humans;Optical character recognition software;Performance evaluation;Image analysis;Engines;Text analysis;Performance analysis;Testing;Pixel;Skeleton;evaluation;binarization}
}

@Book{pearl:88,
  Title                    = {Probabilistic {R}easoning in {I}ntelligent {S}ystems:
 {N}etworks of {P}lausible {I}nference},
  Author                   = {Judea Pearl},
  Publisher                = {Morgan Kaufman Publishers},
  Year                     = {1988},

  Address                  = {San Mateo, CA}
}

@InProceedings{Redmon2016CVPR,
  Title                    = {You Only Look Once: Unified, Real-Time Object Detection},
  Author                   = {J. Redmon and S. Divvala and R. Girshick and A. Farhadi},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2016},
  Month                    = {June},
  Pages                    = {779-788},

  Doi                      = {10.1109/CVPR.2016.91},
  ISSN                     = {1063-6919},
  Keywords                 = {image classification;image representation;neural nets;object detection;you only look once;unified real-time object detection;object classifiers;bounding boxes;class probabilities;neural network;detection pipeline;detection performance;YOLO model;object representation;DPM;R-CNN;natural images;Computer architecture;Microprocessors;Object detection;Training;Real-time systems;Neural networks;Pipelines}
}

@Article{Ren2015TPAMI,
  Title                    = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  Author                   = {S. Ren and K. He and R. Girshick and J. Sun},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2017},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1137-1149},
  Volume                   = {39},

  Doi                      = {10.1109/TPAMI.2016.2577031},
  ISSN                     = {0162-8828},
  Keywords                 = {graphics processing units;neural nets;object detection;faster-R-CNN;real-time object detection;region proposal networks;RPN;full-image convolutional features;high-quality region proposals;attention mechanisms;deep VGG-16 model;GPU;object detection accuracy;PASCAL VOC 2007;PASCAL VOC 2012;MS COCO datasets;COCO 2015 competitions;ILSVRC;Proposals;Object detection;Convolutional codes;Feature extraction;Search problems;Detectors;Training;Object detection;region proposal;convolutional neural network}
}

@Article{Risnumawan2014,
  Title                    = {A robust arbitrary text detection system for natural scene images},
  Author                   = {Anhar Risnumawan and Palaiahankote Shivakumara and Chee Seng Chan and Chew Lim Tan},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2014},
  Number                   = {18},
  Pages                    = {8027 - 8048},
  Volume                   = {41},

  Doi                      = {https://doi.org/10.1016/j.eswa.2014.07.008},
  ISSN                     = {0957-4174},
  Keywords                 = {Arbitrary text detection, Invariant properties, Symmetry features, Ellipse growing, Text verification, Text restoration},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417414004060}
}

@Article{Rodriguez-Serrano2015IJCV,
  Title                    = {Label Embedding: {A} Frugal Baseline for Text Recognition},
  Author                   = {Jos{\'{e}} A. Rodr{\'{\i}}guez{-}Serrano and Albert Gordo and Florent Perronnin},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {193--207},
  Volume                   = {113},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/ijcv/Rodriguez-Serrano15},
  Doi                      = {10.1007/s11263-014-0793-6},
  Timestamp                = {Wed, 17 May 2017 14:25:25 +0200},
  Url                      = {https://doi.org/10.1007/s11263-014-0793-6}
}

@Article{Santos2014JSTARS,
  Title                    = {Efficient and Effective Hierarchical Feature Propagation},
  Author                   = {J. A. dos Santos and O. A. B. Penatti and P. Gosselin and A. X. Falc\~{a}o and S. Philipp-Foliguet and R. d. S. Torres},
  Journal                  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  Year                     = {2014},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {4632-4643},
  Volume                   = {7},

  Abstract                 = {Many methods have been recently proposed to deal with the large amount of data provided by the new remote sensing technologies. Several of those methods rely on the use of segmented regions. However, a common issue in region-based applications is the definition of the appropriate representation scale of the data, a problem usually addressed by exploiting multiple scales of segmentation. The use of multiple scales, however, raises new challenges related to the definition of effective and efficient mechanisms for extracting features. In this paper, we address the problem of extracting features from a hierarchy by proposing two approaches that exploit the existing relationships among regions at different scales. The H-Propagation propagates any histogram-based low-level descriptors. The bag-of-visual-word (BoW)-Propagation approach uses the BoWs model to propagate features along multiple scales. The proposed methods are very efficient, as features need to be extracted only at the base of the hierarchy and yield comparable results to low-level extraction approaches.},
  Doi                      = {10.1109/JSTARS.2014.2341175},
  ISSN                     = {1939-1404},
  Keywords                 = {feature extraction;geophysical image processing;image segmentation;remote sensing;hierarchical feature propagation;remote sensing technologies;segmented regions;region-based applications;segmentation multiple scales;feature extraction problem;H-Propagation;histogram-based low-level descriptors;BoW-Propagation approach;bag-of-visual-word;BoWs model;Feature extraction;Visualization;Dictionaries;Histograms;Image segmentation;Remote sensing;Encoding;Bag-of-visual-words (BoWs);feature extraction;hierarchical segmentation;remote sensing images;Bag-of-visual-words (BoWs);feature extraction;hierarchical segmentation;remote sensing images}
}

@InProceedings{Shi2017CVPR,
  Title                    = {Detecting Oriented Text in Natural Images by Linking Segments},
  Author                   = {B. Shi and X. Bai and S. Belongie},
  Booktitle                = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2017},
  Month                    = {July},
  Pages                    = {3482-3490},

  Doi                      = {10.1109/CVPR.2017.371},
  ISSN                     = {1063-6919},
  Keywords                 = {edge detection;feature extraction;image classification;image colour analysis;image segmentation;neural nets;text analysis;text detection;SegLink;oriented text detection method;locally detectable elements;oriented box;adjacent segments;fully-convolutional neural network;nonLatin text;natural images;state-of-the-art text detection methods;horizontal Latin text;Segment Linking;Image segmentation;Feature extraction;Object detection;Neural networks;Proposals;Robustness;Indexes}
}

@Article{Shi2017TPAMI,
  Title                    = {An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition},
  Author                   = {B. Shi and X. Bai and C. Yao},
  Journal                  = IEEE_J_PAMI,
  Year                     = {2017},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {2298-2304},
  Volume                   = {39},

  Doi                      = {10.1109/TPAMI.2016.2646371},
  ISSN                     = {0162-8828},
  Keywords                 = {computer vision;feature extraction;image recognition;image segmentation;learning (artificial intelligence);music;neural nets;text detection;sequence modeling;transcription;end-to-end trainable neural network;neural network architecture;image-based sequence recognition;computer vision;feature extraction;lexicon-free scene text recognition tasks;lexicon-based scene text recognition tasks;IIIT-5K datasets;street view text datasets;image-based music score recognition;ICDAR datasets;Feature extraction;Text recognition;Neural networks;Image recognition;Logic gates;Convolutional codes;Context;Sequence recognition;scene text recognition;neural network;convolutional neural network;long-short term memory;optical music recognition},
  Timestamp                = {2018.08.12}
}

@InProceedings{Shi2016CVPR,
  Title                    = {Robust Scene Text Recognition with Automatic Rectification},
  Author                   = {B. Shi and X. Wang and P. Lyu and C. Yao and X. Bai},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2016},
  Month                    = {June},
  Pages                    = {4168-4176},

  Doi                      = {10.1109/CVPR.2016.452},
  ISSN                     = {1063-6919},
  Keywords                 = {image sequences;neural nets;splines (mathematics);text detection;scene text recognition;natural images;RARE;robust text recognizer with automatic rectification;deep neural network;spatial transformer network;STN;sequence recognition network;SRN;thin-plate-spline transformation;TPS transformation;irregular text recognition;perspective text;curved text;Text recognition;Image recognition;Decoding;Character recognition;Neural networks;Generators;Robustness}
}

@InProceedings{Shrivastava2016CVPR,
  Title                    = {Training Region-Based Object Detectors with Online Hard Example Mining},
  Author                   = {A. Shrivastava and A. Gupta and R. Girshick},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2016},
  Month                    = {June},
  Pages                    = {761-769},

  Doi                      = {10.1109/CVPR.2016.89},
  ISSN                     = {1063-6919},
  Keywords                 = {computer vision;convolution;data mining;image classification;neural nets;object detection;object detection;image classification;computer vision;online hard example mining;OHEM;region-based ConvNets;Training;Detectors;Object detection;Support vector machines;Convergence;Computer vision;Computational modeling}
}

@Article{Silva2018PR,
  Title                    = {Graph-based bag-of-words for classification},
  Author                   = {Fernanda B. Silva and
 Rafael de Oliveira Werneck and
 Siome Goldenstein and
 Salvatore Tabbone and
 Ricardo da Silva Torres},
  Journal                  = {Pattern Recognition},
  Year                     = {2018},
  Pages                    = {266--285},
  Volume                   = {74},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/pr/SilvaWGTT18},
  Doi                      = {10.1016/j.patcog.2017.09.018},
  Timestamp                = {Wed, 29 Nov 2017 15:52:49 +0100},
  Url                      = {https://doi.org/10.1016/j.patcog.2017.09.018}
}

@Article{Simonyan2014CORR,
  Title                    = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  Author                   = {Karen Simonyan and
 Andrew Zisserman},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1409.1556},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  Eprint                   = {1409.1556},
  Timestamp                = {Mon, 13 Aug 2018 01:00:00 +0200},
  Url                      = {http://arxiv.org/abs/1409.1556}
}

@Article{Tang2017TIP,
  Title                    = {Scene Text Detection and Segmentation Based on Cascaded Convolution Neural Networks},
  Author                   = {Y. Tang and X. Wu},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2017},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {1509-1520},
  Volume                   = {26},

  Doi                      = {10.1109/TIP.2017.2656474},
  ISSN                     = {1057-7149},
  Keywords                 = {image segmentation;neural nets;text detection;VGGNet-16;SNet;DNet;CNN-based CTR refinement model;candidate text region extraction model;computer vision;cascaded convolution neural networks;scene text segmentation;scene text detection;Feature extraction;Image segmentation;Image edge detection;Convolution;Training;Neural networks;Image color analysis;Scene Text detection;scene text segmentation;text-aware candidate text region extraction;candidate text region refinement;candidate text region classification}
}

@Article{Tariyal2016CoRR,
  Title                    = {Greedy Deep Dictionary Learning},
  Author                   = {Snigdha Tariyal and
 Angshul Majumdar and
 Richa Singh and
 Mayank Vatsa},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1602.00203},

  Archiveprefix            = {arXiv},
  Eprint                   = {1602.00203},
  Url                      = {http://arxiv.org/abs/1602.00203}
}

@Article{Tariyal2016IEEEAccess,
  Title                    = {Deep Dictionary Learning},
  Author                   = {S. Tariyal and A. Majumdar and R. Singh and M. Vatsa},
  Journal                  = {IEEE Access},
  Year                     = {2016},
  Pages                    = {10096-10109},
  Volume                   = {4},

  Doi                      = {10.1109/ACCESS.2016.2611583},
  ISSN                     = {2169-3536},
  Keywords                 = {feature extraction;learning (artificial intelligence);pattern classification;pattern clustering;deep dictionary learning;matrix factorization;feature extraction;greedy layer by layer approach;benchmark datasets;classification accuracies;clustering accuracies;electrical appliance classification;Dictionaries;Feature extraction;Machine learning;Matrix decomposition;Sparse matrices;Neural networks;Deep learning;dictionary learning;feature representation}
}

@Article{Tian2018TPAMI,
  Title                    = {A Unified Framework for Tracking Based Text Detection and Recognition from Web Videos},
  Author                   = {Shu Tian and Xu{-}Cheng Yin and Ya Su and Hong{-}Wei Hao},
  Journal                  = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  Year                     = {2018},
  Number                   = {3},
  Pages                    = {542--554},
  Volume                   = {40},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/pami/TianYSH18},
  Doi                      = {10.1109/TPAMI.2017.2692763},
  Timestamp                = {Mon, 19 Feb 2018 11:32:40 +0100},
  Url                      = {https://doi.org/10.1109/TPAMI.2017.2692763}
}

@Article{Veit2016ICDAR,
  Title                    = {COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images},
  Author                   = {Andreas Veit and Tomas Matera and Lukas Neumann and Jiri Matas and Serge J. Belongie},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1601.07140}
}

@InProceedings{Wang2011ICCV,
  Title                    = {End-to-end scene text recognition},
  Author                   = {Kai Wang and Boris Babenko and Serge J. Belongie},
  Booktitle                = {{IEEE} International Conference on Computer Vision, {ICCV} 2011, Barcelona, Spain, November 6-13, 2011},
  Year                     = {2011},
  Pages                    = {1457--1464},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/conf/iccv/WangBB11},
  Doi                      = {10.1109/ICCV.2011.6126402},
  Timestamp                = {Wed, 24 May 2017 08:31:05 +0200},
  Url                      = {https://doi.org/10.1109/ICCV.2011.6126402}
}

@InProceedings{Wang2010ECCV,
  Title                    = {Word Spotting in the Wild},
  Author                   = {Wang, Kai and Belongie, Serge},
  Booktitle                = {Computer Vision -- ECCV 2010},
  Year                     = {2010},

  Address                  = {Berlin, Heidelberg},
  Editor                   = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  Pages                    = {591--604},
  Publisher                = {Springer Berlin Heidelberg},

  Abstract                 = {We present a method for spotting words in the wild, i.e., in real images taken in unconstrained environments. Text found in the wild has a surprising range of difficulty. At one end of the spectrum, Optical Character Recognition (OCR) applied to scanned pages of well formatted printed text is one of the most successful applications of computer vision to date. At the other extreme lie visual CAPTCHAs -- text that is constructed explicitly to fool computer vision algorithms. Both tasks involve recognizing text, yet one is nearly solved while the other remains extremely challenging. In this work, we argue that the appearance of words in the wild spans this range of difficulties and propose a new word recognition approach based on state-of-the-art methods from generic object recognition, in which we consider object categories to be the words themselves. We compare performance of leading OCR engines -- one open source and one proprietary -- with our new approach on the ICDAR Robust Reading dataset and a new word spotting dataset we introduce in this paper: the Street View Text dataset. We show improvements of up to 16{\%} on the datasets, demonstrating the feasibility of a new approach to a seemingly old problem.},
  ISBN                     = {978-3-642-15549-9}
}

@Article{Wang2018IJCV,
  Title                    = {Transferring Deep Object and Scene Representations for Event Recognition in Still Images},
  Author                   = {Wang, Limin and Wang, Zhe and Qiao, Yu and Van Gool, Luc},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2018},

  Month                    = {Apr},
  Number                   = {2},
  Pages                    = {390--409},
  Volume                   = {126},

  Day                      = {01},
  Doi                      = {10.1007/s11263-017-1043-5},
  ISSN                     = {1573-1405},
}

@InProceedings{Wang2012ICPR,
  Title                    = {End-to-end text recognition with convolutional neural networks},
  Author                   = {T. Wang and D. J. Wu and A. Coates and A. Y. Ng},
  Booktitle                = {Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)},
  Year                     = {2012},
  Month                    = {Nov},
  Pages                    = {3304-3308},

  ISSN                     = {1051-4651},
  Keywords                 = {feature extraction;handwritten character recognition;multilayer perceptrons;natural scenes;text detection;unsupervised learning;end-to-end text recognition;convolutional neural network;natural image processing;multilayer neural network;unsupervised feature learning;text detection;character recognizer module;off-the-shelf method;scene text recognition system;Street View Text;ICDAR 2003;lexicon driven recognition system;Text recognition;Character recognition;Benchmark testing;Accuracy;Detectors;Neural networks;Standards}
}

@Article{Weinman2014TPAMI,
  Title                    = {Toward Integrated Scene Text Reading},
  Author                   = {J. J. Weinman and Z. Butler and D. Knoll and J. Feild},
  Journal                  = IEEE_J_PAMI,
  Year                     = {2014},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {375-387},
  Volume                   = {36},

  Doi                      = {10.1109/TPAMI.2013.126},
  ISSN                     = {0162-8828},
  Keywords                 = {document image processing;image motion analysis;image recognition;image segmentation;image sensors;probability;integrated scene text reading;digital camera usage;worldly text abundance;pattern recognition;document processing;unconstrained lexicons;motion blur;curved layouts;perspective projection;occlusion;probabilistic methods;character segmentation;word segmentation;Image segmentation;Character recognition;Text recognition;Probabilistic logic;Hidden Markov models;Noise;Robustness;Scene text recognition;cropped word recognition;character recognition;discriminative semi-Markov model;image binarization;skew detection;baseline estimation;text guidelines;word normalization;word segmentation;Algorithms;Artificial Intelligence;Data Interpretation, Statistical;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Natural Language Processing;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Systems Integration},
  Timestamp                = {2018.08.27}
}

@Article{Wolf2006IJDAR,
  Title                    = {Object count/area graphs for the evaluation of object detection and segmentation algorithms},
  Author                   = {Wolf, Christian
and Jolion, Jean-Michel},
  Journal                  = {International Journal of Document Analysis and Recognition (IJDAR)},
  Year                     = {2006},

  Month                    = {Sep},
  Number                   = {4},
  Pages                    = {280--296},
  Volume                   = {8},

  Abstract                 = {Evaluation of object detection algorithms is a non-trivial task: a detection result is usually evaluated by comparing the bounding box of the detected object with the bounding box of the ground truth object. The commonly used precision and recall measures are computed from the overlap area of these two rectangles. However, these measures have several drawbacks: they don't give intuitive information about the proportion of the correctly detected objects and the number of false alarms, and they cannot be accumulated across multiple images without creating ambiguity in their interpretation. Furthermore, quantitative and qualitative evaluation is often mixed resulting in ambiguous measures.},
  Day                      = {01},
  Doi                      = {10.1007/s10032-006-0014-0},
  ISSN                     = {1433-2825},
  Url                      = {https://doi.org/10.1007/s10032-006-0014-0}
}

@Article{Wu2016TIP,
  Title                    = {Contour Restoration of Text Components for Recognition in Video/Scene Images},
  Author                   = {Yirui Wu and Palaiahnakote Shivakumara and Tong Lu and Chew Lim Tan and Michael Blumenstein and G. Hemantha Kumar},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2016},
  Number                   = {12},
  Pages                    = {5622--5634},
  Volume                   = {25},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/WuSLTBK16},
  Doi                      = {10.1109/TIP.2016.2607426},
  Timestamp                = {Thu, 27 Jul 2017 10:19:12 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2016.2607426}
}

@Article{Xie2018TPAMI,
  Title                    = {Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition},
  Author                   = {Z. Xie and Z. Sun and L. Jin and H. Ni and T. Lyons},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2018},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {1903-1917},
  Volume                   = {40},

  Doi                      = {10.1109/TPAMI.2017.2732978},
  ISSN                     = {0162-8828},
  Keywords                 = {feature extraction;feedforward neural nets;handwritten character recognition;image segmentation;learning (artificial intelligence);online handwritten Chinese text recognition;ambiguous segmentation;variable-length input sequences;path signature;online pen-tip trajectories;informative signature feature maps;multispatial-context fully convolutional recurrent network;spatial-semantic context;local invariance;learning spatial-semantic context;MC-FCRN;feature sequence prediction;lexicon constraints;Dataset-CASIA benchmark;Dataset-ICDAR benchmark;Text recognition;Hidden Markov models;Trajectory;Semantics;Context modeling;Robustness;Feature extraction;Handwritten Chinese text recognition;path signature;residual recurrent network;multiple spatial contexts;implicit language model}
}

@Article{Yan2018TITS,
  Title                    = {Effective Uyghur Language Text Detection in Complex Background Images for Traffic Prompt Identification},
  Author                   = {Chenggang Yan and Hongtao Xie and Shun Liu and Jian Yin and Yongdong Zhang and Qionghai Dai},
  Journal                  = {{IEEE} Trans. Intelligent Transportation Systems},
  Year                     = {2018},
  Number                   = {1},
  Pages                    = {220--229},
  Volume                   = {19},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tits/YanXLYZD18},
  Doi                      = {10.1109/TITS.2017.2749977},
  Timestamp                = {Thu, 25 Jan 2018 11:12:59 +0100},
}

@Article{Yang2017TIP,
  Title                    = {Tracking Based Multi-Orientation Scene Text Detection: A Unified Framework With Dynamic Programming},
  Author                   = {C. Yang and X. Yin and W. Pei and S. Tian and Z. Zuo and C. Zhu and J. Yan},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2017},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {3235-3248},
  Volume                   = {26},

  Doi                      = {10.1109/TIP.2017.2695104},
  ISSN                     = {1057-7149},
  Keywords                 = {dynamic programming;image fusion;object tracking;text detection;video signal processing;tracking based multiorientation scene text detection;dynamic programming;skew distortion;video text detection methods;multiple frames;multiinformation fusion-based multiorientation text detection method;text region extraction;optimal tracking trajectory;prediction information;multiorientation scene text images;ICDAR 2015 scene videos;USTB-SV1K scene videos;MSRA-TD500 scene videos;Videos;Feature extraction;Tracking;Text recognition;Robustness;Dynamic programming;Distortion;Scene text detection;tracking-based text detection;multi-orientation scene text;dynamic programming}
}

@Article{Yang2014TLT,
  Title                    = {Content Based Lecture Video Retrieval Using Speech and Video Text Information},
  Author                   = {Haojin Yang and Christoph Meinel},
  Journal                  = {{TLT}},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {142--154},
  Volume                   = {7},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tlt/YangM14},
  Doi                      = {10.1109/TLT.2014.2307305},
  Timestamp                = {Sun, 28 May 2017 13:20:49 +0200},
  Url                      = {https://doi.org/10.1109/TLT.2014.2307305}
}

@Article{Yang2014MTAP,
  Title                    = {A framework for improved video text detection and recognition},
  Author                   = {Haojin Yang and Bernhard Quehl and Harald Sack},
  Journal                  = {Multimedia Tools Appl.},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {217--245},
  Volume                   = {69},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/mta/YangQS14},
  Doi                      = {10.1007/s11042-012-1250-6},
  Timestamp                = {Sun, 28 May 2017 13:22:30 +0200},
  Url                      = {https://doi.org/10.1007/s11042-012-1250-6}
}

@Article{Yao2014TIP,
  Title                    = {A Unified Framework for Multioriented Text Detection and Recognition},
  Author                   = {Cong Yao and Xiang Bai and Wenyu Liu},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2014},
  Number                   = {11},
  Pages                    = {4737--4749},
  Volume                   = {23},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/YaoBL14},
  Doi                      = {10.1109/TIP.2014.2353813},
  Timestamp                = {Fri, 26 May 2017 22:51:54 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2014.2353813}
}

@InProceedings{Yao2012CVPR,
  Title                    = {Detecting texts of arbitrary orientations in natural images},
  Author                   = {C. Yao and X. Bai and W. Liu and Y. Ma and Z. Tu},
  Booktitle                = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {1083-1090},

  Doi                      = {10.1109/CVPR.2012.6247787},
  ISSN                     = {1063-6919},
  Keywords                 = {feature extraction;image classification;text detection;arbitrary orientations;natural images;practical vision systems;smart phones;near-horizontal text detection;two-level classification scheme;feature set;text intrinsic characteristics;complex natural scenes;Shape;Image edge detection;Algorithm design and analysis;Clutter;Joining processes;Histograms;Robustness}
}

@InProceedings{Yao2014CVPR,
  Title                    = {Strokelets: {A} Learned Multi-scale Representation for Scene Text Recognition},
  Author                   = {Cong Yao and Xiang Bai and Baoguang Shi and Wenyu Liu},
  Booktitle                = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2014, Columbus, OH, USA, June 23-28, 2014},
  Year                     = {2014},
  Pages                    = {4042--4049},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/conf/cvpr/YaoBSL14},
  Doi                      = {10.1109/CVPR.2014.515},
  Timestamp                = {Thu, 25 May 2017 00:41:18 +0200},
  Url                      = {https://doi.org/10.1109/CVPR.2014.515}
}

@InProceedings{Yao2007ICWAPR,
  Title                    = {Locating text based on connected component and SVM},
  Author                   = {Yao, Jin-Liang and Wang, Yan-Qing and Weng, Lu-Bin and Yang, Yi-Ping},
  Booktitle                = {Wavelet Analysis and Pattern Recognition, 2007. ICWAPR'07. International Conference on},
  Year                     = {2007},
  Organization             = {IEEE},
  Pages                    = {1418--1423},
  Volume                   = {3}
}

@Article{Ye2015PAMI,
  Title                    = {Text Detection and Recognition in Imagery: {A} Survey},
  Author                   = {Qixiang Ye and David S. Doermann},
  Journal                  = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  Year                     = {2015},
  Number                   = {7},
  Pages                    = {1480--1500},
  Volume                   = {37},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/pami/YeD15},
  Doi                      = {10.1109/TPAMI.2014.2366765},
  Timestamp                = {Wed, 17 May 2017 10:56:27 +0200},
}

@InProceedings{Ye_2014_ICIP,
  Title                    = {Robust scene text detection using integrated feature discrimination},
  Author                   = {Q. Ye and D. S. Doermann},
  Booktitle                = {2014 IEEE International Conference on Image Processing (ICIP)},
  Year                     = {2014},
  Month                    = {Oct},
  Pages                    = {1678-1682},

  Abstract                 = {Scene text detection in images of cluttered backgrounds and/or multilingual context is very challenging. In this paper, we propose a discriminative approach that integrates appearance and consensus features for robust scene text detection. We propose an integrated discrimination model to perform text classification as well as control component grouping. We design shape, stroke and structural features to describe text component appearance and the consensus among them. Experimental results on three public datasets show that the proposed approach is robust to cluttered backgrounds, and is applicable in multilingual environments.},
  Doi                      = {10.1109/ICIP.2014.7025336},
  ISSN                     = {1522-4880},
  Keywords                 = {clutter;image classification;text detection;scene text image detection;integrated feature discrimination model;cluttered background;multilingual context environment;text classification;control component grouping;Shape;Feature extraction;Robustness;Vectors;Text recognition;Clustering algorithms;Training;Text detection;Discriminative model;Feature integration},
  Timestamp                = {2018.08.12}
}

@Article{Yi2014TIP,
  Title                    = {Scene Text Recognition in Mobile Applications by Character Descriptor and Structure Configuration},
  Author                   = {Chucai Yi and Yingli Tian},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2014},
  Number                   = {7},
  Pages                    = {2972--2982},
  Volume                   = {23},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/YiT14},
  Doi                      = {10.1109/TIP.2014.2317980},
  Timestamp                = {Fri, 26 May 2017 22:51:39 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2014.2317980}
}

@Article{Yi2014TM,
  Title                    = {Portable Camera-Based Assistive Text and Product Label Reading From Hand-Held Objects for Blind Persons},
  Author                   = {C. Yi and Y. Tian and A. Arditi},
  Journal                  = {IEEE/ASME Transactions on Mechatronics},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {3},
  Pages                    = {808-817},
  Volume                   = {19},

  Abstract                 = {We propose a camera-based assistive text reading framework to help blind persons read text labels and product packaging from hand-held objects in their daily lives. To isolate the object from cluttered backgrounds or other surrounding objects in the camera view, we first propose an efficient and effective motion-based method to define a region of interest (ROI) in the video by asking the user to shake the object. This method extracts moving object region by a mixture-of-Gaussians-based background subtraction method. In the extracted ROI, text localization and recognition are conducted to acquire text information. To automatically localize the text regions from the object ROI, we propose a novel text localization algorithm by learning gradient features of stroke orientations and distributions of edge pixels in an Adaboost model. Text characters in the localized text regions are then binarized and recognized by off-the-shelf optical character recognition software. The recognized text codes are output to blind users in speech. Performance of the proposed text localization algorithm is quantitatively evaluated on ICDAR-2003 and ICDAR-2011 Robust Reading Datasets. Experimental results demonstrate that our algorithm achieves the state of the arts. The proof-of-concept prototype is also evaluated on a dataset collected using ten blind persons to evaluate the effectiveness of the system's hardware. We explore user interface issues and assess robustness of the algorithm in extracting and reading text from different objects with complex backgrounds.},
  Doi                      = {10.1109/TMECH.2013.2261083},
  ISSN                     = {1083-4435},
  Keywords                 = {edge detection;feature extraction;Gaussian processes;handicapped aids;human computer interaction;image motion analysis;learning (artificial intelligence);optical character recognition;text detection;user interfaces;video cameras;video signal processing;ICDAR-2011 robust reading datasets;ICDAR-2003 robust reading datasets;user interface;recognized text codes;off-the-shelf optical character recognition software;text characters;Adaboost model;edge pixel distributions;stroke orientations;gradient feature learning;text localization algorithm;text information;text recognition;moving object region extraction;mixture-of-Gaussians-based background subtraction method;ROI;region of interest;motion-based method;product packaging;text labels;camera-based assistive text reading framework;blind persons;handheld objects;product label reading;portable camera-based assistive text;Assistive devices;blindness;distribution of edge pixels;hand-held objects;optical character recognition (OCR);stroke orientation;text reading;text region localization}
}

@Article{Yin2015TPAMI,
  Title                    = {Multi-Orientation Scene Text Detection with Adaptive Clustering},
  Author                   = {Xu{-}Cheng Yin and Wei{-}Yi Pei and Jun Zhang and Hong{-}Wei Hao},
  Journal                  = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  Year                     = {2015},
  Number                   = {9},
  Pages                    = {1930--1937},
  Volume                   = {37},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/pami/YinPZH15},
  Doi                      = {10.1109/TPAMI.2014.2388210},
  Timestamp                = {Thu, 08 Jun 2017 09:06:12 +0200},
  Url                      = {https://doi.org/10.1109/TPAMI.2014.2388210}
}

@InProceedings{Yin2012ICPR,
  Title                    = {Effective text localization in natural scene images with MSER, geometry-based grouping and AdaBoost},
  Author                   = {Xuwang Yin and Xu{-}Cheng Yin and Hong{-}Wei Hao and Khalid Iqbal},
  Booktitle                = {Proceedings of the 21st International Conference on Pattern Recognition, {ICPR} 2012, Tsukuba, Japan, November 11-15, 2012},
  Year                     = {2012},
  Pages                    = {725--728},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/conf/icpr/YinYHI12},
  Timestamp                = {Sun, 30 Apr 2017 09:41:46 +0200},
  Url                      = {http://ieeexplore.ieee.org/document/6460237/}
}

@Article{Yin2014TPAMI,
  Title                    = {Robust Text Detection in Natural Scene Images},
  Author                   = {X. Yin and X. Yin and K. Huang and H. Hao},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2014},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {970-983},
  Volume                   = {36},

  Doi                      = {10.1109/TPAMI.2013.182},
  ISSN                     = {0162-8828},
  Keywords                 = {learning (artificial intelligence);object detection;pattern clustering;probability;robust text detection;natural scene images;content-based image analysis;pruning algorithm;MSER;maximally stable extremal regions;minimizing regularized variations strategy;single-link clustering algorithm;distance weights;clustering threshold;self-training distance metric learning algorithm;posterior probabilities;text classifier;f-measure;multilingual database;street view database;multiorientation database;born-digital databases;Clustering algorithms;Algorithm design and analysis;Measurement;Vegetation;Robustness;Databases;Educational institutions;Text processing;Scene Analysis;Image Processing and Computer Vision;Computing Methodologies;Scene text detection;maximally stable extremal regions;single-link clustering;distance metric learning}
}

@Article{Yin2016TIP,
  Title                    = {Text Detection, Tracking and Recognition in Video: {A} Comprehensive Survey},
  Author                   = {Xu{-}Cheng Yin and Ze{-}Yu Zuo and Shu Tian and Cheng{-}Lin Liu},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2016},
  Number                   = {6},
  Pages                    = {2752--2773},
  Volume                   = {25},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/YinZTL16},
  Doi                      = {10.1109/TIP.2016.2554321},
  Timestamp                = {Thu, 08 Jun 2017 09:01:56 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2016.2554321}
}

@Article{Zhang2013Neurocomputing,
  Title                    = {Text extraction from natural scene image: {A} survey},
  Author                   = {Honggang Zhang and Kaili Zhao and Yi{-}Zhe Song and Jun Guo},
  Journal                  = {Neurocomputing},
  Year                     = {2013},
  Pages                    = {310--323},
  Volume                   = {122},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/ijon/ZhangZSG13},
  Doi                      = {10.1016/j.neucom.2013.05.037},
  Timestamp                = {Tue, 27 Mar 2018 13:23:49 +0200},
  Url                      = {https://doi.org/10.1016/j.neucom.2013.05.037}
}

@Article{Zhang2014TIP,
  Title                    = {A Novel Text Detection System Based on Character and Link Energies},
  Author                   = {Jing Zhang and Rangachar Kasturi},
  Journal                  = {{IEEE} Trans. Image Processing},
  Year                     = {2014},
  Number                   = {9},
  Pages                    = {4187--4198},
  Volume                   = {23},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/tip/ZhangK14},
  Doi                      = {10.1109/TIP.2014.2341935},
  Timestamp                = {Fri, 26 May 2017 22:51:39 +0200},
  Url                      = {https://doi.org/10.1109/TIP.2014.2341935}
}

@InProceedings{Kaihua2014ECCV,
  Title                    = {Fast Visual Tracking via Dense Spatio-temporal Context Learning},
  Author                   = {Zhang, Kaihua and Zhang, Lei and Liu, Qingshan and Zhang, David and Yang, Ming-Hsuan},
  Booktitle                = {Computer Vision -- ECCV 2014},
  Year                     = {2014},

  Address                  = {Cham},
  Editor                   = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  Pages                    = {127--141},
  Publisher                = {Springer International Publishing},

  ISBN                     = {978-3-319-10602-1}
}

@InProceedings{Zhang2015CVPR,
  Title                    = {Symmetry-Based Text Line Detection in Natural Scenes},
  Author                   = {Zheng Zhang and Wei Shen and Cong Yao and Xiang Bai},
  Booktitle                = IEEE_C_CVPR,
  Year                     = {2015},
  Month                    = {June},

  Abstract                 = {Recently, a variety of real-world applications have triggered huge demand for techniques that can extract textual information from natural scenes. Therefore, scene text detection and recognition have become active research topics in computer vision. In this work, we investigate the problem of scene text detection from an alternative perspective and propose a novel algorithm for it. Different from traditional methods, which mainly make use of the properties of single characters or strokes, the proposed algorithm exploits the symmetry property of character groups and allows for direct extraction of text lines from natural images. The experiments on the latest ICDAR benchmarks demonstrate that the proposed algorithm achieves state-of-the-art performance. Moreover, compared to conventional approaches, the proposed algorithm shows stronger adaptability to texts in challenging scenarios.},
  Timestamp                = {2018.08.12}
}

@InProceedings{Zhang2016CVPR,
  Title                    = {Multi-Oriented Text Detection With Fully Convolutional Networks},
  Author                   = {Zhang, Zheng and Zhang, Chengquan and Shen, Wei and Yao, Cong and Liu, Wenyu and Bai, Xiang},
  Booktitle                = IEEE_C_CVPR,
  Year                     = {2016},
  Month                    = {June},

  Abstract                 = {In this paper, we propose an unconventional approach for text detection in natural images. Both global and local cues are taken into account for localizing text lines in a coarse-to-fine procedure. First, a Fully Convolutional Network (FCN) model is trained for predicting a salient map of text regions in a holistic manner. Then, a set of hypotheses text lines are estimated by combining the salient map and MSER components. Finally, another FCN classifier is used for predicting the centroid of each character, in order to remove the false hypotheses. The framework is general for handling texts in multiple orientations, languages and fonts. The proposed method consistently achieves the state-of-the-art performance on three text detection benchmarks: MSRA-TD500, ICDAR2015, and ICDAR2013.},
  Timestamp                = {2018.08.12}
}

@InProceedings{Zhong2017ICASSP,
  Title                    = {DeepText: A new approach for text proposal generation and text detection in natural images},
  Author                   = {Z. Zhong and L. Jin and S. Huang},
  Booktitle                = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2017},
  Month                    = {March},
  Pages                    = {1208-1212},

  Doi                      = {10.1109/ICASSP.2017.7952348},
  ISSN                     = {2379-190X},
  Keywords                 = {feedforward neural nets;image classification;text detection;DeepText;text region proposal generation;text detection;natural images;fully convolutional neural network;inception region proposal network;Inception-RPN;multiscale windows;convolutional feature maps;text characteristic prior bounding boxes;sliding position;high recall word region proposals;ambiguous text category information;multilevel region-of-interest pooling;text classification;nontext classification;localization refinement;ICDAR robust text detection benchmarks;Proposals;Feature extraction;Robustness;Benchmark testing;Training;Convolution;Standards;text detection;convolutional neural network;region proposal network;natural images}
}

@InProceedings{Zhou2017CVPR,
  Title                    = {EAST: An Efficient and Accurate Scene Text Detector},
  Author                   = {X. Zhou and C. Yao and H. Wen and Y. Wang and S. Zhou and W. He and J. Liang},
  Booktitle                = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2017},
  Month                    = {July},
  Pages                    = {2642-2651},

  Doi                      = {10.1109/CVPR.2017.283},
  ISSN                     = {1063-6919},
  Keywords                 = {feature extraction;natural scenes;neural net architecture;text detection;deep neural network models;COCO-Text dataset;MSRA-TD500 dataset;scene text detection;EAST;ICDAR 2015 dataset;neural network architecture;text lines;Pipelines;Geometry;Neural networks;Feature extraction;Proposals;Merging;Shape}
}

@InProceedings{Zhu2016CVPR,
  Title                    = {A Text Detection System for Natural Scenes with Convolutional Feature Learning and Cascaded Classification},
  Author                   = {S. Zhu and R. Zanibbi},
  Booktitle                = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2016},
  Month                    = {June},
  Pages                    = {625-632},

  Doi                      = {10.1109/CVPR.2016.74},
  ISSN                     = {1063-6919},
  Keywords                 = {edge detection;feature extraction;graph theory;image classification;image colour analysis;image segmentation;learning (artificial intelligence);natural scenes;text detection;visual databases;text detection system;convolutional feature learning;cascaded classification;data-driven method;coarse-to-fine detection;character pixels;Text-Conv;connected component extraction;CC extraction;edge features;color features;graph-based segmentation;convolutional feature maps;convolutional neural networks;CNN;convolutional k-means;convolution masks;local patch features;neighboring patch features;detection accuracy improvement;Word-Graph algorithm;word segmentation improvement;false-character detection pruning;foreground-text regions;bounding box intersection;pixel intersection;word detection f-measure;pixel f-measure;character f-measure;ICDAR 2015 Robust Reading Focused Scene Text dataset;homogenous color;natural scenes;false-word detection pruning;Feature extraction;Detectors;Image color analysis;Robustness;Image edge detection;Text recognition;Convolutional codes}
}

@Article{Zhu2018TITS,
  Title                    = {Cascaded Segmentation-Detection Networks for Text-Based Traffic Sign Detection},
  Author                   = {Y. Zhu and M. Liao and M. Yang and W. Liu},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2018},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {209-219},
  Volume                   = {19},

  Doi                      = {10.1109/TITS.2017.2768827},
  ISSN                     = {1524-9050},
  Keywords                 = {feature extraction;image colour analysis;image segmentation;learning (artificial intelligence);neural nets;object detection;text detection;traffic engineering computing;cascaded segmentation-detection networks;traffic sign detection framework;fully convolutional network;fast neural network;two-stage detection method;text detection part;publicly available traffic sign data;text-based traffic signs;candidate traffic sign areas;deep learning components;traffic guide panel data;Chinese traffic signs;English traffic signs;Text recognition;Character recognition;Detectors;Machine learning;Assistive technology;Text detection;text-based traffic sign detection;fully convolutional network;textboxes;object detection}
}

@Article{Zhu2016FCS,
  Title                    = {Scene text detection and recognition: recent advances and future trends},
  Author                   = {Yingying Zhu and Cong Yao and Xiang Bai},
  Journal                  = {Frontiers Comput. Sci.},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {19--36},
  Volume                   = {10},

  Bibsource                = {dblp computer science bibliography, https://dblp.org},
  Biburl                   = {https://dblp.org/rec/bib/journals/fcsc/ZhuYB16},
  Doi                      = {10.1007/s11704-015-4488-0},
  Timestamp                = {Mon, 19 Feb 2018 18:18:37 +0100},
  Url                      = {https://doi.org/10.1007/s11704-015-4488-0}
}

@InProceedings{Zitnick2014ECCV,
  Title                    = {Edge Boxes: Locating Object Proposals from Edges},
  Author                   = {Zitnick, C. Lawrence
and Doll{\'a}r, Piotr},
  Booktitle                = {Computer Vision -- ECCV 2014},
  Year                     = {2014},

  Address                  = {Cham},
  Editor                   = {Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne},
  Pages                    = {391--405},
  Publisher                = {Springer International Publishing},

  ISBN                     = {978-3-319-10602-1}
}

@Misc{Scopus,
  Title                    = {Elsevier's Scopus},
  HowPublished             = {\url{https://www.scopus.com/}}
}


@INPROCEEDINGS{Sandler2018CVPR,
author={M. Sandler and A. Howard and M. Zhu and A. Zhmoginov and L. Chen},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
year={2018},
volume={},
number={},
pages={4510-4520},
abstract={In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
keywords={convolution;image segmentation;mobile computing;object detection;mobile models;mobile semantic segmentation models;Mobile DeepLabv3;inverted residual structure;bottleneck layers;intermediate expansion layer;lightweight depthwise convolutions;narrow layers;COCO object detection;MobileNetV2;inverted residuals;linear bottlenecks;mobile architecture;Manifolds;Neural networks;Computer architecture;Standards;Computational modeling;Task analysis},
doi={10.1109/CVPR.2018.00474},
ISSN={2575-7075},
month={June},}

@InProceedings{Liu2016ECCV,
  author    = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  title     = {{SSD}: Single Shot MultiBox Detector},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  pages     = {21--37},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\$}{\$}300 {\backslash}times 300{\$}{\$}300{\texttimes}300input, SSD achieves 74.3ï¿½{\%} mAP on VOC2007 test at 59ï¿½FPS on a Nvidia Titan X and for {\$}{\$}512 {\backslash}times 512{\$}{\$}512{\texttimes}512input, SSD achieves 76.9ï¿½{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  isbn      = {978-3-319-46448-0},
}

@article{Perez2003ToG,
 author = {P{\'e}rez, Patrick and Gangnet, Michel and Blake, Andrew},
 title = {Poisson Image Editing},
 journal = {ACM Trans. Graph.},
 issue_date = {July 2003},
 volume = {22},
 number = {3},
 month = jul,
 year = {2003},
 issn = {0730-0301},
 pages = {313--318},
 numpages = {6},
 doi = {10.1145/882262.882269},
 acmid = {882269},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Poisson equation, guided interpolation, image gradient, interactive image editing, seamless cloning, selection editing},
}

@INPROCEEDINGS{Neubeck2006ICPR, 
author={A. Neubeck and L. Van Gool}, 
booktitle={18th International Conference on Pattern Recognition (ICPR'06)}, 
title={Efficient Non-Maximum Suppression}, 
year={2006}, 
volume={3}, 
number={}, 
pages={850-855}, 
keywords={computer vision;image resolution;nonmaximum suppression;computer vision;image resolution;Computer vision;Data mining;Filters;Application software;Image resolution;Pixel;Image reconstruction;Object recognition;Image texture analysis;Gas insulated transmission lines}, 
doi={10.1109/ICPR.2006.479}, 
ISSN={1051-4651}, 
month={Aug},}

@article{Howard2017CoRR,
  author    = {Andrew G. Howard and
               Menglong Zhu and
               Bo Chen and
               Dmitry Kalenichenko and
               Weijun Wang and
               Tobias Weyand and
               Marco Andreetto and
               Hartwig Adam},
  title     = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
               Applications},
  journal   = {CoRR},
  volume    = {abs/1704.04861},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.04861},
  archivePrefix = {arXiv},
  eprint    = {1704.04861},
  timestamp = {Mon, 13 Aug 2018 16:46:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HowardZCKWWAA17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{HeCVPR2016, 
author={K. He and X. Zhang and S. Ren and J. Sun}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Deep Residual Learning for Image Recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={770-778}, 
keywords={image classification;learning (artificial intelligence);neural nets;object detection;COCO segmentation;ImageNet localization;ILSVRC & COCO 2015 competitions;deep residual nets;COCO object detection dataset;visual recognition tasks;CIFAR-10;ILSVRC 2015 classification task;ImageNet test set;VGG nets;residual nets;ImageNet dataset;residual function learning;deeper neural network training;image recognition;deep residual learning;Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}, 
doi={10.1109/CVPR.2016.90}, 
ISSN={1063-6919}, 
month={June},}

@article{Redmon2018CoRR,
  title={YOLOv3: An Incremental Improvement},
  author={Joseph Redmon and Ali Farhadi},
  journal={CoRR},
  year={2018},
  volume={abs/1804.02767}
}

@Article{Everingham2015,
author={Everingham, Mark and Eslami, S. M. Ali and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
title={The Pascal Visual Object Classes Challenge: A Retrospective},
journal={International Journal of Computer Vision},
year={2015},
month={Jan},
volume={111},
number={1},
pages={98--136},
issn={1573-1405},
doi={10.1007/s11263-014-0733-5}
}

@INPROCEEDINGS{Wu2017CVPRW, 
author={B. Wu and F. Iandola and P. H. Jin and K. Keutzer}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
title={SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving}, 
year={2017}, 
volume={}, 
number={}, 
pages={446-454}, 
doi={10.1109/CVPRW.2017.60}, 
ISSN={2160-7516}, 
month={July},}

@article{Tieleman2012,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, T. and Hinton, G.},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}


@INPROCEEDINGS{He2017ICCV, 
author={P. He and W. Huang and T. He and Q. Zhu and Y. Qiao and X. Li}, 
booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
title={Single Shot Text Detector with Regional Attention}, 
year={2017}, 
volume={}, 
number={}, 
pages={3066-3074}, 
keywords={convolution;feature extraction;natural scenes;neural nets;text detection;single shot text detector;regional attention;word-level bounding boxes;natural image;attention mechanism;text regions;background interference;convolutional features;hierarchical inception module;single-scale images;attentional map;FCN-based text detectors;multiscale inception features;Fully Convolutional Network;Detectors;Convolutional codes;Predictive models;Heating systems;Computer vision;Interference}, 
doi={10.1109/ICCV.2017.331}, 
ISSN={2380-7504}, 
month={Oct},}


@INPROCEEDINGS{Lin2017CVPR, 
author={T. {Lin} and P. {DollÃ¡r} and R. {Girshick} and K. {He} and B. {Hariharan} and S. {Belongie}}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Feature Pyramid Networks for Object Detection}, 
year={2017}, 
volume={}, 
number={}, 
pages={936-944}, 
keywords={feature extraction;image representation;neural nets;object detection;object recognition;deep convolutional networks;pyramidal hierarchy;high-level semantic feature maps;Feature Pyramid Network;generic feature extractor;basic Faster R-CNN system;COCO detection benchmark;multiscale object detection;pyramid representations;FPN;Feature extraction;Detectors;Semantics;Computer architecture;Proposals;Object detection;Robustness}, 
doi={10.1109/CVPR.2017.106}, 
ISSN={1063-6919}, 
month={July},}

@comment{jabref-meta: selector_keywords:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_review:}
