% This file was created with JabRef 2.7.
% Encoding: ISO8859_1

@INPROCEEDINGS{Al2015ICDAR,
  author = {M. Al Azawi and M. Liwicki and T. M. Breuel},
  title = {Combination of multiple aligned recognition outputs using WFST and
    LSTM},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {31-35},
  month = {Aug},
  abstract = {The contribution of this paper is a new strategy of integrating multiple
    recognition outputs of diverse recognizers. Such an integration can
    give higher performance, The and more accurate outputs than a single
    recognition system. The problem of aligning various Optical Character
    Recognition (OCR) results lies in the difficulties to find the correspondence
    on character, word, line and page level. These difficulties arise
    from segmentation and recognition errors which are produced by the
    OCRs. Therefore, alignment techniques are required for synchronizing
    the outputs in order to compare them. Most existing approaches fail
    when the same error occurs in the multiple OCRs. If the corrections
    do not appear in one of the OCR approaches are unable to improve
    the results. We design a Line-to-Page alignment with edit rules using
    Weighted Finite-State Transducers (WFST). These edit rules are based
    on edit operations: insertion, deletion and substitution. Therefore,
    an approach is designed using Recurrent Neural Networks with Long
    Short-Term Memory (LSTM) to predict these types of errors. A Character-Epsilon
    alignment is designed to normalize the size of the strings for the
    LSTM alignment. The LSTM returns best voting, especially when the
    heuristic approaches are unable to vote among various O. C. R. engines.
    L. S. T. M. predicts the correct characters even if the O. C. R.
    could not produce the characters in the outputs. The approaches are
    evaluated on OCR's output from the U. W. I. I. I. and historical
    German Fraktur dataset which are obtained from state-of-the-art OCR
    systems. The experiments shows that the error rate of the LSTM approach
    has the best performance with around 0.40\%, while other approaches
    are between 1.26\% and 2.31\%.},
  doi = {10.1109/ICDAR.2015.7333720},
  review = {This paper designs a Line-to-Page alignment with edit rules using
    Weighted Finite-State Transducers (WFST). These edit rules are based
    on edit operations: insertion, deletion and substitution. Therefore,
    an approach is designed using Recurrent Neural Networks with Long
    Short-Term Memory (LSTM) to predict these types of errors. A Character-Epsilon
    alignment is designed to normalize the size of the strings for the
    LSTM alignment. The LSTM returns best voting, especially when the
    heuristic approaches are unable to vote among various OCR engines.}
}

@INPROCEEDINGS{Cai2017ICDAR,
  author = {M. Cai and W. Hu and K. Chen and L. Sun and S. Liang and X. Mo and
    Q. Huo},
  title = {An Open Vocabulary OCR System with Hybrid Word-Subword Language Models},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {519-524},
  month = {Nov},
  abstract = {The accuracy of a typical state-of-the-art optical character recognition
    (OCR) system benefits greatly from using a language model (LM). However,
    a conventional LM has a limited vocabulary, resulting in out-of-vocabulary
    (OOV) words that cannot be recognized by the O. C. R. system. In
    this paper we present an open vocabulary O. C. R. system based on
    a hybrid L. M. The vocabulary of the hybrid L. M. consists of both
    words and subwords. The out-of-vocabulary (OOV) words can be generated by combinations of
    subwords. A refined hybrid LM training scheme is applied by interpolating
    a standard hybrid LM, a word-based L. M. and a subword-based LM.
    An efficient word combination method is performed by modeling optional
    space symbols in a decoding network. The overall system deals with
    OOV words in a general, data-driven and language-independent way.
    We conduct experiments on an English handwriting OCR task. Evaluations
    on three testing sets demonstrate that the OCR system with the proposed
    method achieves a word error rate of 33.4\% on an OOV-only testing
    set, yet without degrading the recognition accuracies on the other
    two testing sets mainly consisting of in-vocabulary words.},
  doi = {10.1109/ICDAR.2017.91},
  issn = {2379-2140},
  keywords = {handwritten character recognition;interpolation;natural language processing;optical
    character recognition;open vocabulary OCR system;hybrid word-subword
    language models;language model;out-of-vocabulary;OOV words;refined
    hybrid LM training scheme;standard hybrid LM;efficient word combination
    method;English handwriting OCR task;word error rate;in-vocabulary
    words;optical character recognition system;Optical character recognition
    software;Decoding;Vocabulary;Training;Task analysis;Standards;Pragmatics},
  review = {This paper presents an open vocabulary OCR system based on a hybrid
    language model (LM). The vocabulary of the hybrid LM consists of
    both words and subwords. The out-of-vocabulary (OOV) words can be generated by combinations
    of subwords. A refined hybrid LM training scheme is applied by interpolating
    a standard hybrid LM, a word-based LM and a subword-based LM. An
    efficient word combination method is performed by modeling optional
    space symbols in a decoding network.}
}

@INPROCEEDINGS{Calarasanu2015ICDAR,
  author = {S. Calarasanu and J. Fabrizio and S. Dubuisson},
  title = {Using histogram representation and Earth Mover's Distance as an evaluation
    tool for text detection},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {221-225},
  month = {Aug},
  abstract = {In the context of text detection evaluation, it is essential to use
    protocols that are capable of describing both the quality and the
    quantity aspects of detection results. In this paper we propose a
    novel visual representation and evaluation tool that captures the
    whole nature of a detector by using histograms. First, two histograms
    (coverage and accuracy) are generated to visualize the different
    characteristics of a detector. Secondly, we compare these two histograms
    to a so called optimal one to compute representative and comparable
    scores. To do so, we introduce the usage of the Earth Mover's Distance
    as a reliable evaluation tool to estimate recall and precision scores.
    Results obtained on the ICDAR 2013 dataset show that this method
    intuitively characterizes the accuracy of a text detector and gives
    at a glance various useful characteristics of the analyzed algorithm.},
  doi = {10.1109/ICDAR.2015.7333756},
  keywords = {data visualisation;text detection;histogram representation;earth mover
    distance;text detection evaluation tool;visual representation;reliable
    evaluation tool;ICDAR 2013 dataset;Visualization;Histograms},
  review = {This paper proposes a novel visual representation and evaluation tool
    that capture the whole nature of a text detector by using histograms.
    First, two histograms (coverage and accuracy) are generated to visualize
    the different characteristics of a detector. Secondly, these two
    histograms are compared to a so called optimal one to compute representative
    and comparable scores. To do so, the authors introduce the usage
    of the Earth Mover's Distance as a reliable evaluation tool to estimate
    recall and precision scores.}
}

@ARTICLE{Chen2016IJDAR,
  author = {Chen, Yui-Lang and Yu, Pao-Ta},
  title = {An evidence-based model of saliency feature extraction for scene
    text analysis},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  year = {2016},
  volume = {19},
  pages = {269--287},
  number = {3},
  month = {Sep},
  abstract = {Saliency text is that characters are ordered with visibility, Saliency
    and expressivity. It also contains important clues for video analysis,
    indexing and retrieval. Thus, in order to localize the saliency text,
    a critical stage is to collect key points from real text pixels.
    In this paper we propose an evidence-based model of saliency feature
    extraction (SFE) to probe saliency text points (STPs) which have
    strong text signal structure in multi-observations simultaneously
    and always appear between text and its background. Through the multi-observations,
    each signal structure with rhythms of signal segments is extracted
    at every location in the visual field. It supports source of evidences
    for our evidence-based model, where evidences are measured to effectively
    estimate the degrees of plausibility for obtaining the S. T. P. The
    evaluation results on benchmark datasets also demonstrate that our
    proposed approach achieves the state-of-the-art performance on exploring
    real text pixels and significantly outperforms the existing algorithms
    for detecting text candidates. The STPs can be the extremely reliable
    text candidates for future text detectors.},
  day = {01},
  doi = {10.1007/s10032-016-0270-6},
  issn = {1433-2825},
  review = {This paper proposes an evidence-based model of saliency feature extraction
    (SFE) to probe saliency text points (STPs) which have strong text
    signal structure in multi-observations simultaneously and always
    appear between text and its background. Through the multi-observations,
    each signal structure with rhythms of signal segments is extracted
    at every location in the visual field.},
  url = {https://doi.org/10.1007/s10032-016-0270-6}
}

@INPROCEEDINGS{Chng2017ICDAR,
  author = {Chee Kheng Ch'ng and Chee Seng Chan},
  title = {Total-Text: A Comprehensive Dataset for Scene Text Detection and
    Recognition},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {935-942},
  month = {Nov},
  abstract = {Text in curve orientation, despite being one of the common text orientations
    in real world environment, has close to zero existence in well received
    scene text datasets such as ICDAR'13 and main motivation of Total-Text
    is to fill this gap, M. S. R. A.-TD500. The and facilitate a new
    research direction for the scene text community. On top of conventional
    horizontal and multi-oriented text, it features curved-oriented text.
    Total-Text is highly diversified in orientations, more than half
    of its images have a combination of more than two orientations. Recently
    a new breed of solutions that casted text detection as a segmentation
    problem has demonstrated their effectiveness against multi-oriented
    text. In order to evaluate its robustness against curved text we
    fine-tuned DeconvNet and benchmark it on Total-Text. Total-Text with
    its annotation is available at https://github.com/cs-chan/Total-Text-Dataset.},
  doi = {10.1109/ICDAR.2017.157},
  issn = {2379-2140},
  keywords = {feature extraction;image segmentation;text analysis;text detection;scene
    text detection;curve orientation;received scene text datasets;Total-Text;scene
    text community;curved text;text orientations;scene text recognition;Feature
    extraction;Image recognition;Text recognition;Robustness;Algorithm
    design and analysis;Image color analysis;Image segmentation;Scene
    text dataset;Curve-oriented text;Segmentation-based text detection},
  review = {This paper introduced the Total-text dataset. Text in curve orientation,
    despite being one of the common text orientations in real world environment,
    has close to zero existence in well received scene text datasets
    such as ICDAR'13 and main motivation of Total-Text is to fill this
    gap. On top of conventional horizontal and multi-oriented text, it
    features curved-oriented texts.}
}

@ARTICLE{Elagouni2014IJDAR,
  author = {Elagouni, Khaoula and Garcia, Christophe and Mamalet, Franck and
    S{\'e}billot, Pascale},
  title = {Text recognition in multimedia documents: a study of two neural-based
    OCRs using and avoiding character segmentation},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  year = {2014},
  volume = {17},
  pages = {19--31},
  number = {1},
  month = {Mar},
  abstract = {Text embedded in multimedia documents represents an important semantic
    information that helps to automatically access the content. This
    paper proposes two neural-based optical character recognition (OCR)
    systems that handle the text recognition problem in different ways.
    The first approach segments a text image into individual characters
    before recognizing them, while the second one avoids the segmentation
    step by integrating a multi-scale scanning scheme that allows to
    jointly localize and recognize characters at each position and scale.
    Some linguistic knowledge is also incorporated into the proposed
    schemes to remove errors due to recognition confusions. Both OCR
    systems are applied to caption texts embedded in videos and in natural
    scene images and provide outstanding results showing that the proposed
    approaches outperform the state-of-the-art methods.},
  day = {01},
  doi = {10.1007/s10032-013-0202-7},
  issn = {1433-2825},
  review = {This paper proposes two neural-based optical character recognition
    (OCR) systems that handle the text recognition problem in different
    ways. The first approach segments a text image into individual characters
    before recognizing them, while the second one avoids the segmentation
    step by integrating a multi-scale scanning scheme that allows to
    jointly localize and recognize characters at each position and scale.
    Both OCR systems are applied to caption texts embedded in videos
    and in natural scene images.},
  url = {https://doi.org/10.1007/s10032-013-0202-7}
}

@INPROCEEDINGS{En2017ICDAR,
  author = {M. En and R. Li and J. Li and B. Liu},
  title = {Feature Pyramid Based Scene Text Detector},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {06},
  pages = {3-8},
  month = {Nov},
  abstract = {Features are critical for detecting texts in natural scene images.
    Nowadays most of scene text detection algorithm leverage powerful
    feature learning power of convolutional neural networks (CNNs) to
    learn discriminative features which could distinguish text from non-text
    well, Features and perform detection based on these features. It
    is known that features from low layers of CNN are high-resolution
    but have low discriminative power and less semantic information and
    this compromises the representative capacity of the features. On
    the other hand, feature maps from high layers are discriminative
    but coarse-resolution, which harms the power for detecting small
    objects. In this paper we present a feature pyramid based text detector
    (FPTD) for detecting scene texts at different scales especially texts
    at small scales. Our framework is based on the state-of-the-art framework
    "Single Shot detector" (SSD) but not like S. S. D. which performs
    detection on feature maps from later-stage of the network which are
    coarse in resolution so they cannot get satisfied results on small
    objects. Our framework incorporates feature pyramid mechanism with
    S. S. D. framework. Specifically in the framework we adopt a top-down
    fusion strategy to build new features with strong semantics while
    keep fine details. Text detections are conducted on multiple new
    constructed features respectively during a single forward pass. All
    detection results from each layer are gathered and undergo a non-maximum
    suppression (NMS) process. Since detection is conducted on feature
    maps from several layers which at different scales but are all discriminative,
    our framework has strong power to detect texts at different scales.
    Experimental results confirm that our framework achieves competitive
    performance on the ICDAR2013 text location benchmark and with marginal
    extra cost.},
  doi = {10.1109/ICDAR.2017.341},
  issn = {2379-2140},
  keywords = {feature extraction;image classification;learning (artificial intelligence);natural
    scenes;neural nets;object detection;text analysis;text detection;feature
    pyramid based scene text detector;natural scene images;feature maps;feature
    pyramid based text detector;scene texts;feature pyramid mechanism;SSD
    framework;text detections;scene text detection algorithm;Feature
    extraction;Semantics;Object detection;Neural networks;Detectors;Buildings;Proposals;scene
    text;deep learning;CNN;feature fusion;feature pyramid;multi-scale},
  review = {This paper presents a feature pyramid based text detector (FPTD) for
    detecting scene texts at different scales especially texts at small
    scales. The proposed framework is based on the state-of-the-art framework ``Single Shot detector" (SSD), and it incorporates feature pyramid mechanism with SSD framework. Specifically, in the framework, the authors adopt a top-down fusion strategy to build new features with strong semantics while keep fine details. Text detections are conducted on multiple new constructed features respectively during a single forward pass. All detection results from each layer are gathered and undergo a non-maximum suppression (NMS) process.}
}

@ARTICLE{Fabrizio2016IJDAR,
  author = {Fabrizio, Jonathan and Robert-Seidowsky, Myriam and Dubuisson, S{\'e}verine
    and Calarasanu, Stefania and Boissel, Rapha{\"e}l},
  title = {TextCatcher: a method to detect curved and challenging text in natural
    scenes},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  year = {2016},
  volume = {19},
  pages = {99--117},
  number = {2},
  month = {Jun},
  abstract = {In this paper, we propose a text detection algorithm which is hybrid
    and multi-scale. First, it relies on a connected component-based
    approach: After the segmentation of the image, a classification step
    using a new wavelet descriptor spots the letters. A. new graph modeling
    and its traversal procedure allow to form candidate text areas. Second,
    a texture-based approach discards the false positives. Finally, the
    detected text areas are precisely cut out and a new binarization
    step is introduced. The main advantage of our method is that few
    assumptions are put forward. Thus, ``challenging texts'' like multi-sized,
    multi-colored multi-oriented or curved text can be localized. The
    efficiency of TextCatcher has been validated on three different datasets:
    Two come from the I. C. D. A. R. competition and the third one contains
    photographs we have taken with various daily life texts. We present
    both qualitative and quantitative results.},
  day = {01},
  doi = {10.1007/s10032-016-0264-4},
  issn = {1433-2825},
  review = {This paper proposes a text detection algorithm which is hybrid and
    multi-scale. First, it relies on a connected component-based approach:
    After the segmentation of the image, a classification step using
    a new wavelet descriptor spots the letters. A new graph modeling
    and its traversal procedure allow to form candidate text areas. Second,
    a texture-based approach discards the false positives. Finally, the
    detected text areas are precisely cut out and a new binarization
    step is introduced.},
  url = {https://doi.org/10.1007/s10032-016-0264-4}
}

@ARTICLE{Fraz2015IJDAR,
  author = {Fraz, Muhammad and Sarfraz, M. Saquib and Edirisinghe, Eran A.},
  title = {Exploiting colour information for better scene text detection and
    recognition},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  year = {2015},
  volume = {18},
  pages = {153--167},
  number = {2},
  month = {Jun},
  abstract = {This paper presents an approach for text detection, This and recognition
    in scene images. The main contribution of this paper is to demonstrate
    that the colour information within the images if efficiently exploited
    is good enough to identify text regions from the surrounding noise.
    In the same way, the colour information present in character and
    word images can be used to achieve significant performance improvement
    in the recognition of characters and words. The proposed pipeline
    makes use of the colour information and low-level image processing
    operations to enhance text information that improves the overall
    performance of text detection and recognition in the wild. The proposed
    method offers two main advantages. First, it enhances the text regions
    up to a level of clarity where a simple off-the-shelf feature representation
    and classification method achieves state-of-the-art recognition performance.
    Second, the proposed framework is computationally fast as compared
    to other text detection and recognition techniques that offer good
    accuracy at the cost of significantly high latency. We performed
    extensive experimentation to evaluate our method on challenging benchmark
    datasets (Chars74K, ICDAR03, ICDAR11 and SVT) and the results show
    a considerable performance improvement.},
  day = {01},
  doi = {10.1007/s10032-015-0239-x},
  issn = {1433-2825},
  review = {This paper proposes a pipeline that makes use of the colour information
    and low-level image processing operations to enhance text information
    that improves the overall performance of text detection and recognition
    in the wild. The proposed method offers two main advantages. First,
    it enhances the text regions up to a level of clarity where a simple
    off-the-shelf feature representation and classification method achieves
    state-of-the-art recognition performance. Second, the proposed framework
    is computationally fast.},
  url = {https://doi.org/10.1007/s10032-015-0239-x}
}

@INPROCEEDINGS{Gaddour2017ICDAR,
  author = {H. Gaddour and S. Kanoun and N. Vincent},
  title = {Color Stability and Homogeneity Regions to Detect Text in Real Scene
    Images: CSHR},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1283-1288},
  month = {Nov},
  abstract = {In this paper, a novel method called C. S. H. R. for affine invariant
    detection of stable and homogeneous parts of the extremal regions
    to localize text in natural scene images is proposed. The basic idea
    of this method was to apply two local thresholds to extract the extremal
    regions by their color homogeneity and select the candidate regions
    by maximum and minimum surface limits. Then, the candidate regions
    were filtered according to a stability criterion to extract the maximally
    stable parts of the extremal regions. Finally, the text regions are
    filtered using region area, orientation, and aspect ratio properties
    as well as features specific to the Arabic language to focus on Arabic
    writing. The proposed approach which was tested on the ICDAR2003
    database and on our database showed an improvement over the existing
    methods.},
  doi = {10.1109/ICDAR.2017.211},
  issn = {2379-2140},
  keywords = {image colour analysis;image filtering;image segmentation;stability;text
    detection;maximum surface limits;minimum surface limits;color stability;color
    homogeneity regions;text detection;natural scene imaging;aspect ratio
    properties;Arabic language;ICDAR2003 database;Arabic writing;affine
    invariant detection;CSHR;Image color analysis;Stability criteria;Colored
    noise;Detectors;Image segmentation;Image edge detection;Extremal
    Regions;Stability;Homogeniety;Text Detection;Scene Images},
  review = {This paper proposes a novel method called CSHR for affine invariant
    detection of stable and homogeneous parts of the extremal regions
    to localize text in natural scene images is proposed. The basic idea
    of this method was to apply two local thresholds to extract the extremal
    regions by their color homogeneity and select the candidate regions
    by maximum and minimum surface limits. Then, the candidate regions
    were filtered according to a stability criterion to extract the maximally
    stable parts of the extremal regions. Finally, the text regions are
    filtered using region area, orientation and aspect ratio properties
    as well as features specific to the Arabic language to focus on Arabic
    writing.}
}

@INPROCEEDINGS{Ganguly2017ICDAR,
  author = {D. Ganguly and S. Agarwal and S. Chaudhury},
  title = {Improving Classical OCRs for Brahmic Scripts Using Script Grammar
    Learning},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {07},
  pages = {37-41},
  month = {Nov},
  abstract = {Classical OCRs based on isolated character (symbol) recognition have
    been the fundamental way of generating textual representations, particularly
    for Indian scripts, until the time transcription-based approaches
    gained momentum. Though the former approaches have been criticized
    as prone to failures their accuracy has nevertheless been fairly
    decent in comparison with the newer transcription-based approaches.
    Analysis of isolated character recognition OCRs for Hindi and Bangla
    revealed most errors were generated in converting the output of the
    classifier to valid Unicode sequences, i.e., script grammar generation.
    Linguistic rules to generate scripts are inadequately integrated
    thus resulting in a rigid Unicode generation scheme which is cumbersome
    to understand and error prone in adapting to new Indian scripts.
    In this paper we propose a machine learning-based classifier symbols
    to Unicode generation scheme which outperforms the existing generation
    scheme and improves accuracy for Devanagari and scripts, Bangla},
  doi = {10.1109/ICDAR.2017.363},
  issn = {2379-2140},
  keywords = {image representation;learning (artificial intelligence);optical character
    recognition;pattern classification;script grammar learning;textual
    representations;Indian scripts;isolated character recognition OCRs;valid
    Unicode sequences;script grammar generation;rigid Unicode generation
    scheme;machine learning-based classifier symbols;symbol recognition;classical
    OCRs;Brahmic scripts;isolated character recognition;Bangla scripts;Devanagari
    scripts;Hidden Markov models;Optical character recognition software;Error
    analysis;Pragmatics;Training;Character recognition;Pipelines;Optical
    Character Recognition;Hidden Markov Models;Indian Language Scripts},
  review = {This paper proposes a machine learning-based classifier symbols to
    Unicode generation scheme which outperforms the existing generation
    scheme and improves accuracy for Devanagari and scripts, Bangla.}
}

@INPROCEEDINGS{Gao2015ICDAR,
  author = {R. Gao and S. Eguchi and S. Uchida},
  title = {True color distributions of scene text and background},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {506-510},
  month = {Aug},
  abstract = {Color feature, as one of the low level features, plays important role
    in image processing object recognition and other fields. For example,
    in the task of scene text detection and recognition, lots of methodologies
    employ features that utilize color contrast of text and the corresponding
    background for connected component extraction. However, the true
    distributions of text and its background, in terms of color, is still
    not examined because it requires an enough number of scene text database
    with pixel-level labelled text/non-text ground truth. To clarify
    the relationship between text and its background, in this paper,
    we aim at investigating the color non-parametric distribution of
    text and its background using a large database that contains 3018
    scene images and 600 characters. The results of our experiments show
    that text, 98 and its background can be discriminated by means of
    color, therefore color feature can be used for scene text detection.},
  doi = {10.1109/ICDAR.2015.7333813},
  keywords = {feature extraction;image colour analysis;text detection;true color
    distribution;image processing;object recognition;scene text detection;scene
    text recognition;feature extraction;pixel-level labelled nontext
    ground truth;Image color analysis;Feature extraction;Databases;Text
    recognition;Data mining;Optical character recognition software;Histograms},
  review = {This paper aims at investigating the color non-parametric distribution
    of text and its background using a large database that contains 3018
    scene images and 600 characters. The results of performed experiments
    show that text, 98 and its background can be discriminated by means
    of color, therefore color feature can be used for scene text detection.}
}

@INPROCEEDINGS{Ghosh2017ICDAR,
  author = {S. K. Ghosh and E. Valveny and A. D. Bagdanov},
  title = {Visual Attention Models for Scene Text Recognition},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {943-948},
  month = {Nov},
  abstract = {In this paper we propose an approach to lexicon-free recognition of
    text in scene images. Our approach relies on a LSTM-based soft visual
    attention model learned from convolutional features. A set of feature
    vectors are derived from an intermediate convolutional layer corresponding
    to different areas of the image. This permits encoding of spatial
    information into the image representation. In this way, the framework
    is able to learn how to selectively focus on different parts of the
    image. At every time step the recognizer emits one character using
    a weighted combination of the convolutional feature vectors according
    to the learned attention model. Training can be done end-to-end using
    only word level annotations. In addition, we show that modifying
    the beam search algorithm by integrating an explicit language model
    leads to significantly better recognition results. We validate the
    performance of our approach on standard S. V. T. and ICDAR'03 scene
    text datasets, showing state-of-the-art performance in unconstrained
    text recognition.},
  doi = {10.1109/ICDAR.2017.158},
  issn = {2379-2140},
  keywords = {image recognition;image representation;text analysis;text detection;learned
    attention model;word level annotations;beam search algorithm;scene
    text recognition;LSTM;convolutional features;image representation;Text
    recognition;Decoding;Image recognition;Character recognition;Visualization;Computational
    modeling;Adaptation models},
  review = {This paper proposes an approach to lexicon-free recognition of text
    in scene images. The proposed approach relies on an LSTM-based soft
    visual attention model learned from convolutional features. A set
    of feature vectors are derived from an intermediate convolutional
    layer corresponding to different areas of the image. This permits
    encoding of spatial information into the image representation. In
    this way, the framework is able to learn how to selectively focus
    on different parts of the image. At every time step the recognizer
    emits one character using a weighted combination of the convolutional
    feature vectors according to the learned attention model.}
}

@ARTICLE{Gomez2016IJDAR,
  author = {Gomez, Lluis and Karatzas, Dimosthenis},
  title = {A fast hierarchical method for multi-script and arbitrary oriented
    scene text extraction},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  year = {2016},
  volume = {19},
  pages = {335--349},
  number = {4},
  month = {Dec},
  abstract = {Typography and layout lead to the hierarchical organization of text
    in words, text lines, paragraphs. This inherent structure is a key
    property of text in any script and language, which has nonetheless
    been minimally leveraged by existing scene text detection methods.
    This paper addresses the problem of text segmentation in natural
    scenes from a hierarchical perspective. Contrary to existing methods,
    we make explicit use of text structure aiming directly to the detection
    of region groupings corresponding to text within a hierarchy produced
    by an agglomerative similarity clustering process over individual
    regions. We propose an optimal way to construct such a hierarchy
    introducing a feature space designed to produce text group hypotheses
    with high recall and a novel stopping rule combining a discriminative
    classifier and a probabilistic measure of group meaningfulness based
    on perceptual organization. Results obtained over four standard datasets,
    covering text in variable orientations and different languages, demonstrate
    that our algorithm, while being trained in a single mixed dataset
    outperforms state-of-the-art methods in unconstrained scenarios.},
  day = {01},
  doi = {10.1007/s10032-016-0274-2},
  issn = {1433-2825},
  review = {This paper addresses the problem of text segmentation in natural scenes
    from a hierarchical perspective. Contrary to existing methods, its
    authors make explicit use of text structure aiming directly to the
    detection of region groupings corresponding to text within a hierarchy
    produced by an agglomerative similarity clustering process over individual
    regions. The authors propose an optimal way to construct such an
    hierarchy introducing a feature space designed to produce text group
    hypotheses with high recall and a novel stopping rule combining a
    discriminative classifier and a probabilistic measure of group meaningfulness
    based on perceptual organization.},
  url = {https://doi.org/10.1007/s10032-016-0274-2}
}

@INPROCEEDINGS{Gomez2017ICDAR,
  author = {R. Gomez and B. Shi and L. Gomez and L. Numann and A. Veit and J.
    Matas and S. Belongie and D. Karatzas},
  title = {ICDAR2017 Robust Reading Challenge on COCO-Text},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1435-1443},
  month = {Nov},
  abstract = {This report presents the final results of the ICDAR 2017 Robust Reading
    Challenge on COCO-Text. A challenge on scene text detection, This
    and recognition based on the largest real scene text dataset currently
    available: the COCO-Text dataset. The competition is structured around
    three tasks: Text Localization, Cropped Word Recognition and competition
    received a total of 27 submissions over the different opened tasks.
    This report describes the datasets, End-To-End Recognition. The and
    the ground truth, details the performance evaluation protocols used
    and presents the final results along with a brief summary of the
    participating methods.},
  doi = {10.1109/ICDAR.2017.234},
  issn = {2379-2140},
  keywords = {feature extraction;image recognition;text analysis;text detection;scene
    text detection;largest real scene text dataset;COCO-Text dataset;End-To-End
    Recognition;ICDAR2017 Robust Reading Challenge;text localization;cropped
    word recognition;Task analysis;Text recognition;Robustness;Image
    recognition;Performance evaluation;Protocols},
  review = {This report presents the final results of the ICDAR 2017 Robust Reading
    Challenge on COCO-Text.}
}

@ARTICLE{Hinami2016IJDAR,
  author = {Hinami, Ryota and Liu, Xinhao and Chiba, Naoki and Satoh, Shin'ichi},
  title = {Bidirectional extraction and recognition of scene text with layout
    consistency},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  year = {2016},
  volume = {19},
  pages = {83--98},
  number = {2},
  month = {Jun},
  abstract = {Text recognition in natural scene images is a challenging task that
    has recently been garnering increased research attention. In this
    paper, we propose a method for recognizing text by utilizing the
    layout consistency of a text string. We estimate the layout (four
    lines of a text string) using initial character extraction and recognition
    result. On the basis of the layout consistency across a word, we
    perform character extraction and recognition again using four lines,
    which is more accurate than the first process. Our layout estimation
    method is different from previous methods in terms of exploiting
    character recognition results and its use of a class-conditional
    layout model. More accurate and robust estimation is achieved and
    it can be used to refine character extraction and recognition. We
    call this two-way process-from extraction and recognition to layout
    and from layout to extraction and recognition-``bidirectional'' to
    discriminate it from previous feedback refinement approaches. Experimental
    results demonstrate that our bidirectional processes provide a boost
    to the performance of word recognition.},
  day = {01},
  doi = {10.1007/s10032-016-0261-7},
  issn = {1433-2825},
  review = {This paper proposes a method for recognizing text by utilizing the
    layout consistency of a text string. The method estimates the layout
    (four lines of a text string) using initial character extraction
    and recognition result. On the basis of the layout consistency across
    a word, the method extracts and recognizes characters again using
    four lines, which is more accurate than the first process. The proposed
    layout estimation method is different from previous methods in terms
    of exploiting character recognition results and its use of a class-conditional
    layout model.},
  url = {https://doi.org/10.1007/s10032-016-0261-7}
}

@INPROCEEDINGS{JeanCaurant2017ICDAR,
  author = {A. Jean-Caurant and N. Tamani and V. Courboulay and J. Burie},
  title = {Lexicographical-Based Order for Post-OCR Correction of Named Entities},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1192-1197},
  month = {Nov},
  abstract = {We are in the era of information access in which a huge amount of
    text is extracted from scanned documents, We and made available digitally
    to be used in search processes. However, old or poorly scanned documents
    suffer from bad recognition, which leads to not only imperfect Optical
    Character Recognition (OCR) but to bad indexation and unattainable
    information, as well. To cope with the aforementioned issues, we
    introduce in this paper a lexicographical-based approach for Post-O.
    C. R. correction applied to named entities. By combining lexicographically
    a contextual similarity and an edit distance, the approach builds
    a graph connecting similar named entities, in order to automatically
    correct the corresponding O. C. R. processed text. We evaluated our
    approach on a generated dataset. The first results obtained showed
    that despite the high level of degradation of the text the approach
    succeeded in correcting more than a third of named entities without
    the need for any external knowledge.},
  doi = {10.1109/ICDAR.2017.197},
  issn = {2379-2140},
  keywords = {document image processing;optical character recognition;text detection;Post-OCR
    correction;post-OCR correction;information access;bad indexation;Optical
    Character Recognition;Optical character recognition software;Character
    recognition;Tools;Data models;Google;Computational modeling;Context
    modeling;OCR Post-correction;Named Entities;Entity Resolution;Word
    Embeddings;Digital Humanities},
  review = {This paper introduces a lexicographical-based approach for Post-OCR
    correction applied to named entities. By combining lexicographically
    a contextual similarity and an edit distance, the approach builds
    a graph connecting similar named entities, in order to automatically
    correct the corresponding OCR processed text.}
}

@INPROCEEDINGS{Jia2017ICDAR,
  author = {W. Jia and L. Sun and Z. Zhong and X. Mo and G. Ma and Q. Huo},
  title = {A Robust Approach to Detecting Text from Images of Whiteboards and
    Handwritten Notes},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {813-818},
  month = {Nov},
  abstract = {Detect text from the images of whiteboards, Detecting and handwritten
    notes is an important yet under-researched topic. In this paper,
    we present a robust approach to solving this challenging problem
    as follows. First, given a color image color enhanced. Contrasting
    Extremal Regions (CERs) are extracted from its grayscale image as
    candidate text connected components (CCs). Second four shallow neural
    networks are used to preprune efficiently most of unambiguous non-text
    CCs. Third a Fast R.-C. N. N. based approach is proposed to filter
    out remaining nontext CCs by leveraging contextual information and
    to estimate the corresponding text-line orientation in the position
    of each remaining text CC. Fourth, each pair of the remaining text
    CCs within a certain distance and orientation constraint are connected
    to construct a directed graph. Finally, based on the estimated textline
    orientations, candidate text-lines are generated easily by pruning
    greedily redundant edges in the graph to make each vertex have at
    most one direct successor and one direct predecessor, respectively.
    Our proposed approach has achieved promising results on an in-house
    testing set consisting of 285 camera-captured images of whiteboards
    and handwritten notes.},
  doi = {10.1109/ICDAR.2017.138},
  issn = {2379-2140},
  keywords = {directed graphs;document image processing;feature extraction;image
    colour analysis;image filtering;image segmentation;neural nets;text
    analysis;text detection;orientation constraint;handwritten notes;color
    image;grayscale image;shallow neural networks;Fast R-CNN based approach;textline
    orientations;Contrasting Extremal Regions;candidate text connected
    components;remaining nontext CC;directed graph;whiteboards;Robustness;Proposals;Neural
    networks;Data models;Training;Standards;Image edge detection;handwritten
    text detection;natural scene image;text-line grouping;Fast R-CNN},
  review = {This paper presents a robust approach to solving the problem of detect
    text from the images of whiteboards as follows. First, given a color
    image color enhanced Contrasting Extremal Regions (CERs) are extracted
    from its grayscale image as candidate text connected components (CCs).
    Second four shallow neural networks are used to preprune efficiently
    most of unambiguous non-text CCs. Third a Fast RCNN based approach
    is proposed to filter out remaining nontext CCs by leveraging contextual
    information and to estimate the corresponding text-line orientation
    in the position of each remaining text CC. Fourth, each pair of the
    remaining text CCs within a certain distance and orientation constraint
    are connected to construct a directed graph. Finally, based on the
    estimated textline orientations, candidate text-lines are generated
    easily by pruning greedily redundant edges in the graph to make each
    vertex have at most one direct successor and one direct predecessor,
    respectively.}
}

@INPROCEEDINGS{Liu2015ICDAR,
  author = {X. Liu and T. Lu},
  title = {Natural Scene character recognition using Markov Random Field},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {396-400},
  month = {Aug},
  abstract = {Text recognition in natural scenes is challenging due to the variations
    on text font, font size, color and scene background. In this paper,
    we propose a novel method to recognize scene characters based on
    born-digital template matching, which is formulated by a Markov Random
    Field framework by combining both local point features and global
    spatial structures of scene characters to improve the accuracy. Then
    the character recognition result of the proposed MRF framework is
    determined by the votes from multiple one-versus-one classifiers.
    The experiments on two benchmark datasets show that the effectiveness
    of the proposed method, which outperforms the recent state-of-the-art
    natural scene recognition methods.},
  doi = {10.1109/ICDAR.2015.7333791},
  keywords = {character recognition;image classification;image matching;Markov processes;natural
    scenes;natural scene character recognition;born-digital template
    matching;Markov random field framework;local point features;global
    spatial structures;MRF framework;multiple one-versus-one classifiers;Image
    recognition;exemplar matching;character recognition;MRF},
  review = {This paper proposes a novel method to recognize scene characters based
    on born-digital template matching, which is formulated by a Markov
    Random Field framework by combining both local point features and
    global spatial structures of scene characters to improve the accuracy.
    Then the character recognition result of the proposed MRF framework
    is determined by the votes from multiple one-versus-one classifiers.}
}

@ARTICLE{Lu2015IJDAR,
  author = {Lu, Shijian and Chen, Tao and Tian, Shangxuan and Lim, Joo-Hwee and
    Tan, Chew-Lim},
  title = {Scene text extraction based on edges and support vector regression},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  year = {2015},
  volume = {18},
  pages = {125--135},
  number = {2},
  month = {Jun},
  abstract = {This paper presents a scene text extraction technique that automatically
    detects, and segments texts from scene images. Three text-specific
    features are designed over image edges with which a set of candidate
    text boundaries is first detected. For each detected candidate text
    boundary, one or more candidate characters are then extracted by
    using a local threshold that is estimated based on the surrounding
    image pixels. The real characters and words are finally identified
    by a support vector regression model that is trained using bags-of-words
    representation. The proposed technique has been evaluated over the
    latest ICDAR-2013 Robust Reading Competition dataset. Experiments
    show that it obtains superior F-measures of 78.19 {\%} and 75.24 {\%}
    (on atom level), respectively, for the scene text detection and segmentation
    tasks.},
  day = {01},
  doi = {10.1007/s10032-015-0237-z},
  issn = {1433-2825},
  review = {Three text-specific features are designed over image edges with which
    a set of candidate text boundaries is first detected. For each detected
    candidate text boundary, one or more candidate characters are then
    extracted by using a local threshold that is estimated based on the
    surrounding image pixels. The real characters and words are finally
    identified by a support vector regression model that is trained using
    bags-of-words representation.},
  url = {https://doi.org/10.1007/s10032-015-0237-z}
}

@INPROCEEDINGS{Mathew2017ICDAR,
  author = {M. Mathew and M. Jain and C. V. Jawahar},
  title = {Benchmarking Scene Text Recognition in Devanagari, Telugu and Malayalam},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {07},
  pages = {42-46},
  month = {Nov},
  abstract = {Inspired by the success of Deep Learning based approaches to English
    scene text recognition, we pose and bench-mark scene text recognition
    for three Indic scripts Devanagari, Telugu and word images rendered
    from Unicode fonts, which are used for training the recognition system.,
    Malayalam. Synthetic and the performance is bench-marked on a new
    IIIT-ILST dataset comprising of hundreds of real scene images containing
    text in the above mentioned scripts. We use a segmentation free,
    hybrid but end-to-end trainable C. N. N.-R. N. N. deep neural network
    for transcribing the word images to the corresponding texts. The
    cropped word images need not be segmented into the sub-word units
    and the error is calculated and backpropagated for the the given
    word image at once. The network is trained using CTC loss, which
    is proven quite effective for sequence-to-sequence transcription
    tasks. The C. N. N. layers in the network learn to extract robust
    feature representations from word images. The sequence of features
    learnt by the convolutional block is transcribed to a sequence of
    labels by the RNN+C. T. C. block. The transcription is not bound
    by word length or a lexicon and is ideal for Indian languages which
    are highly inflectional.},
  doi = {10.1109/ICDAR.2017.364},
  issn = {2379-2140},
  keywords = {backpropagation;convolution;image recognition;image representation;image
    segmentation;image texture;natural language processing;recurrent
    neural nets;text detection;English scene text recognition;bench-mark
    scene text recognition;deep neural network;synthetic word images;Indic
    scripts - Devanagari;Text recognition;Image segmentation;Optical
    character recognition software;Image recognition;Robustness;Machine
    learning;Benchmark testing;Indian Languages;Scene Text;Indic Scripts;Synthetic
    data;CNN-RNN;Text Recognition;OCR},
  review = {This paper introduces a pose and benchmark scene text recognition
    for three Indic scripts (Devanagari, Telugu, and Malayalam) word
    images rendered from Unicode fonts, which are used for training the recognition
    system. The authors use a segmentation free,
    hybrid but end-to-end trainable CNN-RNN deep neural network for transcribing
    the word images to the corresponding texts. The cropped word images
    need not be segmented into sub-word units and the error is calculated
    and backpropagated for the the given word image at once. The network
    is trained using CTC loss, which is proven quite effective for sequence-to-sequence
    transcription tasks. The CNN layers in the network learn to extract
    robust feature representations from word images. The sequence of
    features learnt by the convolutional block is transcribed to a sequence
    of labels by the RNN+CTC block.}
}

@ARTICLE{Milyaev2015IJDAR,
  author = {Milyaev, Sergey and Barinova, Olga and Novikova, Tatiana and Kohli,
    Pushmeet and Lempitsky, Victor},
  title = {Fast and accurate scene text understanding with image binarization
    and off-the-shelf OCR},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  year = {2015},
  volume = {18},
  pages = {169--182},
  number = {2},
  month = {Jun},
  abstract = {While modern off-the-shelf OCR engines show particularly high accuracy
    on scanned text, text detection and recognition in natural images
    still remain a challenging problem. Here, we demonstrate that O.
    C. R. engines can still perform well on this harder task as long
    as an appropriate image binarization is applied to input photographs.
    They propose a new binarization algorithm that is particularly suitable
    for scene text and systematically evaluate its performance along
    with 12 existing binarization methods. While most existing binarization
    techniques are designed specifically either for text detection or
    for recognition of localized text, our method shows very similar
    results for both large images and localized text regions. Therefore,
    it can be applied to large images directly with no need for re-binarization
    of localized text regions. We also propose the real-time variant
    of this method based on linear-time bilateral filtering. Evaluation
    across different metrics on established natural image text recognition
    benchmarks (I. C. D. A. R. 2003 and shows that our simple, I. C.
    D. A. R. 2011) and fast image binarization method combined with off-the-shelf
    OCR engine achieves state-of-the-art performance for end-to-end text
    understanding in natural images and outperforms recent fancy methods.},
  day = {01},
  doi = {10.1007/s10032-015-0240-4},
  issn = {1433-2825},
  review = {This paper proposes a new binarization algorithm that is particularly
    suitable for scene text and systematically evaluate its performance
    along with 12 existing binarization methods. While most existing
    binarization techniques are designed specifically either for text
    detection or for recognition of localized text, this method shows
    very similar results for both large images and localized text regions.
    Therefore, it can be applied to large images directly with no need
    for re-binarization of localized text regions.},
  url = {https://doi.org/10.1007/s10032-015-0240-4}
}

@INPROCEEDINGS{Naeem2017ICDAR,
  author = {M. F. Naeem and N. u. S. Zia and A. A. Awan and F. Shafait and A.
    u. Hasan},
  title = {Impact of Ligature Coverage on Training Practical Urdu OCR Systems},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {131-136},
  month = {Nov},
  abstract = {A major hurdle in the development of practical Urdu Nastaleeq script
    OCR is the lack of transcribed data, which is a pre-requisite for
    training machine learning algorithms. Most of the previous research
    has focused on UPTI, a publicly available dataset with no particular
    focus on performance on real world images. U. P. T. I. contains only
    6,000 of the most probable 26,000 ligatures of Urdu. We build upon
    U. P. T. I. with a new dataset U. P. T. I. 2.0 that covers over
    18,000 ligatures of Urdu Nastaleeq hence covering over 70% of the
    ligatures that can practically occur. We further train a system on
    U. P. T. I. 2.0 and compare its performance against the only commercial
    Urdu Nastaleeq OCR system to date. Bidirectional Long Short-Term
    Memory (BDLSTM) network is employed with Connectionist Temporal Classification
    (CTC) layer as the recognizer. We show that systems trained on UPTI
    2.0 outperform the commercial system.},
  doi = {10.1109/ICDAR.2017.30},
  issn = {2379-2140},
  keywords = {character sets;image classification;learning (artificial intelligence);natural
    language processing;optical character recognition;ligature coverage;practical
    Urdu Nastaleeq script OCR;transcribed data;publicly available data;commercial
    Urdu Nastaleeq OCR system;real world images;bidirectional long short-term
    memory;practical Urdu OCR system training;machine learning algorithm
    training;Urdu Nastaleeq ligatures;BDLSTM network;connectionist temporal
    classification;CTC;UPTI 2.0;Optical character recognition software;Training;Degradation;Hidden
    Markov models;Data models;Error analysis;Speech recognition;Urdu
    OCR;Recurrent neural networks;Ligature coverage;Tesseract;Machine
    Learning;LSTM;Data set},
  review = {Most of the previous research has focused on UPTI, a publicly available
    dataset with no particular focus on performance on real world images.
    UPTI contains only 6,000 of the most probable 26,000 ligatures of
    Urdu. The authors build upon UPTI with a new dataset UPTI 2.0 that
    covers over 18,000 ligatures of Urdu Nastaleeq hence covering over
    70% of the ligatures that can practically occur. The authors further
    train a system on UPTI 2.0 and compare its performance against the
    only commercial Urdu Nastaleeq OCR system to date. Bidirectional
    Long Short-Term Memory (BDLSTM) network is employed with Connectionist
    Temporal Classification (CTC) layer as the recognizer.}
}

@INPROCEEDINGS{Nagaoka2017ICDAR,
  author = {Y. Nagaoka and T. Miyazaki and Y. Sugaya and S. Omachi},
  title = {Text Detection by Faster R-CNN with Multiple Region Proposal Networks},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {06},
  pages = {15-20},
  month = {Nov},
  abstract = {We propose an end-to-end consistently trainable text detection method
    based on the Faster R-CNN. The original Faster R-CNN is an end-to-end
    CNN for fast, We and accurate object detection. By considering the
    characteristics of texts, a novel architecture that make use of its
    ability on object detection is proposed. Although the original Faster
    R-CNN generates region of interests (RoIs) by a region proposal network
    (RPN) using the feature map of the last convolutional layer, the
    proposed method generates RoIs by multiple RPNs using the feature
    maps of multiple convolutional layers. This method uses multiresolution
    feature maps to detect texts of various sizes simultaneously. To
    aggregate the RoIs we introduce RoI-merge layer and this layer enables
    to select valid RoIs from multiple RPNs effectively. In addition,
    a training strategy is proposed for realizing end-to-end training
    and making each RPN be specialized in text region size. Experimental
    results using ICDAR2013/2015 RRC test dataset show that the proposed
    Multi-RPN method improved detection scores and kept almost the same
    detection speed as compared to the original Faster R-CNN and recent
    methods.},
  doi = {10.1109/ICDAR.2017.343},
  issn = {2379-2140},
  keywords = {feature extraction;neural nets;object detection;text detection;multiple
    region proposal networks;end-to-end consistently trainable text detection
    method;end-to-end CNN;RoI;multiple convolutional layers;multiresolution
    feature maps;object detection;Faster R-CNN;Training;Text recognition;Proposals;Object
    detection;Feature extraction;Computer architecture;Kernel;Text detection;Faster
    R-CNN;Region Proposal Network},
  review = {This paper proposes an end-to-end consistently trainable text detection
    method based on the Faster R-CNN. By considering the characteristics
    of texts, a novel architecture that makes use of its ability on object
    detection is proposed. Although the original Faster R-CNN generates
    region of interests (RoIs) by a region proposal network (RPN) using
    the feature map of the last convolutional layer, the proposed method
    generates RoIs by multiple RPNs using the feature maps of multiple
    convolutional layers. This method uses multiresolution feature maps
    to detect texts of various sizes simultaneously. To aggregate the
    RoIs, the authors introduced a RoI-merge layer and this layer enables to select
    valid RoIs from multiple RPNs effectively. In addition, a training
    strategy is proposed for realizing end-to-end training and making
    each RPN be specialized in text region size.}
}

@INPROCEEDINGS{Nayef2017ICDARa,
  author = {N. Nayef and J. Ogier},
  title = {Semantic Text Detection in Born-Digital Images via Fully Convolutional
    Networks},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {859-864},
  month = {Nov},
  abstract = {Traditional layout analysis methods cannot be easily adapted to born-digital
    images which carry properties from both regular document images,
    Traditional and natural scene images. One layout approach for analyzing
    born-digital images is to separate the text layer from the graphics
    layer before further analyzing any of them. In this paper, we propose
    a method for detecting text regions in such images by casting the
    detection problem as a semantic object segmentation problem. The
    text classification is done in a holistic approach using fully convolutional
    networks where the full image is fed as input to the network and
    the output is a pixel heat map of the same input image size. This
    solves the problem of low resolution images and the variability of
    text scale within one image. It also eliminates the need for finding
    interest points, candidate text locations or low level components.
    The experimental evaluation of our method on the ICDAR 2013 dataset
    shows that our method outperforms state-of-the-art methods. The detected
    text regions also allow flexibility to later apply methods for finding
    text components at character, word or textline levels in different
    orientations.},
  doi = {10.1109/ICDAR.2017.145},
  issn = {2379-2140},
  keywords = {convolution;document image processing;feedforward neural nets;image
    classification;image resolution;image segmentation;natural scenes;text
    detection;regular document images;natural scene images;born-digital
    images;text layer;semantic object segmentation problem;text classification;fully
    convolutional networks;input image size;low resolution images;text
    components;semantic text detection;text region detection;text locations;layout
    analysis methods;Heating systems;Kernel;Semantics;Training;Image
    segmentation;Layout;Machine learning},
  review = {This paper proposes a method for detecting text regions in born-digital
    images by casting the detection problem as a semantic object segmentation
    problem. The text classification is done in a holistic approach using
    fully convolutional networks where the full image is fed as input
    to the network and the output is a pixel heat map of the same input
    image size.}
}

@INPROCEEDINGS{Nayef2017ICDARb,
  author = {N. Nayef and F. Yin and I. Bizid and H. Choi and Y. Feng and D. Karatzas
    and Z. Luo and U. Pal and C. Rigaud and J. Chazalon and W. Khlif
    and M. M. Luqman and J. Burie and C. Liu and J. Ogier},
  title = {ICDAR2017 Robust Reading Challenge on Multi-Lingual Scene Text Detection
    and Script Identification - RRC-MLT},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1454-1459},
  month = {Nov},
  abstract = {Text detection, Text and recognition in a natural environment are
    key components of many applications, ranging from business card digitization
    to shop indexation in a street. This competition aims at assessing
    the ability of state-of-the-art methods to detect Multi-Lingual Text
    (MLT) in scene images, such as in contents gathered from the Internet
    media and in modern cities where multiple cultures live and communicate
    together. This competition is an extension of the Robust Reading
    Competition (RRC) which has been held since 2003 both in ICDAR and
    in an online context. The proposed competition is presented as a
    new challenge of the RRC. The dataset built for this challenge largely
    extends the previous RRC editions in many aspects: the multi-lingual
    text, the size of the dataset, the multi-oriented text the wide variety
    of scenes. The dataset is comprised of 18 000 images which contain
    text belonging to 9 languages. The challenge is comprised of three
    tasks related to text detection and script classification. We have
    received a total of 16 participations from the research and industrial
    communities. This paper presents the dataset, the tasks and the findings
    of this RRC-MLT challenge.},
  doi = {10.1109/ICDAR.2017.237},
  issn = {2379-2140},
  keywords = {document image processing;feature extraction;image classification;image
    segmentation;Internet;text analysis;text detection;ICDAR2017 Robust
    Reading challenge;script identification - RRC-MLT;recognition;natural
    environment;multilingual text;scene images;Robust Reading Competition;script
    classification;RRC-MLT challenge;multilingual scene text detection;text
    recognition;Internet media;RRC editions;Task analysis;Proposals;Robustness;Text
    recognition;Internet;Training;Scene Text Detection;Multi-lingual
    Text;Script Identification},
  review = {This competition aims at assessing the ability of state-of-the-art
    methods to detect Multi-Lingual Text (MLT) in scene images, such
    as in contents gathered from the Internet media and in modern cities
    where multiple cultures live and communicate together. This competition
    is an extension of the Robust Reading Competition (RRC).This paper
    presents the dataset, the tasks and the findings of this RRC-MLT
    challenge.}
}

@INPROCEEDINGS{Ohyama2017ICDAR,
  author = {W. Ohyama and S. Iwata and T. Wakabayashi and F. Kimura},
  title = {Detection and Recognition of Arabic Text in Video Frames},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {07},
  pages = {20-24},
  month = {Nov},
  abstract = {The authors have developed an end-to-end system for Arabic text recognition
    in video frames. The end-to-end system consists of the steps for
    text-line detection, word segmentation and word recognition. In order
    to achieve high text recognition accuracy we propose a new scheme
    of integrated text detection-recognition scheme, where the true text-lines
    are detected with as higher recall rate as possible and the false
    words in the false lines are rejected in the successive word recognition
    step. We reported a recognition based transition frame detection
    of Arabic news captions in single channel video images. In this paper
    the recognition system is integrated with n-gram language model and
    extended to text detection/recognition of multi-channel video images.
    The multi-channel, multi-font performance of the system is experimentally
    evaluated using AcTiV-D. and AcTiV-R dataset. The multi-channel text
    detection performance for three channels, France24, Russia Today
    and TunisiaNat1 is 91.29% in (F)-measure. The multi-channel, multi-font
    character recognition performance for these channels is 94.84% in
    F.-measure.},
  doi = {10.1109/ICDAR.2017.360},
  issn = {2379-2140},
  keywords = {character recognition;image segmentation;natural language processing;text
    analysis;text detection;video signal processing;video frames;end-to-end
    system;Arabic text recognition;text-line detection;word segmentation;integrated
    text detection-recognition scheme;successive word recognition step;recognition
    based transition frame detection;Arabic news captions;single channel
    video images;multichannel video images;multichannel text detection
    performance;multifont character recognition performance;n-gram language
    model;AcTiV-D dataset;AcTiV-R dataset;France24;Russia Today;TunisiaNat1;Text
    recognition;Image segmentation;Character recognition;Image recognition;Dogs;TV;Dynamic
    programming},
  review = {The authors have developed an end-to-end system for Arabic text recognition
    in video frames. The end-to-end system consists of the steps for
    text-line detection, word segmentation and word recognition. In order
    to achieve high text recognition accuracy, the authors propose a new scheme
    of integrated text detection-recognition scheme, where the true text-lines
    are detected with as higher recall rate as possible and the false
    words in the false lines are rejected in the successive word recognition
    step. The authors reported a recognition based transition frame detection
    of Arabic news captions in single channel video images. In this paper
    the recognition system is integrated with n-gram language model and
    extended to text detection/recognition of multi-channel video images.}
}

@INPROCEEDINGS{Puigcerver2017ICDAR,
  author = {J. Puigcerver},
  title = {Are Multidimensional Recurrent Layers Really Necessary for Handwritten
    Text Recognition?},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {67-72},
  month = {Nov},
  abstract = {Current state-of-the-art approaches to offline Handwritten Text Recognition
    extensively rely on Multidimensional Long Short-Term Memory networks.
    However, these architectures come with quite an expensive computational
    cost and we observe that they extract features visually similar to
    those of convolutional layers, which are computationally cheaper.
    This suggests that the two-dimensional long-term dependencies, which
    are potentially modeled by multidimensional recurrent layers may
    not be essential to achieve a good recognition accuracy at least
    in the lower layers of the architecture. In this work an alternative
    model is explored that relies only on convolutional and one-dimensional
    recurrent layers that achieves better or equivalent results than
    those of the current state-of-the-art architecture and runs significantly
    faster. In addition, we observe that using random distortions during
    training as synthetic data augmentation dramatically improves the
    accuracy of our model. Thus, are multidimensional recurrent layers
    really necessary for Handwritten Text Recognition? Probably not.},
  doi = {10.1109/ICDAR.2017.20},
  issn = {2379-2140},
  keywords = {feature extraction;handwritten character recognition;recurrent neural
    nets;text analysis;multidimensional recurrent layers;expensive computational
    cost;long-term dependencies;one-dimensional recurrent layers;offline
    handwritten text recognition;multidimensional long short-term memory
    networks;Training;Feature extraction;Computer architecture;Error
    analysis;Text recognition;Computational modeling;Hidden Markov models;recurrent
    neural networks;convolutional neural networks;long short-term memory;handwritten
    text recognition;statistical hypothesis testing},
  review = {In this work, the authors explored an model that relies only on
    convolutional and one-dimensional recurrent layers, which achieved
    better or equivalent results than those of the current state-of-the-art
    architecture and runs significantly faster. In addition, the author
    observed that using random distortions during training as synthetic
    data augmentation dramatically improves the accuracy of the model.}
}

@INPROCEEDINGS{Qin2017ICDAR,
  author = {S. Qin and R. Manduchi},
  title = {Cascaded Segmentation-Detection Networks for Word-Level Text Spotting},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1275-1282},
  month = {Nov},
  abstract = {We introduce an algorithm for word-level text spotting that is able
    to accurately, We and reliably determine the bounding regions of
    individual words of text "in the wild". Our system is formed by the
    cascade of two convolutional neural networks. The first network is
    fully convolutional and is in charge of detecting areas containing
    text. This results in a very reliable but possibly inaccurate segmentation
    of the input image. The second network (inspired by the popular YOLO
    architecture) analyzes each segment produced in the first stage and
    predicts oriented rectangular regions containing individual words.
    No post-processing (e.g. text line grouping) is necessary. With execution
    time of 450 ms for a 1000 by 560 image on a Titan X GPU, our system
    achieves good performance on the ICDAR 2013, 2015 benchmarks [2]
    [1].},
  doi = {10.1109/ICDAR.2017.210},
  issn = {2379-2140},
  keywords = {image segmentation;neural nets;text analysis;text detection;cascaded
    segmentation-detection networks;word-level text spotting;convolutional
    neural networks;oriented rectangular regions;bounding regions;time
    450.0 ms;Image segmentation;Object detection;Computer architecture;Prediction
    algorithms;Training;Reliability;Benchmark testing;scene text detection;convolutional
    neural network},
  review = {This paper introduces an algorithm for word-level text spotting that
    is able to accurately determine the bounding regions of individual
    words of text ``in the wild". The proposed system is formed by the
    cascade of two convolutional neural networks. The first network is
    fully convolutional and is in charge of detecting areas containing
    text. This results in a very reliable but possibly inaccurate segmentation
    of the input image. The second network (inspired by the popular YOLO
    architecture) analyzes each segment produced in the first stage and
    predicts oriented rectangular regions containing individual words.
    No post-processing (e.g. text line grouping) is necessary.}
}

@INPROCEEDINGS{Roy2017ICDAR,
  author = {S. Roy and P. Shivakumara and U. Pal and T. Lu and A. W. B. A. Wahab},
  title = {Temporal Integration for Word-Wise Caption and Scene Text Identification},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {349-354},
  month = {Nov},
  abstract = {Generally video consists of edited text (i.e., caption text) and natural
    text (i.e., scene text) and these two texts differ from one another
    in nature as well as characteristics. Such different behaviors of
    caption and scene texts lead to poor accuracy for text recognition
    in video. In this paper, we explore wavelet decomposition and temporal
    coherency for the classification of caption and scene text. We propose
    wavelet of high frequency sub-bands to separate text candidates that
    are represented by high frequency coefficients in an input word.
    The proposed method studies the distribution of text candidates over
    word images based on the fact that the standard deviation of text
    candidates is high at the first zone, low at the middle zone and
    high at the third zone. This is extracted by mapping standard deviation
    values to 8 equal sized bins formed based on the range of standard
    deviation values. The correlation among bins at the first and second
    levels of wavelets is explored to differentiate caption and scene
    text and for determining the number of temporal frames to be analyzed.
    The properties of caption and scene texts are validated with the
    chosen temporal frames to find the stable property for classification.
    Experimental results on three standard datasets (ICDAR 2015, Y. V.
    T. and show that the proposed method outperforms the existing methods
    in terms of classification rate, License Plate Video) and improves
    recognition rate significantly based on classification results.},
  doi = {10.1109/ICDAR.2017.65},
  issn = {2379-2140},
  keywords = {feature extraction;image classification;text analysis;text detection;video
    signal processing;wavelet transforms;text candidates;temporal coherency;wavelet
    decomposition;natural text;temporal integration;recognition rate;classification
    rate;word images;temporal frames;high frequency sub-bands;text recognition;caption
    text;edited text;scene text identification;word-wise caption;scene
    texts;standard deviation values;high frequency coefficients;Text
    recognition;Standards;Image color analysis;Optical character recognition
    software;Image recognition;Feature extraction;Image segmentation;Caption
    text;Scene text;Wavelet coefficients;Temporal frames;Caption text
    classification;Scene text classification},
  review = {This paper explores wavelet decomposition and temporal coherency for
    the classification of caption and scene text. The authors propose
    wavelet of high frequency sub-bands to separate text candidates that
    are represented by high frequency coefficients in an input word.
    The proposed method studies the distribution of text candidates over
    word images based on the fact that the standard deviation of text
    candidates is high at the first zone, low at the middle zone and
    high at the third zone. This is extracted by mapping standard deviation
    values to 8 equal sized bins formed based on the range of standard
    deviation values. The correlation among bins at the first and second
    levels of wavelets is explored to differentiate caption and scene
    text and for determining the number of temporal frames to be analyzed.}
}

@INPROCEEDINGS{Sabir2017ICDAR,
  author = {E. Sabir and S. Rawls and P. Natarajan},
  title = {Implicit Language Model in LSTM for OCR},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {07},
  pages = {27-31},
  month = {Nov},
  abstract = {Neural networks have become the technique of choice for OCR, but many
    aspects of how and why they deliver superior performance are still
    unknown. One key difference between current neural network techniques
    using LSTMs and the previous state-of-the-art HMM systems is that
    HMM systems have a strong independence assumption. In comparison
    LSTMs have no explicit constraints on the amount of context that
    can be considered during decoding. In this paper we show that they
    learn an implicit LM and attempt to characterize the strength of
    the LM in terms of equivalent n-gram context. We show that this implicitly
    learned language model provides a 2.4% CER improvement on our synthetic
    test set when compared against a test set of random characters (i.e.
    not naturally occurring sequences) and that the LSTM learns to use
    up to 5 characters of context (which is roughly 88 frames in our
    configuration). We believe that this is the first ever attempt at
    characterizing the strength of the implicit LM in LSTM based OCR
    systems.},
  doi = {10.1109/ICDAR.2017.361},
  issn = {2379-2140},
  keywords = {hidden Markov models;neural nets;optical character recognition;HMM
    systems;neural network techniques;OCR systems;strong independence
    assumption;neural networks;LSTM;implicit language model;Optical character
    recognition software;Hidden Markov models;Training;Context modeling;Task
    analysis;Character recognition;Testing;LSTM;Language Model;OCR},
  review = {In this paper, the authors show that current neural network techniques using
    LSTMs learn an implicit LM and attempt to characterize the strength
    of the LM in terms of equivalent n-gram context. The authors show that this
    implicitly learned language model provides a 2.4% CER improvement
    on their synthetic test set when compared against a test set of random
    characters (i.e. not naturally occurring sequences) and that the
    LSTM learns to use up to 5 characters of context (which is roughly
    88 frames in their configuration)}
}

@INPROCEEDINGS{Sharma2015ICDARa,
  author = {N. Sharma and R. Mandal and R. Sharma and U. Pal and M. Blumenstein},
  title = {ICDAR2015 Competition on Video Script Identification (CVSI 2015)},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {1196-1200},
  month = {Aug},
  abstract = {This paper presents the final results of the ICDAR 2015 Competition
    on Video Script Identification. A description and performance
    of the participating systems in the competition are reported. The
    general objective of the competition is to evaluate and benchmark
    the available methods on word-wise video script identification. It
    also provides a platform for researchers around the globe to particularly
    address the video script identification problem and video text recognition
    in general. The competition was organised around four different tasks
    involving various combinations of scripts comprising tri-script and
    multi-script scenarios. The dataset used in the competition comprised
    ten different scripts. In total, six systems were received from five
    participants over the tasks offered. This report details the competition
    dataset specifications, evaluation criteria summary of the participating
    systems and their performance across different tasks. The systems
    submitted by Google Inc. were the winner of the competition for all
    the tasks, whereas the systems received from Huazhong University
    of Science and (HUST), Technology and were very close competitors.,
    Computer Vision Center (CVC)},
  doi = {10.1109/ICDAR.2015.7333950},
  keywords = {computer vision;video signal processing;ICDAR2015 competition;video
    script identification;general objective;video text recognition;competition
    dataset specifications;Google Inc;Huazhong University of Science
    and Technology;HUST;computer vision center;CVC;Google;Video scripts
    identification;multi-lingual OCR;multi-lingual video-text;competition},
  review = {This paper presents the final results of the ICDAR 2015 Competition
    on Video Script Identification. A description, This and performance
    of the participating systems in the competition are reported.}
}

@INPROCEEDINGS{Sharma2015ICDARb,
  author = {N. Sharma and R. Mandal and R. Sharma and P. P. Roy and U. Pal and
    M. Blumenstein},
  title = {Multi-lingual text recognition from video frames},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {951-955},
  month = {Aug},
  abstract = {Text recognition from video frames is a challenging task due to low
    resolution, blur, complex and coloured backgrounds, noise, to mention
    a few. Consequently the traditional ways of text recognition from
    scanned documents having simple backgrounds fails when applied to
    video text. Although there are various techniques available for text
    recognition from handwritten and printed documents with simple backgrounds,
    text recognition from video frames has not been comprehensively investigated,
    especially for multi-lingual videos. In this paper we present a technique
    for multi-lingual video text recognition which involves script identification
    in the first stage followed by word and character recognition and
    finally the results are refined using a post-processing technique.
    Considering the inherent problems in videos, a Spatial Pyramid Matching
    (SPM) based technique, using patch-based S. I. F. T. descriptors
    and SVM classifier, is employed for script identification. In the
    next stage, a Hidden Markov Model (HMM) based approach is used for
    word and character recognition, which utilizes the context information.
    Finally, a lexicon-based post-processing technique is applied to
    verify and refine the word recognition results. The proposed method
    was tested on a dataset comprising of 4800 words from three different
    scripts, namely, Roman (English) Hindi and script identification
    results obtained are encouraging. The word, Bengali. The and character
    recognition results are also encouraging considering the complexity
    and problems associated with video text processing.},
  doi = {10.1109/ICDAR.2015.7333902},
  keywords = {character recognition;hidden Markov models;image classification;image
    matching;support vector machines;transforms;video signal processing;video
    frames;multilingual video text recognition;script identification;character
    recognition;word recognition;spatial pyramid matching;SPM based technique;patch-based
    SIFT descriptors;SVM classifier;hidden Markov model;HMM;lexicon-based
    post-processing technique;Roman;Hindi;Bengali;Hidden Markov models;Character
    recognition;Optical character recognition software;Accuracy;Image
    segmentation;Handwriting recognition},
  review = {This paper presents a technique for multi-lingual video text recognition
    which involves script identification in the first stage followed
    by word and character recognition and finally the results are refined
    using a post-processing technique. Considering the inherent problems
    in videos, a Spatial Pyramid Matching (SPM) based technique, using
    patch-based SIFT descriptors and SVM classifier, is employed for
    script identification. In the next stage, a Hidden Markov Model (HMM)
    based approach is used for word and character recognition, which
    utilizes the context information. Finally, a lexicon-based post-processing
    technique is applied to verify and refine the word recognition results.}
}

@INPROCEEDINGS{Su2017ICDAR,
  author = {F. Su and W. Ding and L. Wang and S. Shan and H. Xu},
  title = {Text Proposals Based on Windowed Maximally Stable Extremal Region
    for Scene Text Detection},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {376-381},
  month = {Nov},
  abstract = {The generation of text proposals (i.e. local candidate regions most
    likely containing textual components) is one critical, The and prerequisite
    step in scene text detection task. As one popular text proposal algorithm,
    the Maximally Stable Extremal Region (MSER), has been exploited by
    many successful text detection methods while on the other hand has
    difficulties in handling complicated scene text involving touching
    characters and characters composed of multiple unconnected parts
    (e.g. Chinese characters and text in dot matrix fonts). In this paper,
    we propose a novel text proposal method for localizing text in natural
    images, which integrates the M. S. E. R. algorithm with the multi-scale
    sliding window framework and efficiently extracts Windowed Maximally
    Stable Extremal Regions (WMSERs) as text proposals. We further present
    effective proposal filtering and grouping algorithms for exploiting
    WMSER-based proposals in text detection task. Experiments on public
    scene text datasets demonstrate the promising aspects of the proposed
    method in dealing with complicated scene text.},
  doi = {10.1109/ICDAR.2017.69},
  issn = {2379-2140},
  keywords = {character sets;feature extraction;image segmentation;text analysis;text
    detection;text proposals;windowed maximally stable extremal region;scene
    text detection task;text proposal method;Windowed Maximally Stable
    Extremal Regions;public scene text datasets;Proposals;Microsoft Windows;Feature
    extraction;Erbium;Histograms;Task analysis;Filtering;text proposal;scene
    text detection;MSER;sliding window;random walk},
  review = {This paper proposes a novel text proposal method for localizing text
    in natural images, which integrates the MSER algorithm with the multi-scale
    sliding window framework and efficiently extracts Windowed Maximally
    Stable Extremal Regions (WMSERs) as text proposals. The authors further present
    effective proposal filtering and grouping algorithms for exploiting
    WMSER-based proposals in text detection task.}
}

@INPROCEEDINGS{Su2015ICDAR,
  author = {F. Su and H. Xu},
  title = {Robust seed-based stroke width transform for text detection in natural
    images},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {916-920},
  month = {Aug},
  abstract = {Text detection in natural scene images is challenging due to the significant
    variations of the appearance of the text itself, Text and its interaction
    with the context. The popular stroke width transform (SWT) algorithm
    is highly efficient but sensitive to the defects of the edges extracted
    from the input image when searching for the matching edge pixels
    for computing potential stroke width. In this paper, we propose a
    novel seed-based variant of S. W. T. that enhances significantly
    the robustness of the original algorithm to complicated image contextual
    interference and varied text appearance. We first search for the
    seed segment of strokes, which is defined as a consecutive sequence
    of neighbouring rays (pairs of edge pixels) with regular length and
    satisfying certain constraints and grow from them to localize more
    stroke segments. We then exploit the principal width and direction
    information of stroke captured by the stroke segments detected to
    rectify inaccurate stroke width and recover missed stroke parts,
    which are resulted from erroneous and noisy edges in complex natural
    images. The stroke segments detected are also exploited to improve
    the accuracy of candidate character localization. The experimental
    results on public datasets demonstrated the effectiveness of the
    proposed method.},
  doi = {10.1109/ICDAR.2015.7333895},
  keywords = {edge detection;image matching;text analysis;transforms;stroke width
    transform;robust seed;text detection;natural images;SWT algorithm;edges
    extraction;matching edge pixels;computing potential stroke width;image
    contextual interference;stroke segments;direction information;inaccurate
    stroke;noisy edges;character localization;public datasets;Robustness;Context;Image
    edge detection;Transforms;Joining processes;Image color analysis;Feature
    extraction},
  review = {This paper proposes a novel seed-based variant of SWT that enhances
    significantly the robustness of the original algorithm to complicated
    image contextual interference and varied text appearance. The method
    first searches for the seed segment of strokes, which is defined
    as a consecutive sequence of neighbouring rays (pairs of edge pixels)
    with regular length and satisfying certain constraints and grow from
    them to localize more stroke segments. It then exploits the principal
    width and direction information of stroke captured by the stroke
    segments detected to rectify inaccurate stroke width and recover
    missed stroke parts, which are resulted from erroneous and noisy
    edges in complex natural images. The stroke segments detected are
    also exploited to improve the accuracy of candidate character localization.}
}

@INPROCEEDINGS{Sung2015ICDAR,
  author = {M. Sung and B. Jun and H. Cho and D. Kim},
  title = {Scene text detection with robust character candidate extraction method},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {426-430},
  month = {Aug},
  abstract = {The maximally stable extremal region (MSER) method has been widely
    used to extract character candidates, but because of its requirement
    for maximum stability, high text detection performance is difficult
    to obtain. To overcome this problem we propose a robust character
    candidate extraction method that performs E. R. tree construction
    sub-path partitioning sub-path pruning and character candidate selection
    sequentially. Then, we use the AdaBoost trained character classifier
    to verify the extracted character candidates. Then, we use heuristics
    to refine the classified character candidates and group the refined
    character candidates into text regions according to their geometric
    adjacency and color similarity. We also apply the proposed text detection
    method to two different color channels Cr and Cb
    and obtain the final detection result by combining the detection
    results on the three different channels. The proposed text detection
    method on ICDAR 2013 dataset achieved 8%, 1% and 4% improvements
    in recall rate, precision rate and f-score, respectively, compared
    to the state-of-the-art methods.},
  doi = {10.1109/ICDAR.2015.7333797},
  keywords = {feature extraction;learning (artificial intelligence);text detection;scene
    text detection;maximally stable extremal region method;MSER method;robust
    character candidate extraction method;ER tree construction;character
    candidate selection;AdaBoost trained character classifier;Erbium;Classification
    algorithms;Image resolution},
  review = {This paper proposes a robust character candidate extraction method
    that performs ER tree construction sub-path partitioning sub-path
    pruning and character candidate selection sequentially. Then, the
    method uses the AdaBoost trained character classifier to verify the
    extracted character candidates. Then,  heuristics are used to refine
    the classified character candidates and group the refined character
    candidates into text regions according to their geometric adjacency
    and color similarity. The proposed text detection method is applied
    to two different color channels Cr and Cb and
    obtain the final detection result by combining the detection results
    on the three different channels.}
}

@INPROCEEDINGS{Turki2017ICDAR,
  author = {H. Turki and M. Ben Halima and A. M. Alimi},
  title = {Text Detection Based on MSER and CNN Features},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {949-954},
  month = {Nov},
  abstract = {Text detection in natural scenes holds great importance in the field
    of research, Text and still remains a challenge and an important
    task because of size, various fonts, line orientation different illumination
    conditions weak characters and complex backgrounds in image. The
    contribution of our proposed method is to filtering out complex backgrounds
    by combining three strategies. These are enhancing the edge candidate
    detection in HSV space color using the fractal dimension (FD) to
    transform the image intensities, then using MSER candidate detection
    to get different masks applied in HSV space color as well as gray
    color. After that, we opt for the Stroke Width Transform (SWT) and
    heuristic filtering. Such strategies are followed so as to maximize
    the capacity of zones text pixels candidates and distinguish between
    text boxes and the rest of the image. The components selected non
    text are filtered by classifying the characters candidates using
    Support Vector Machines (SVM) exploring Convolutional Neural Networks
    (CNN) features and Histogram of Oriented Gradients (HOG) vector features.
    We use the technique of word grouping who the boundary box localization
    select different words in the image where false positives text blocks
    are eliminated by geometrical properties. The evaluation of the proposed
    method demonstrate the effectiveness of our method for complex foreground
    through the experimental results tested on three benchmarks ICDAR2013,
    ICDAR2015 and MSRA-TD500.},
  doi = {10.1109/ICDAR.2017.159},
  issn = {2379-2140},
  keywords = {edge detection;feature extraction;image classification;image colour
    analysis;image filtering;natural scenes;neural nets;support vector
    machines;text detection;transforms;HSV space color;gray color;heuristic
    filtering;zones text pixels candidates;text boxes;characters candidates;positives
    text blocks;complex foreground;text detection;natural scenes;line
    orientation;different illumination conditions;weak characters;edge
    candidate detection;image intensities;MSER candidate detection;oriented
    gradients vector features;CNN features;complex backgrounds filtering;fractal
    dimension;stroke width transform;support vector machines;convolutional
    neural networks;histogram of oriented gradients;word grouping;boundary
    box localization;ICDAR2013 benchmarks;ICDAR2015 benchmarks;MSRA-TD500
    benchmarks;Image edge detection;Image color analysis;Merging;Fractals;Feature
    extraction;Filtering;Support vector machines;Scene text detection;MSERs;HOG;CNN;SVM},
  review = {The contribution of their proposed method is to filtering out complex
    backgrounds by combining three strategies. These are enhancing the
    edge candidate detection in HSV space color using the fractal dimension
    (FD) to transform the image intensities, then using MSER candidate
    detection to get different masks applied in HSV space color as well
    as gray color. After that, they opt for the Stroke Width Transform
    (SWT) and heuristic filtering. Such strategies are followed so as
    to maximize the capacity of zones text pixels candidates and distinguish
    between text boxes and the rest of the image. The components selected
    non text are filtered by classifying the characters candidates using
    Support Vector Machines (SVM) exploring Convolutional Neural Networks
    (CNN) features and Histogram of Oriented Gradients (HOG) vector features.
    The authors use the technique of word grouping who the boundary box localization
    select different words in the image where false positives text blocks
    are eliminated by geometrical properties.}
}

@INPROCEEDINGS{Van2017ICDAR,
  author = {D. N. Van and S. Lu and X. Bai and N. Ouarti and M. Mokhtari},
  title = {Max-Pooling Based Scene Text Proposal for Scene Text Detection},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1295-1300},
  month = {Nov},
  abstract = {Automatic reading texts in scenes is an attracting increasing interest
    in recent years due to various context awareness applications. Leverage
    on the advantages of object proposal in generic object detection,
    we propose a max-pooling based scene text proposal technique aiming
    for automatic extraction of texts in scenes. Given a scene image,
    a max-pooling based grouping technique is designed to search for
    scene text proposals within a feature map which is computed from
    image edges. Searched proposals are then ranked by a scoring function
    that is defined based on the histogram of oriented gradient. The
    proposed technique has been evaluated on two publicly available scene
    text datasets including the ICDAR2015 dataset and the Street View
    Text (SVT) dataset. Experiments show that the proposed technique
    obtains superior proposal performance as compared with state-of-the-arts,
    especially when a small number of proposals is selected. In addition,
    it also obtains state-of-the-art scene text spotting when integrated
    with a scene text recognition model.},
  doi = {10.1109/ICDAR.2017.213},
  issn = {2379-2140},
  keywords = {edge detection;feature extraction;gradient methods;object detection;text
    detection;object detection;scene text datasets;max-pooling based
    scene text proposal technique;scene Text detection;scene text recognition
    model;state-of-the-art scene text spotting;Street View Text dataset;scene
    text proposals;max-pooling based grouping technique;Proposals;Image
    edge detection;Training;Feature extraction;Correlation;Electronic
    mail;Text recognition;Max-pooling grouping;Scene text proposal;Scene
    text detection},
  review = {This paper proposes a max-pooling based scene text proposal technique
    aiming for automatic extraction of texts in scenes. Given a scene
    image, a max-pooling based grouping technique is designed to search
    for scene text proposals within a feature map which is computed from
    image edges. Searched proposals are then ranked by a scoring function
    that is defined based on the histogram of oriented gradient.}
}

@INPROCEEDINGS{Wang2017ICDARa,
  author = {C. Wang and F. Yin and C. Liu},
  title = {Scene Text Detection with Novel Superpixel Based Character Candidate
    Extraction},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {929-934},
  month = {Nov},
  abstract = {Maximally stable extremal region (MSER) is popularly used for candidate
    character candidate extraction in scene text detection. Its requirement
    of maximum stability hinders high performance on images of high variability.
    In this paper, we propose a novel character candidate extraction
    method based on superpixel segmentation and hierarchical clustering.
    The proposed superpixel segmentation algorithm for scene text image
    takes advantage of the color consistency of characters and fuses
    color and edge information. Based on superpixel segmentation, character
    candidates are extracted by single-link clustering. To improve the
    accuracy of non-text candidate filtering, we use a deep convolutional
    neural networks (DCNN) classifier and double threshold strategy for
    classification. Experimental results on public datasets demonstrate
    that the proposed superpixel based method performs better than MSER
    in character candidate extraction and the proposed system achieves
    competitive performance compared to state-of-the-art methods.},
  doi = {10.1109/ICDAR.2017.156},
  issn = {2379-2140},
  keywords = {feature extraction;image classification;image colour analysis;image
    segmentation;neural nets;text analysis;text detection;scene text
    detection;candidate character candidate extraction;character candidate
    extraction method;superpixel segmentation algorithm;scene text image;edge
    information;character candidates;maximally stable extremal region;superpixel
    based character candidate extraction;single-link clustering;deep
    convolutional neural networks classifier;double threshold strategy;nontext
    candidate filtering;Image segmentation;Image color analysis;Clustering
    algorithms;Image edge detection;Erbium;Feature extraction;Computational
    efficiency;superpixel;character candidate extraction;text detection},
  review = {This paper proposes a novel character candidate extraction method
    based on superpixel segmentation and hierarchical clustering. The
    proposed superpixel segmentation algorithm for scene text image takes
    advantage of the color consistency of characters and fuses color
    and edge information. Based on superpixel segmentation, character
    candidates are extracted by single-link clustering. To improve the
    accuracy of non-text candidate filtering, the authors use a deep convolutional
    neural networks (DCNN) classifier and double threshold strategy for
    classification.}
}

@INPROCEEDINGS{Wang2015ICDARb,
  author = {Q. Wang and Y. Lu and S. Sun},
  title = {Text detection in nature scene images using two-stage nontext filtering},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {106-110},
  month = {Aug},
  abstract = {We present a text detection method in natural scene images based on
    two-stage nontext filtering. Firstly, we detect multi-channel maximally
    stable extremal regions (MSERs) as character candidates. To reduce
    the amount of repeating components, we merge the MSERs by choosing
    the most character-like ones when overlap happens. Then nontext components
    are filtered out by a two-stage labeling procedure wherein we combine
    random forests with C. R. F. Finally components labeled as text are
    grouped into words by an edge-cut strategy and false positives are
    eliminated by a HOG-based classifier. The experimental results on
    the ICDAR2013 database show the effectiveness of the proposed method.},
  doi = {10.1109/ICDAR.2015.7333735},
  keywords = {image classification;learning (artificial intelligence);natural scenes;text
    detection;MSER;two-stage nontext filtering;text detection method;natural
    scene images;multichannel maximally stable extremal regions;character
    candidates;nontext components;two-stage labeling procedure;random
    forests;CRF;edge-cut strategy;HOG-based classifier;ICDAR2013 database;Image
    edge detection;text detection;MSERs;CRF;nontext filtering;random
    forests;edge-cut},
  review = {This paper presents a text detection method in natural scene images
    based on two-stage nontext filtering. Firstly, the method detects
    multi-channel maximally stable extremal regions (MSERs) as character
    candidates. To reduce the amount of repeating components, MSERs are
    merged by choosing the most character-like ones when overlap happens.
    Then nontext components are filtered out by a two-stage labeling
    procedure wherein random forests are combined with CRF. Finally components
    labeled as text are grouped into words by an edge-cut strategy and
    false positives are eliminated by a HOG-based classifier.}
}

@INPROCEEDINGS{Wang2017ICDAR,
  author = {X. Wang and Y. Jiang and S. Yang and X. Zhu and W. Li and P. Fu and
    H. Wang and Z. Luo},
  title = {End-to-End Scene Text Recognition in Videos Based on Multi Frame
    Tracking},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1255-1260},
  month = {Nov},
  abstract = {Text detection, Text and recognition in scene images and videos attract
    much attention in computer vision recently. However, most existing
    text detection and recognition methods only focus on static images.
    In this paper an end-to-end scene text recognition method based on
    multi frame tracking is proposed for text in videos, in which temporal
    information is employed to improve performance. First, an end-to-end
    text recognition method based on a unified deep neural network is
    used to detect and recognize text in each frame of the input video.
    Then, multi frame text tracking is employed through associations
    of texts in current frame and several previous frames to obtain final
    results. Experiments on ICDAR datasets demonstrate that the proposed
    method outperforms the state-of-the-art methods in end-to-end video
    text recognition.},
  doi = {10.1109/ICDAR.2017.207},
  issn = {2379-2140},
  keywords = {computer vision;feature extraction;neural nets;object tracking;text
    analysis;text detection;video signal processing;end-to-end video
    text recognition;static images;multiframe text tracking;end-to-end
    scene text recognition;text detection;unified deep neural network;Text
    recognition;Videos;Video sequences;Image recognition;Neural networks;Trajectory;Convolution;end-to-end
    text recognition;text in videos;deep neural network;multi frame tracking},
  review = {This paper presents an end-to-end scene text recognition method based
    on multi frame tracking for texts in videos, in which temporal information
    is employed to improve performance. First, an end-to-end text recognition
    method based on a unified deep neural network is used to detect and
    recognize text in each frame of the input video. Then, multi frame
    text tracking is employed through associations of texts in current
    frame and several previous frames to obtain final results.}
}

@INPROCEEDINGS{Wu2017ICDARa,
  author = {D. Wu and R. Wang and P. Dai and Y. Zhang and X. Cao},
  title = {Deep Strip-Based Network with Cascade Learning for Scene Text Localization},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {826-831},
  month = {Nov},
  abstract = {Scene text detection is currently a popular research topic in the
    computer vision community. However, it is a challenging task due
    to the variations of texts and clutter backgrounds. In this paper,
    we propose a novel framework for scene text localization. Based on
    the region proposal network, a Strip-based Text Detection Network
    (STDN) is developed with vertical anchor mechanism to predict the
    text/non-text strip-shaped proposals. Meanwhile we incorporate the
    recurrent neural network layers in the proposed network to refine
    the predicted results. Specifically hard example mining is performed
    to train the S. T. D. N. with cascade learning which has a remarkable
    improvement in precision. Besides we exploit a clustering algorithm
    to generate anchor dimensions spontaneously without hand-picking
    which is portable and time-saving. The text detection framework achieves
    the state-of-the-art performance on ICDAR2013 with 0.89 F-measure.},
  doi = {10.1109/ICDAR.2017.140},
  issn = {2379-2140},
  keywords = {character recognition;computer vision;data mining;feature extraction;image
    segmentation;learning (artificial intelligence);pattern clustering;recurrent
    neural nets;text analysis;text detection;scene text detection;computer
    vision community;scene text localization;region proposal network;vertical
    anchor mechanism;recurrent neural network layers;cascade learning;text
    detection framework;text detection network;text-nontext strip-shaped
    proposals;deep strip-Based Network;STDN;Proposals;Training;Recurrent
    neural networks;Task analysis;Clustering algorithms;Microsoft Windows;Strips;Scene
    text detection;CNN;RNN;Hard Example Mining},
  review = {This paper proposes a novel framework for scene text localization.
    Based on the region proposal network, a Strip-based Text Detection
    Network (STDN) is developed with vertical anchor mechanism to predict
    the text/non-text strip-shaped proposals. Meanwhile the authors propose
    to incorporate the recurrent neural network layers in the proposed
    network to refine the predicted results. Specifically hard example
    mining is performed to train the STDN with cascade learning which
    has a remarkable improvement in precision. Besides the authors exploit
    a clustering algorithm to generate anchor dimensions spontaneously
    without hand-picking which is portable and time-saving.}
}

@INPROCEEDINGS{Wu2017ICDARb,
  author = {Y. Wu and W. Wang and S. Palaiahnakote and T. Lu},
  title = {A Robust Symmetry-Based Method for Scene/Video Text Detection through
    Neural Network},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1249-1254},
  month = {Nov},
  abstract = {Text detection in video/scene images has gained a significant attention
    in the field of image processing, Text and document analysis due
    to the inherent challenges caused by variations in contrast, orientation,
    background text type font type non-uniform illumination and so on.
    In this paper, we propose a novel text detection method to explore
    symmetry property and appearance features of text for improved accuracy
    and robustness. First, the proposed method explores Extremal Regions
    (ER) for detecting text candidates in images. Then we propose a novel
    feature named as Multi-domain Strokes Symmetry Histogram (MSSH) for
    each text candidate, which describes the inherent symmetry property
    of stroke pixel pairs in gray gradient and frequency domains. Furthermore,
    deep convolutional features are extracted to describe the appearance
    for each text candidate. We further fuse them by Auto-Encoder network
    to define a more discriminative text descriptor for classification.
    Finally, the proposed method constructs text lines based on the classification
    results. We demonstrate the effectiveness and robustness detection
    results of our proposed method by testing on four different benchmark
    databases.},
  doi = {10.1109/ICDAR.2017.206},
  issn = {2379-2140},
  keywords = {feature extraction;image classification;neural nets;text detection;neural
    network;image processing;document analysis;nonuniform illumination;text
    candidate;inherent symmetry property;deep convolutional features;discriminative
    text descriptor;robustness detection results;text detection method;robust
    symmetry-based method;scene-video text detection;video-scene images;text
    type illumination;font type illumination;multidomain strokes symmetry
    histogram;auto-encoder network;Feature extraction;Erbium;Robustness;Histograms;Fuses;Image
    color analysis;Transforms;text detection;symmetry property;convolutional
    network;deep learning;auto-encoder network},
  review = {This paper proposes a novel text detection method to explore symmetry
    property and appearance features of text for improved accuracy and
    robustness. First, the proposed method explores Extremal Regions
    (ER) for detecting text candidates in images. Then a novel feature,
    named as Multi-domain Strokes Symmetry Histogram (MSSH), is proposed
    for each text candidate, which describes the inherent symmetry property
    of stroke pixel pairs in gray gradient and frequency domains. Furthermore,
    deep convolutional features are extracted to describe the appearance
    for each text candidate. These features are combined by Auto-Encoder
    network to define a more discriminative text descriptor for classification.
    Finally, the proposed method constructs text lines based on the classification
    results.}
}

@INPROCEEDINGS{Yousfi2015ICDARa,
  author = {S. Yousfi and S. Berrani and C. Garcia},
  title = {Deep learning and recurrent connectionist-based approaches for Arabic
    text recognition in videos},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {1026-1030},
  month = {Aug},
  abstract = {This paper focuses on recognizing Arabic embedded text in videos.
    The proposed methods proceed without applying any prior pre-processing
    operations or character segmentation. Difficulties related to the
    video or text properties are faced using a learned robust representation
    of the input text image. This is performed using Convolutional Neural
    Networks, This and are computed using a multi-scale sliding window
    scheme. A connectionist recurrent approach is then used. It is trained
    to predict correct transcriptions of the input image from the associated
    sequence of features. Proposed methods are extensively evaluated
    on a large video database recorded from several Arabic TV channels.,
    Deep Auto-Encoders. Features},
  doi = {10.1109/ICDAR.2015.7333917},
  keywords = {feature extraction;image recognition;image representation;image sequences;natural
    language processing;text analysis;video signal processing;visual
    databases;recurrent connectionist;deep learning;video Arabic text
    recognition;Arabic embedded text;character segmentation;input text
    image;convolutional neural networks;deep autoencoders;multiscale
    sliding window scheme;associated sequence;video database;Arabic TV
    channels;Hidden Markov models},
  review = {This paper focuses on recognizing Arabic embedded text in videos.
    The proposed methods proceed without applying any prior pre-processing
    operations or character segmentation. Difficulties related to the
    video or text properties are faced using a learned robust representation
    of the input text image. This is performed using Convolutional Neural
    Networks, which are computed using a multi-scale sliding window scheme.
    A connectionist recurrent approach is then used. It is trained to
    predict correct transcriptions of the input image from the associated
    sequence of features.}
}

@INPROCEEDINGS{Yousfi2015ICDARb,
  author = {S. Yousfi and S. Berrani and C. Garcia},
  title = {ALIF: A dataset for Arabic embedded text recognition in TV broadcast},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {1221-1225},
  month = {Aug},
  abstract = {This paper proposes a dataset, called ALIF, for Arabic embedded text
    recognition in TV broadcast. The dataset is publicly available
    for a non-commercial use. It is composed of a large number of manually
    annotated text images that were extracted from Arabic T. V. broadcast.
    It is the first public dataset dedicated to the development and the
    evaluation of video Arabic OCR techniques. Text images in the dataset
    are highly variable in terms of text characteristics (fonts, sizes,
    colors...) and acquisition conditions (background complexity, low
    resolution, non-uniform luminosity and contrast...). Moreover, an
    important part of the dataset is finely annotated, i.e. the text
    in an image is segmented into characters paws and words and each
    segment is labeled. The dataset can hence be used for both segmentation-based
    and segmentation-free text recognition techniques. In order to illustrate
    how the ALIF dataset can be used, the results of an evaluation study
    that we have conducted on different techniques for Arabic text recognition
    are also presented.},
  doi = {10.1109/ICDAR.2015.7333958},
  keywords = {image segmentation;optical character recognition;text detection;video
    signal processing;Arabic embedded text recognition;TV broadcast;manually
    annotated text images;video Arabic OCR techniques;segmentation-free
    text recognition techniques;segmentation-based text recognition techniques;ALIF
    dataset;Optical character recognition software;Artificial intelligence;Iron;Metadata;FAA;Chlorine;Artificial
    neural networks;Embedded text recognition;video OCR;dataset;Arabic},
  review = {This paper proposes a dataset, called ALIF, for Arabic embedded text
    recognition in TV broadcast. It is composed of a large number of
    manually annotated text images that were extracted from Arabic T.
    V. broadcast. Text images in the dataset are highly variable in terms
    of text characteristics (fonts, sizes, colors...) and acquisition
    conditions (background complexity, low resolution, non-uniform luminosity
    and contrast...). Moreover, an important part of the dataset is finely
    annotated, i.e. the text in an image is segmented into characters
    paws and words and each segment is labeled.}
}

@INPROCEEDINGS{Zayene2017ICDAR,
  author = {O. Zayene and J. Hennebert and R. Ingold and N. E. BenAmara},
  title = {ICDAR2017 Competition on Arabic Text Detection and Recognition in
    Multi-Resolution Video Frames},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1460-1465},
  month = {Nov},
  abstract = {This paper describes the multi-resolution Arabic Text detection, This
    and recognition in Video Competition-AcTiVComp held in the context
    of the 14<sup>th</sup> International Conference on Document Analysis
    and Recognition (ICDAR' 2017), during November 9-15, 2017 in Kyoto
    Japan. The main objective of this competition is to evaluate the
    performance of participants' algorithms for automatically detecting
    and recognizing Arabic texts in video frames using the freely available
    Arabic-Text-in-Video (AcTiV) dataset. A first edition was held in
    the framework of the 23<sup>rd</sup> International Conference on
    Pattern Recognition (ICPR'2016). Three groups with five systems are
    participating to the second edition of AcTiVComp. These systems are
    tested in a blind manner on a closed-subset of the AcTiV database,
    which is unknown to all participants. In addition to the experimental
    setup and observed results, we also provide a short description of
    the participating groups and their systems.},
  doi = {10.1109/ICDAR.2017.238},
  issn = {2379-2140},
  keywords = {feature extraction;handwriting recognition;natural language processing;pattern
    recognition;text analysis;text detection;multiresolution video frames;Arabic
    text detection;Arabic-Text-in-Video;Arabic text recognition;Protocols;Text
    recognition;Measurement;Task analysis;Training;TV;XML;Arabic Text
    Detection;Arabic Text Recognition;Video-OCR;AcTiV dataset;ICDAR competition},
  review = {This paper describes the multi-resolution Arabic Text detection, This
    and recognition in Video Competition-AcTiVComp held in the context
    of the 14<sup>th</sup> International Conference on Document Analysis
    and Recognition (ICDAR' 2017)}
}

@INPROCEEDINGS{Zayene2015ICDAR,
  author = {O. Zayene and J. Hennebert and S. Masmoudi Touj and R. Ingold and
    N. Essoukri Ben Amara},
  title = {A dataset for Arabic text detection, tracking and recognition in
    news videos- AcTiV},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {996-1000},
  month = {Aug},
  abstract = {Recently, promising results have been reported on video text detection
    and recognition. Most of the proposed methods are tested on private
    datasets with non-uniform evaluation metrics. We report here on the
    development of a publicly accessible annotated video dataset designed
    to assess the performance of different artificial Arabic text detection,
    tracking and recognition systems. The dataset includes 80 videos
    (more than 850,000 frames) collected from 4 different Arabic news
    channels. An attempt was made to ensure maximum diversities of the
    textual content in terms of size, position and background. This data
    is accompanied by detailed annotations for each textbox. We also
    present a region-based text detection approach in addition to a set
    of evaluation protocols on which the performance of different systems
    can be measured.},
  doi = {10.1109/ICDAR.2015.7333911},
  keywords = {natural language processing;optical character recognition;text detection;video
    signal processing;video text detection;video text recognition;private
    datasets;non-uniform evaluation metrics;publicly accessible annotated
    video dataset;artificial Arabic text detection system;artificial
    Arabic text tracking system;artificial Arabic text recognition systems;Arabic
    news channels;textual content;region-based text detection approach;AcTiV;Manganese;High
    definition video;Random access memory;Ferroelectric films;Nonvolatile
    memory;Protocols;Video OCR;Video database;Benchmark;Arabic text},
  review = {This paper reports upon the development of a publicly accessible annotated
    video dataset designed to assess the performance of different artificial
    Arabic text detection, tracking and recognition systems. The dataset
    includes 80 videos (more than 850,000 frames) collected from 4 different
    Arabic news channels.}
}

@INPROCEEDINGS{Zhong2017ICDAR,
  author = {Z. Zhong and L. Sun and Q. Huo},
  title = {Improved Localization Accuracy by LocNet for Faster R-CNN Based Text
    Detection},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {923-928},
  month = {Nov},
  abstract = {Although Faster R-CNN based approaches have achieved promising results
    for text detection, their localization accuracy is not satisfactory
    in certain cases. In this paper, we propose to use a LocNet to improve
    the localization accuracy of a Faster R.-C. N. N. based text detector.
    Given a proposal generated by region proposal network (RPN) instead
    of predicting directly the bounding box coordinates of the concerned
    text instance the proposal is enlarged to create a search region
    so that conditional probabilities to each row and column of this
    search region can be assigned, which are then used to infer accurately
    the concerned bounding box. Experiments demonstrate that the proposed
    approach boosts the localization accuracy for Faster R-CNN based
    text detection significantly. Consequently, our new text detector
    has achieved superior performance on I. C. D. A. R.-2011 I. C. D.
    A. R.-2013 and text detection benchmark tasks., M. U. L. T. I. L.
    I. G. U. L.},
  doi = {10.1109/ICDAR.2017.155},
  issn = {2379-2140},
  keywords = {feature extraction;probability;text analysis;text detection;LocNet;region
    proposal network;concerned text instance;MULTILIGUL text detection
    benchmark tasks;improved localization accuracy;faster R-CNN based
    text detection;RPN;ICDAR-2013 text detection benchmark tasks;ICDAR-2011
    text detection benchmark tasks;Proposals;Detectors;Task analysis;Training;Feature
    extraction;Image color analysis;Visualization;Accurate text detection;Faster
    R-CNN;LocNet},
  review = {This paper proposes to use a LocNet to improve the localization accuracy
    of a Faster RCNN based text detector. Given a proposal generated
    by region proposal network (RPN), instead of predicting directly
    the bounding box coordinates of the concerned text instance, the
    proposal is enlarged to create a search region so that conditional
    probabilities to each row and column of this search region can be
    assigned, which are then used to infer accurately the concerned bounding
    box.}
}

@INPROCEEDINGS{Zhu2015ICDARa,
  author = {A. Zhu and Y. Dong and G. Wang},
  title = {Recognizing perspective scene text with context feature},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {526-530},
  month = {Aug},
  abstract = {Text recognition has gained significant attention from the computer
    vision community. Correct character recognition is the premise of
    text recognition, Text and affects the overall performance to large
    extent. This paper proposes a novel character representation for
    scene text recognition. First, a context-based feature that contains
    local information and relevant feature is extracted from
    key points. The relativity is measured by the distance of vector
    that is generated by a trained Gaussian Mixture Model (GMM) between
    the target key point and other key points in each context bin. In
    order to recognize each individual character, we adopt a bag-of-words
    approach, in which the rotation-invariant context features are densely
    extracted from an individual character. All key points' context features
    are prone to build a vocabulary of visual words by using k-means
    clustering. Then we train a set of two-class linear Support Vector
    Machines in a one-vs-all schema for each category character. By using
    densely extracted context features that are rotation-invariant and
    efficient, our method is capable of recognizing perspective texts
    of arbitrary orientations. The evaluation results on benchmark datasets
    demonstrate that our proposed scheme of scene character recognition
    is highly efficient and achieves state-of-the-art performance on
    not only fontal character recognition but also perspective characters'.},
  doi = {10.1109/ICDAR.2015.7333817},
  keywords = {computer vision;feature extraction;Gaussian processes;mixture models;support
    vector machines;text detection;scene text recognition;context feature
    extraction;computer vision community;correct character recognition;Gaussian
    mixture model;GMM;bag-of-word approach;rotation-invariant context
    feature extraction;k-means clustering;two-class linear support vector
    machine;scene character recognition;fontal character recognition;Text
    recognition;Mixture models;Image resolution},
  review = {This paper proposes a novel character representation for scene text
    recognition. First, a context-based feature that contains local information
    and relevant feature is extracted from key points. The
    relativity is measured by the distance of vector that is generated
    by a trained Gaussian Mixture Model (GMM) between the target key
    point and other key points in each context bin. In order to recognize
    each individual character, a bag-of-words approach is adoted, in
    which the rotation-invariant context features are densely extracted
    from an individual character. All key points' context features are
    prone to build a vocabulary of visual words by using k-means clustering.
    Then a set of two-class linear Support Vector Machines is trained
    in a one-vs-all schema for each category character.}
}

@INPROCEEDINGS{Zhu2017ICDARb,
  author = {A. Zhu and S. Uchida},
  title = {Scene Text Relocation with Guidance},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {1289-1294},
  month = {Nov},
  abstract = {Applying object proposal technique for scene text detection becomes
    popular for its significant improvement in speed, Applying and accuracy
    for object detection. However, some of the text regions after the
    proposal classification are overlapped and hard to remove or merge.
    In this paper, we present a scene text relocation system that refines
    the detection from text proposals to text. An object proposal-based
    deep neural network is employed to get the text proposals. To tackle
    the detection overlapping problem, a refinement deep neural network
    relocates the overlapped regions by estimating the text probability
    inside and locating the accurate text regions by thresholding. Since
    the space between words in different text lines are various, a guidance
    mechanism is proposed in text relocation to guide where to extract
    the text regions in word level. This refinement procedure helps boost
    the precision after removing multiple overlapped text regions or
    joint cracked text regions. The experimental results on standard
    benchmark I. C. D. A. R. 2013 demonstrate the effectiveness of the
    proposed approach.},
  doi = {10.1109/ICDAR.2017.212},
  issn = {2379-2140},
  keywords = {image classification;neural nets;text detection;object proposal technique;scene
    text detection;object detection;scene text relocation system;text
    proposals;detection overlapping problem;refinement deep neural network;text
    probability;accurate text regions;multiple overlapped text regions;joint
    cracked text regions;standard benchmark ICDAR 2013;refinement procedure;guidance
    mechanism;indifferent text lines;Proposals;Feature extraction;Microsoft
    Windows;Text recognition;Object detection;Neural networks;Visualization},
  review = {This paper presents a scene text relocation system that refines the
    detection from text proposals to text. An object proposal-based deep
    neural network is employed to get the text proposals. To tackle the
    detection overlapping problem, a refinement deep neural network relocates
    the overlapped regions by estimating the text probability inside
    and locating the accurate text regions by thresholding. Since the
    space between words in different text lines are various, a guidance
    mechanism is proposed in text relocation to guide where to extract
    the text regions in word level.}
}

@INPROCEEDINGS{Zhu2017ICDAR,
  author = {X. Zhu and Y. Jiang and S. Yang and X. Wang and W. Li and P. Fu and
    H. Wang and Z. Luo},
  title = {Deep Residual Text Detection Network for Scene Text},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and
    Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {807-812},
  month = {Nov},
  abstract = {Scene text detection is a challenging problem in computer vision.
    In this paper, Scene and we propose a novel text detection network
    based on prevalent object detection frameworks. In order to obtain
    stronger semantic feature and we adopt ResNet as feature extraction
    layers and exploit multi-level feature by combining hierarchical
    convolutional networks. A vertical proposal mechanism is utilized
    to avoid proposal classification and while regression layer remains
    working to improve localization accuracy. Our approach evaluated
    on ICDAR2013 dataset achieves 0.91 F-measure and which outperforms
    previous state-of-the-art results in scene text detection.},
  doi = {10.1109/ICDAR.2017.137},
  issn = {2379-2140},
  keywords = {computer vision;convolution;feature extraction;image classification;neural
    nets;object detection;regression analysis;text detection;object detection
    frameworks;semantic feature;hierarchical convolutional networks;feature
    extraction layers;deep residual text detection network;scene text
    detection;vertical proposal mechanism;Proposals;Feature extraction;Semantics;Object
    detection;Task analysis;Training;Recurrent neural networks;Scene
    text detection;Deep Residual Networks;CTPN},
  review = {This paper proposes a novel text detection network based on prevalent
    object detection frameworks. In order to obtain stronger semantic
    feature, ResNet is adopted as feature extraction layers and multi-level
    features are exploited by combining hierarchical convolutional networks.
    A vertical proposal mechanism is utilized to avoid proposal classification
    and while regression layer remains working to improve localization
    accuracy.}
}

@INPROCEEDINGS{Zuo2015ICDAR,
  author = {Z. Zuo and S. Tian and W. Pei and X. Yin},
  title = {Multi-strategy tracking based text detection in scene videos},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition
    (ICDAR)},
  year = {2015},
  pages = {66-70},
  month = {Aug},
  abstract = {Text detection, Text and tracking in scene videos are important prerequisites
    for content-based video analysis and retrieval, wearable camera systems
    and mobile devices augmented reality translators. Here, we present
    a novel multi-strategy tracking based text detection approach in
    scene videos. In this approach, a state-of-the-art scene text detection
    module [1] is first used to detect text in each video frame. Then
    a multi-strategy text tracking technique is proposed which uses tracking
    by detection spatio-temporal context learning and linear prediction
    to predict the candidate text location sequentially and adaptively
    integrates and selects the best matching text block from the candidate
    blocks with a rule-based method. This multi-strategy tracking technique
    can combine the advantages of the three different tracking techniques
    and afterwards make remedies to the disadvantages of them. Experiments
    on a variety of scene videos show that our proposed approach is effective
    and robust to reduce false alarm and improve the accuracy of detection.},
  doi = {10.1109/ICDAR.2015.7333727},
  keywords = {image matching;image sequences;knowledge based systems;object tracking;spatiotemporal
    phenomena;text detection;video signal processing;content-based video
    analysis;content-based video retrieval;wearable camera systems;mobile
    device augmented reality translators;multistrategy tracking based
    text detection approach;scene videos;scene text detection module;video
    frame;multistrategy text tracking technique;spatiotemporal context
    learning;linear prediction;candidate text location prediction;text
    block matching;rule-based method;Yttrium;IP networks},
  review = {This paper presents a novel multi-strategy tracking based text detection
    approach in scene videos. In this approach, a state-of-the-art scene
    text detection module is first used to detect text in each video
    frame. Then a multi-strategy text tracking technique is proposed
    which uses tracking by detection spatio-temporal context learning
    and linear prediction to predict the candidate text location sequentially
    and adaptively integrates and selects the best matching text block
    from the candidate blocks with a rule-based method.}
}

@comment{jabref-meta: databaseType:bibtex;}

